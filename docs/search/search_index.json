{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"My notes","text":"<p>Select a section from the sidebar. Sample dot files are also available.</p>"},{"location":"aws/","title":"Amazon Web Services","text":""},{"location":"aws/#aws-cli","title":"AWS CLI","text":""},{"location":"aws/#install-the-aws-command-line-interface","title":"Install the AWS command line interface","text":"<pre><code>curl \"https://awscli.amazonaws.com/AWSCLIV2.pkg\" -o \"AWSCLIV2.pkg\"\nsudo installer -pkg AWSCLIV2.pkg -target /\n</code></pre>"},{"location":"aws/#check-if-the-aws-cli-is-installed-correctly","title":"Check if the AWS CLI is installed correctly","text":"<pre><code># you may need to reload your shell's rc file first.\naws --version\n</code></pre>"},{"location":"aws/#configure-the-aws-cli","title":"Configure the AWS CLI","text":"<pre><code>aws configure\n</code></pre>"},{"location":"aws/#alexa-skills-kit-ask-cli","title":"Alexa Skills Kit (ASK) CLI","text":"<p>Set up credentials for AWS account Quick Start ASK CLI</p>"},{"location":"aws/#aws-amplify","title":"AWS Amplify","text":"<pre><code>npm install -g @aws-amplify/cli\namplify configure\n</code></pre>"},{"location":"docker/","title":"Docker","text":""},{"location":"docker/#install","title":"Install","text":""},{"location":"docker/#macos","title":"macOS","text":"<pre><code>brew install --cask docker\n</code></pre> <ul> <li>You must use the --cask version. Otherwise only the client is included and can't run the Docker daemon. Then open the Docker app and grant privileged access when asked. Only then will you be able to use docker.</li> </ul>"},{"location":"docker/#linux-raspberry-pi-os","title":"Linux / Raspberry Pi OS:","text":"<p>If you just ran <code>apt upgrade</code> on your Raspberry Pi, reboot before installing Docker. Follow the appropriate installation method.  </p> <p>If you don't want to have to prefix commands with sudo add your user to the <code>docker</code> group. This is equivalent to giving that user root privileges.</p> <pre><code>cat /etc/group | grep docker # see if docker group exists\nsudo usermod -aG docker $USER\n</code></pre> <p>Follow Post-installation steps to configure log rotation.  </p>"},{"location":"docker/#use","title":"Use","text":""},{"location":"docker/#architecture","title":"Architecture","text":"<p>Supported Architectures Platform specifiers </p> Value Normalized Examples aarch64 arm64 Apple M1/M2, Raspberry Pi 4 armhf arm Raspberry Pi 2 armel arm/v6 i386 386 x86_64 amd64 Intel (Default) x86-64 amd64 Intel (Default) <p>Docker Desktop for Apple silicon can run (or build) an Intel image using emulation. The <code>--platform</code> option sets the platform if server is multi-platform capable.  </p> <p>Macbook M1/M2 is based on ARM. The Docker host is detected as linux/arm64/v8 by Docker.  </p> <p>On a Macbook M1/M2 running a native arm64 linux image instead of an amd64 (x86-64 Intel) image with emulation: When installing apps on the container either install the package for the ARM platform if they provide one, or run an amd64 docker image (Docker will use emulation). </p> <p>See Docker host info. You need to use the real field names (not display names) so we'll grab the info in json to get the real field names and use <code>jq</code> to display it nicely. Then we'll render them using Go templates.</p> <pre><code>docker info --format '{{json .}}' | jq .\n\ndocker info --format \"{{.Plugins.Volume}}\"\ndocker info --format \"{{.OSType}} - {{.Architecture}} CPUs: {{.NCPU}}\"\n</code></pre> <p>With the default images, Docker Desktop for Apple silicon warns us the requested image's platform (linux/amd64) is different from the detected host platform (linux/arm64/v8) and no specific platform was requested. <code>--platform linux/amd64</code> is the default and the same as  <code>--platform linux/x86_64</code>. It runs (or builds) an Intel image. <code>--platform linux/arm64</code> runs (or builds) an aarch64 (arm64) image. Architecture used by Appple M1/M2.  </p> <pre><code>docker run -it --name container1 debian # WARNING. uname -m -&gt; x86_64 (amd64)\ndocker run -it --platform linux/amd64 --name container1 debian # NO warning. uname -m -&gt; x86_64 (amd64). On Mac it emulates Intel.\ndocker run -it --platform linux/arm64 --name container1 debian # NO warning. uname -m -&gt; aarch64 (arm64). Mac native.\n\n# On a Mac M2 you can also use this image\ndocker run -it --name mycontainer arm64v8/debian bash # NO warning. uname -m -&gt; aarch64 (arm64). Mac native.\n</code></pre>"},{"location":"docker/#examples","title":"Examples","text":"<p>Dockerfile instruction to keep a container running</p> <pre><code>CMD tail -f /dev/null\n</code></pre> <p>Running containers</p> <pre><code># sanity check\ndocker version\ndocker --help\n\n# \"docker run\" = \"docker container run\"\n\ndocker run --name container1 debian # created and Exited\ndocker run -it --name container2 debian # created and Up. Interactive tty\ndocker run --detach --name container3 debian # created and Exited\ndocker run -it --detach --name container4 debian # created and Up. Running in background\n\ndocker stop container4 # Exited\ndocker restart container4 # Up\ndocker attach container4 # Interactive tty\n\n# To have it removed automatically when it's stopped \ndocker run -it --rm --name mycontainer debian # created and Up. Interactive tty. \n# Run bash in a new container with interactive tty, based on debian image.\ndocker run -it --name mycontainer debian bash\n# Get the IDs of all stopped containers. Options: all, quiet (only IDs), filter.\ndocker ps -aq -f status=exited\n\n</code></pre> <p>Working with images and volumes</p> <pre><code>\n# Show all images (including intermediate images)\ndocker image ls -a\n\n# Build an image from a Dockerfile and give it a name (or name:tag).\ndocker build -f mydockerfile -t santisbon/myimage .\ndocker builder prune -a     # Remove all unused build cache, not just dangling ones\ndocker image rm santisbon/myimage\n\n# Volumes\ndocker volume create my-vol\ndocker volume ls\ndocker volume inspect my-vol\ndocker volume prune\ndocker volume rm my-vol\n\n# Start a container from an image.\ndocker run \\\n--name mycontainer \\\n--hostname mycontainer \\\n--mount source=my-vol,target=/data \\\nsantisbon/myimage\n\n# Get rid of all stopped containers. Options: remove **anonymous volumes** associated with the container.\ndocker rm -v $(docker ps -aq -f status=exited)\n# You might want to make an alias:\nalias drm='docker rm -v $(docker ps -aq -f status=exited)'\n# or\nalias drm='sudo docker rm -v $(sudo docker ps -aq -f status=exited)'\n\n# on macOS the volume mountpoint (/var/lib/docker/volumes/) is in the VM that Docker Desktop uses to provide the Linux environment.\n# You can read it through a container given extended privileges. \n# Use: \n# - the \"host\" PID namespace, \n# - an image e.g. debian, \n# - the nsenter command to run a program (sh) in a different namespace\n# - the target process with PID 1\n# - enter following namspaces of the target process: mount, UTS, network, IPC.\ndocker run -it --privileged --pid=host debian nsenter -t 1 -m -u -n -i sh\n</code></pre> <p>If you want to build multi-platform docker images:</p> <pre><code>docker buildx create --name mybuilder --driver docker-container --bootstrap\ndocker buildx use mybuilder\n\ndocker buildx inspect\ndocker buildx ls\n</code></pre> <p>Docker Compose. Multi-container deployments in a compose.yaml file. Using Compose in Production </p> <pre><code>docker compose -p mastodon-bot-project up --detach\n# or\ndocker compose -p mastodon-bot-project create\ndocker compose -p mastodon-bot-project start\n\n# connect to a running container\ndocker exec -it bot-app bash\n\ndocker compose -p mastodon-bot-project down\n# or\ndocker compose -p mastodon-bot-project stop\n</code></pre> <p>Kubernetes</p> <pre><code># Convert the Docker Compose file to k8s files\nkompose --file compose.yaml convert\n\n# Make sure the container image is available in a repository. \n# You can build it with ```docker build``` or ```docker compose create``` and push it to a public repository.\ndocker image push user/repo\n# multiple -f filenames or a folder \nkubectl apply -f ./k8s\nkubectl get pods\nkubectl delete -f ./k8s\n</code></pre>"},{"location":"editor/","title":"Code Editor (VSCode/VSCodium)","text":"<p>Q. VSCodium cannot be opened because Apple cannot check it for malicious software. A. You have to \"right click\" the .app and then hold the alt/option key, while clicking \"Open\" (only on first launch).</p> <p>Disable telemetry in: Code -&gt; Settings -&gt; Telemetry Settings -&gt; Telemetry Level -&gt; off.</p> <p>Useful extensions:  </p> <ul> <li>Better TOML</li> <li>Compare Folders</li> <li>Draw.io Integration</li> <li>Remote - SSH</li> </ul>"},{"location":"git/","title":"Git","text":""},{"location":"git/#install","title":"Install","text":"<p>macOS  </p> <pre><code>brew install git\ngit --version\n\n# GitHub CLI\nbrew install gh \ngh auth login\ngh config set editor \"codium -w\" # or code, nano, etc\n</code></pre> <p>GitHub CLI reference.</p> <p>Linux</p> <pre><code>sudo yum install git # or git-all\ngit --version\n</code></pre>"},{"location":"git/#configure","title":"Configure","text":"<pre><code>git config --global core.editor 'codium --wait'\n\ngit config --global diff.tool codium\ngit config --global difftool.codium.cmd 'codium --wait --diff $LOCAL $REMOTE'\n\ngit config --global merge.tool codium\ngit config --global mergetool.codium.cmd 'codium --wait $MERGED'\n\n# or\ncodium ~/.gitconfig # and paste from sample dot file. Also: codium, vscode, nano\n</code></pre> <p>Then you can <code>git difftool main feature-branch</code>.  </p> <p>If using AWS CodeCommit do this after configuring the AWS CLI:</p> <pre><code>git config --global credential.helper '!aws codecommit credential-helper $@'\ngit config --global credential.UseHttpPath true\n</code></pre> <p>Troubleshooting CodeCommit</p> <p>To create a squash function:</p> <pre><code>git config --global alias.squash-all '!f(){ git reset $(git commit-tree \"HEAD^{tree}\" \"$@\");};f'\n</code></pre> <p>Info</p> <ul> <li>Git allows you to escape to a shell (like bash or zsh) using the ! (bang). Learn more.</li> <li><code>commit-tree</code> creates a new commit object based on the provided tree object and emits the new commit object id on stdout.</li> <li><code>tree</code> objects correspond to UNIX directory entries. </li> <li><code>reset</code> resets current HEAD to the specified state e.g a commit.</li> <li>The <code>master^{tree}</code> syntax specifies the tree object that is pointed to by the last commit on your <code>master</code> branch. So <code>HEAD^{tree}</code> is the tree object pointed to by the last commit on your current branch.</li> <li>If you\u2019re using ZSH, the <code>^</code> character is used for globbing, so you have to enclose the whole expression in quotes: <code>\"HEAD^{tree}\"</code></li> </ul> <p>Then just run:</p> <pre><code>git squash-all -m \"a brand new start\"\ngit push -f\n</code></pre>"},{"location":"git/#set-up-git-autocompletion-bash","title":"Set up git autocompletion (bash)","text":"<pre><code>cd ~\ncurl -OL https://github.com/git/git/raw/master/contrib/completion/git-completion.bash\nmv ~/git-completion.bash ~/.git-completion.bash\n</code></pre>"},{"location":"git/#set-up-ssh-key","title":"Set up SSH key","text":"<ol> <li>Check for existing keys.</li> <li>Generate a new SSH key and add it to ssh-agent. You may need to set permissions to your key file with <code>chmod 600</code>.</li> <li>Add the public key to your GitHub account.</li> </ol> <p>If the terminal is no longer authenticating you:</p> <pre><code>git remote -v # is it https or ssh? Should be ssh\ngit remote remove origin\ngit remote add origin git@github.com:user/repo.git\n</code></pre> <p>Still having issues? Start the ssh-agent in the background and add your SSH private key to the ssh-agent.</p> <pre><code>ps -ax | grep ssh-agent\neval \"$(ssh-agent -s)\"\nssh-add --apple-use-keychain ~/.ssh/id_ed25519\n</code></pre> <p>If you need to add your public key to Github again copy and paste it on your Settings page on Github:</p> <pre><code>pbcopy &lt; ~/.ssh/id_ed25519.pub\n</code></pre>"},{"location":"install-os/","title":"Do a clean install of the OS","text":"<p>On macOS download the new version from System Preferences, Software Update (or the Mac App Store) and create the bootable media with:</p> <pre><code>sudo /Applications/Install\\ macOS\\ Ventura.app/Contents/Resources/createinstallmedia --volume /Volumes/MyVolume/\n</code></pre> <p>using the volume that matches the name of the external drive you are using.  </p> <p>If you have an Intel-powered Mac here's how to install macOS from a bootable installer:</p> <ol> <li>Plug in your bootable media.</li> <li>Turn off your Mac.</li> <li>Press and hold Option/Alt while the Mac starts up - keep pressing the key until you see a screen showing the bootable volume.</li> </ol> <p>If you have an Apple silicon Mac here's how to install macOS from a bootable installer:</p> <ol> <li>Plug in your bootable media.</li> <li>Turn off your Mac.</li> <li>Press the power button to turn on the Mac - but keep it pressed until you see the startup options window including your bootable volume.  </li> </ol> <p>Open Finder and show hidden files with Command + Shift + . (period) or with</p> <pre><code>defaults write com.apple.finder AppleShowAllFiles TRUE; killall Finder\n</code></pre>"},{"location":"k8s/","title":"Kubernetes","text":""},{"location":"k8s/#kubernetes-essentials","title":"Kubernetes Essentials","text":"<p>Interactive Diagram Working with k8s objects Log rotation </p> <p>In order for Kubernetes to pull your container image you need to first push it to an image repository like Docker Hub. To avoid storing your Docker Hub password unencrypted in $HOME/.docker/config.json when you <code>docker login</code> to your account, use a credentials store. A helper program lets you interact with such a keychain or external store. </p> <p>If you're doing this on your laptop with Docker Desktop it already provides a store. Otherwise, use one of the stores supported by the <code>docker-credential-helper</code>. Now <code>docker login</code> on your terminal or on the Docker Desktop app.  </p>"},{"location":"k8s/#designing-applications-for-kubernetes","title":"Designing Applications for Kubernetes","text":"<p>Based on the 12-Factor App Design Methodology</p> <p>A Cloud Guru course. It uses Ubuntu 20.04 Focal Fossa LTS and the Calico network plugin instead of Flannel. Example with 1 control plane node and 2 worker nodes.</p>"},{"location":"k8s/#building-a-kubernetes-cluster","title":"Building a Kubernetes Cluster","text":"<p>Reference Installing kubeadm Creating a cluster with kubeadm </p> <ul> <li>kubeadm sometimes doesn't work with the latest and greatest version of docker right away.</li> </ul> <p><code>kubeadm</code> simplifies the process of setting up a k8s cluster. <code>containerd</code> manages the complete container lifecycle of its host system, from image transfer and storage to container execution and supervision to low-level storage to network attachments. <code>kubelet</code> handles running containers on a node. <code>kubectl</code> is a tool for managing the cluster.  </p> <p>If you wish, you can set an appropriate hostname for each node. On the control plane node:  </p> <pre><code>sudo hostnamectl set-hostname k8s-control\n</code></pre> <p>On the first worker node:  </p> <pre><code>sudo hostnamectl set-hostname k8s-worker1\n</code></pre> <p>On the second worker node:  </p> <pre><code>sudo hostnamectl set-hostname k8s-worker2\n</code></pre> <p>On all nodes, set up the hosts file to enable all the nodes to reach each other using these hostnames.</p> <pre><code>sudo nano /etc/hosts\n</code></pre> <p>On all nodes, add the following at the end of the file. You will need to supply the actual private IP address for each node.</p> <pre><code>&lt;control plane node private IP&gt; k8s-control\n&lt;worker node 1 private IP&gt; k8s-worker1\n&lt;worker node 2 private IP&gt; k8s-worker2\n</code></pre> <p>Log out of all three servers and log back in to see these changes take effect.  </p> <p>On all nodes, set up containerd. You will need to load some kernel modules and modify some system settings as part of this process.  </p> <pre><code># Enable them when the server start up\ncat &lt;&lt; EOF | sudo tee /etc/modules-load.d/containerd.conf\noverlay\nbr_netfilter\nEOF\n\n# Enable them right now without having to restart the server\nsudo modprobe overlay\nsudo modprobe br_netfilter\n\n# Add network configurations k8s will need\ncat &lt;&lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf\nnet.bridge.bridge-nf-call-iptables = 1\nnet.ipv4.ip_forward = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nEOF\n\n# Apply them immediately\nsudo sysctl --system\n</code></pre> <p>Install and configure containerd.  </p> <pre><code>sudo apt-get update &amp;&amp; sudo apt-get install -y containerd\nsudo mkdir -p /etc/containerd\n# Generate the contents of a default config file and save it\nsudo containerd config default | sudo tee /etc/containerd/config.toml\n# Restart containerd to make sure it's using that configuration\nsudo systemctl restart containerd\n</code></pre> <p>On all nodes, disable swap.</p> <pre><code>sudo swapoff -a\n</code></pre> <p>On all nodes, install kubeadm, kubelet, and kubectl.</p> <pre><code># Some required packages\nsudo apt-get update &amp;&amp; sudo apt-get install -y apt-transport-https curl\n# Set up the package repo for k8s packages. Download the key for the repo and add it\ncurl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n# Configure the repo\ncat &lt;&lt; EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list\ndeb https://apt.kubernetes.io/ kubernetes-xenial main\nEOF\nsudo apt-get update\n# Install the k8s packages. Make sure the versions for all 3 are the same.\nsudo apt-get install -y kubelet=1.24.0-00 kubeadm=1.24.0-00 kubectl=1.24.0-00\n# Make sure they're not automatically upgraded. Have manual control over when to update k8s\nsudo apt-mark hold kubelet kubeadm kubectl\n</code></pre> <p>On the control plane node only, initialize the cluster and set up kubectl access.</p> <pre><code>sudo kubeadm init --pod-network-cidr 192.168.0.0/16 --kubernetes-version 1.24.0\n# Config File to authenticate and interact with the cluster with kubectl commands\n# These are in the output of the previous step\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n</code></pre> <p>Verify the cluster is working. It will be in Not Ready status because we haven't configured the networking plugin.</p> <pre><code>kubectl get nodes\n</code></pre> <p>Install the Calico network add-on.</p> <pre><code>kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml\n</code></pre> <p>Get the join command (this command is also printed during kubeadm init . Feel free to simply copy it from there).</p> <pre><code>kubeadm token create --print-join-command\n</code></pre> <p>Copy the join command from the control plane node. Run it on each worker node as root (i.e. with sudo ).</p> <pre><code>sudo kubeadm join ...\n</code></pre> <p>On the control plane node, verify all nodes in your cluster are ready. Note that it may take a few moments for all of the nodes to enter the READY state.</p> <pre><code>kubectl get nodes\n</code></pre>"},{"location":"k8s/#installing-docker","title":"Installing Docker","text":"<p>Reference Install Docker Engine on Ubuntu Docker credentials store  to avoid storing your Docker Hub password unencrypted in <code>$HOME/.docker/config.json</code> when you <code>docker login</code> and <code>docker push</code> your images.  </p> <p>On the system that will build Docker images from source code e.g. a CI server, install and configure Docker. For simplicity we'll use the control plane server just so we don't have to create another server for this exercise.  </p> <p>Create a docker group. Users in this group will have permission to use Docker on the system:</p> <pre><code>sudo groupadd docker\n</code></pre> <p>Install required packages. Note: Some of these packages may already be present on the system, but including them here will not cause any problems:</p> <pre><code>sudo apt-get update &amp;&amp; sudo apt-get install -y apt-transport-https ca-certificates curl gnupg lsb-release\n</code></pre> <p>Set up the Docker GPG key and package repository:</p> <pre><code>curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\necho \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n</code></pre> <p>Install the Docker Engine:</p> <pre><code>sudo apt-get update &amp;&amp; sudo apt-get install -y docker-ce docker-ce-cli\n# Type N (default) or enter to keep your current containerd configuration\n</code></pre> <p>Test the Docker setup:</p> <pre><code>sudo docker version\n</code></pre> <p>Add cloud_user to the docker group in order to give cloud_user access to use Docker:</p> <pre><code>sudo usermod -aG docker cloud_user\n</code></pre> <p>Log out of the server and log back in. Test your setup:</p> <pre><code>docker version\n</code></pre>"},{"location":"k8s/#kubernetes-configuration-with-configmaps-and-secrets","title":"Kubernetes configuration with ConfigMaps and Secrets","text":"<p>III. Config Store config in the environment</p> <p>Reference </p> <p>Encrypting Secret Data at Rest ConfigMaps Secrets </p> <p>Create a production Namespace:</p> <pre><code>kubectl create namespace production\n</code></pre> <p>Get base64-encoded strings for a db username and password:</p> <pre><code>echo -n my_user | base64\necho -n my_password | base64\n</code></pre> <p>Example: Create a ConfigMap and Secret to configure the backing service connection information for the app, including the base64-encoded credentials:</p> <pre><code>cat &gt; my-app-config.yml &lt;&lt;End-of-message \napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: my-app-config\ndata:\n  mongodb.host: \"my-app-mongodb\"\n  mongodb.port: \"27017\"\n\n---\n\napiVersion: v1\nkind: Secret\nmetadata:\n  name: my-app-secure-config\ntype: Opaque\ndata:\n  mongodb.username: dWxvZV91c2Vy\n  mongodb.password: SUxvdmVUaGVMaXN0\n\nEnd-of-message\n</code></pre> <pre><code>kubectl apply -f my-app-config.yml -n production\n</code></pre> <p>Create a temporary Pod to test the configuration setup. Note that you need to supply your Docker Hub username as part of the image name in this file. This passes configuration data in env variables but you could also do it in files that will show up on the containers filesystem.</p> <pre><code>cat &gt; test-pod.yml &lt;&lt;End-of-message\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: test-pod\nspec:\n  containers:\n  - name: my-app-server\n    image: &lt;YOUR_DOCKER_HUB_USERNAME&gt;/my-app-server:0.0.1\n    ports:\n    - name: web\n      containerPort: 3001\n      protocol: TCP\n    env:\n    - name: MONGODB_HOST\n      valueFrom:\n        configMapKeyRef:\n          name: my-app-config\n          key: mongodb.host\n    - name: MONGODB_PORT\n      valueFrom:\n        configMapKeyRef:\n          name: my-app-config\n          key: mongodb.port\n    - name: MONGODB_USER\n      valueFrom:\n        secretKeyRef:\n          name: my-app-secure-config\n          key: mongodb.username\n    - name: MONGODB_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: my-app-secure-config\n          key: mongodb.password\n\nEnd-of-message\n</code></pre> <pre><code>kubectl apply -f test-pod.yml -n production\n</code></pre> <p>Check the logs to verify the config data is being passed to the container:</p> <pre><code>kubectl logs test-pod -n production\n</code></pre> <p>Clean up the test pod:</p> <pre><code>kubectl delete pod test-pod -n production --force\n</code></pre>"},{"location":"k8s/#build-release-run-with-docker-and-deployments","title":"Build, Release, Run with Docker and Deployments","text":"<p>V. Build, release, run Strictly separate build and run stages</p> <p>Reference Labels and Selectors Deployments </p> <p>Example: After you <code>docker build</code> and <code>docker push</code> your image to a repository, create a deployment file for your app. The <code>selector</code> selects pods that have the specified label name and value. <code>template</code> is the pod template. This example puts 2 containers in the same pod for simplicity but in the real world you'll want separate deployments to scale them independently.</p> <pre><code>cat &gt; my-app.yml &lt;&lt;End-of-message\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: my-app-config\ndata:\n  mongodb.host: \"my-app-mongodb\"\n  mongodb.port: \"27017\"\n  .env: |\n    REACT_APP_API_PORT=\"30081\"\n\n---\n\napiVersion: v1\nkind: Secret\nmetadata:\n  name: my-app-secure-config\ntype: Opaque\ndata:\n  mongodb.username: dWxvZV91c2Vy\n  mongodb.password: SUxvdmVUaGVMaXN0\n\n---\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-app-svc\nspec:\n  type: NodePort\n  selector:\n    app: my-app\n  ports:\n  - name: frontend\n    protocol: TCP\n    port: 30080\n    nodePort: 30080\n    targetPort: 5000\n  - name: server\n    protocol: TCP\n    port: 30081\n    nodePort: 30081\n    targetPort: 3001\n\n---\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: my-app\n  template:\n    metadata:\n      labels:\n        app: my-app\n    spec:\n      containers:\n      - name: my-app-server\n        image: &lt;Your Docker Hub username&gt;/my-app-server:0.0.1\n        ports:\n        - name: web\n          containerPort: 3001\n          protocol: TCP\n        env:\n        - name: MONGODB_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: my-app-config\n              key: mongodb.host\n        - name: MONGODB_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: my-app-config\n              key: mongodb.port\n        - name: MONGODB_USER\n          valueFrom:\n            secretKeyRef:\n              name: my-app-secure-config\n              key: mongodb.username\n        - name: MONGODB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: my-app-secure-config\n              key: mongodb.password\n      - name: my-app-frontend\n        image: &lt;Your Docker Hub username&gt;/my-app-frontend:0.0.1\n        ports:\n        - name: web\n          containerPort: 5000\n          protocol: TCP\n        volumeMounts:\n        - name: frontend-config\n          mountPath: /usr/src/app/.env\n          subPath: .env\n          readOnly: true\n      volumes:\n      - name: frontend-config\n        configMap:\n          name: my-app-config\n          items:\n          - key: .env\n            path: .env\nEnd-of-message\n</code></pre> <p>Deploy the app.</p> <pre><code>kubectl apply -f my-app.yml -n production\n</code></pre> <p>Create a new container image version to test the rollout process:</p> <pre><code>docker tag &lt;Your Docker Hub username&gt;/my-app-frontend:0.0.1 &lt;Your Docker Hub username&gt;/my-app-frontend:0.0.2\ndocker push &lt;Your Docker Hub username&gt;/my-app-frontend:0.0.2\n</code></pre> <p>Edit the app manifest <code>my-app.yml</code> to use the <code>0.0.2</code> image version and then:</p> <pre><code>kubectl apply -f my-app.yml -n production\n</code></pre> <p>Get the list of Pods to see the new version rollout:</p> <pre><code>kubectl get pods -n production\n</code></pre>"},{"location":"k8s/#processes-with-stateless-containers","title":"Processes with stateless containers","text":"<p>VI. Processes Execute the app as one or more stateless processes</p> <p>Reference Security Context </p> <p>Edit the app deployment YAML <code>my-app.yml</code>. In the deployment Pod spec, add a new emptyDir volume under volumes:</p> <pre><code>volumes:\n- name: added-items-log\n  emptyDir: {}\n...\n</code></pre> <p>Mount the volume to the server container:</p> <pre><code>containers:\n...\n- name: my-app-server\n  ...\n  volumeMounts:\n  - name: added-items-log\n    mountPath: /usr/src/app/added_items.log\n    subPath: added_items.log\n    readOnly: false\n  ...\n</code></pre> <p>Make the container file system read only:</p> <pre><code>containers:\n...\n- name: my-app-server\n  securityContext:\n    readOnlyRootFilesystem: true\n  ...\n</code></pre> <p>Deploy the changes:</p> <pre><code>kubectl apply -f my-app.yml -n production\n</code></pre>"},{"location":"k8s/#persistent-volumes","title":"Persistent Volumes","text":"<p>Reference Persistent Volumes (PV) <code>local</code> PV </p> <p>Create a <code>StorageClass</code> that supports volume expansion as <code>localdisk-sc.yml</code></p> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: localdisk\nprovisioner: kubernetes.io/no-provisioner\nallowVolumeExpansion: true\n</code></pre> <pre><code>kubectl create -f localdisk-sc.yml\n</code></pre> <p><code>persistentVolumeReclaimPolicy</code> says how storage can be reused when the volume's associated claims are deleted.  </p> <ul> <li>Retain: Keeps all data. An admin must manually clean up and prepare the resource for reuse.</li> <li>Recycle: Automatically deletes all data, allowing  the volume to be reused.</li> <li>Delete: Deletes underlying storage resource automatically (applies to cloud only).  </li> </ul> <p><code>accessModes</code> can be:</p> <ul> <li>ReadWriteOnce: The volume can be mounted as read-write by a single node. Still can allow multiple pods to access the volume when the pods are running on the same node.  </li> <li>ReadOnlyMany: Can be mounted as read-only by many nodes.</li> <li>ReadWriteMany: Can be mounted as read-write by many nodes.</li> <li>ReadWriteOncePod: Can be mounted as read-write by a single Pod. Use ReadWriteOncePod access mode if you want to ensure that only one pod across whole cluster can read that PVC or write to it. This is only supported for CSI volumes.</li> </ul> <p>Create a PersistentVolume in <code>my-pv.yml</code>.</p> <pre><code>kind: PersistentVolume\napiVersion: v1\nmetadata:\n  name: my-pv\nspec:\n  storageClassName: localdisk\n  persistentVolumeReclaimPolicy: Recycle\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\n  hostPath:\n    path: /var/output\n</code></pre> <pre><code>kubectl create -f my-pv.yml\n</code></pre> <p>Check the status of the PersistentVolume.</p> <pre><code>kubectl get pv\n</code></pre> <p>Create a PersistentVolumeClaim that will bind to the PersistentVolume as <code>my-pvc.yml</code></p> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: my-pvc\nspec:\n  storageClassName: localdisk\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 100Mi\n</code></pre> <pre><code>kubectl create -f my-pvc.yml\n</code></pre> <p>Check the status of the PersistentVolume and PersistentVolumeClaim to verify that they have been bound.</p> <pre><code>kubectl get pv\nkubectl get pvc\n</code></pre> <p>Create a Pod that uses the PersistentVolumeClaim as <code>pv-pod.yml</code>.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: pv-pod\nspec:\n  restartPolicy: Never\n  containers:\n  - name: busybox\n    image: busybox\n    command: ['sh', '-c', 'echo Success! &gt; /output/success.txt']\n    volumeMounts:\n    - name: pv-storage\n      mountPath: /output\n  volumes:\n  - name: pv-storage\n    persistentVolumeClaim:\n      claimName: my-pvc\n</code></pre> <pre><code>kubectl create -f pv-pod.yml\n</code></pre> <p>Expand the PersistentVolumeClaim and record the process.</p> <pre><code>kubectl edit pvc my-pvc --record\n</code></pre> <pre><code>...\nspec:\n...\n  resources:\n    requests:\n      storage: 200Mi\n</code></pre> <p>Delete the Pod and the PersistentVolumeClaim.</p> <pre><code>kubectl delete pod pv-pod\nkubectl delete pvc my-pvc\n</code></pre> <p>Check the status of the PersistentVolume to verify that it has been successfully recycled and is available again.</p> <pre><code>kubectl get pv\n</code></pre>"},{"location":"k8s/#port-binding-with-pods","title":"Port Binding with Pods","text":"<p>VII. Port binding Export services via port binding</p> <p>Reference Cluster Networking </p> <p>Challenge: only 1 process can listen on a port per host. So how do all apps on the host use a unique port? In k8s, each pod has its own network namespace and cluster IP address. That IP address is unique within the cluster even if there are multiple worker nodes in the cluster. Tht means ports only need to be unique within each pod. 2 pods can listen on the same port because they each have their own unique IP address within the cluster network. The pods can communicate across nodes simply using the unique IPs.</p> <p>Get a list of Pods in the production namespace:</p> <pre><code>kubectl get pods -n production -o wide\n</code></pre> <p>Copy the name of the IP address of the application Pod. Example: Use the IP address to make a request to the port on the Pod that serves the frontend content:</p> <pre><code>curl &lt;Pod Cluster IP address&gt;:5000\n</code></pre>"},{"location":"k8s/#concurrency-with-containers-and-scaling","title":"Concurrency with Containers and Scaling","text":"<p>VIII. Concurrency Scale out via the process model</p> <p>Reference Deployments </p> <p>By using <code>services</code> to manage access to the app, the service automaticaly picks up the additional pods created during scaling and route traffic to those pods. When you have <code>services</code> alongside <code>deployments</code> you can dynamically change the number of replicas that you have and k8s will take care of everything.</p> <p>Edit the application deployment <code>my-app.yml</code>. Change the number of replicas to 3:</p> <pre><code>...\nreplicas: 3\n</code></pre> <p>Apply the changes:</p> <pre><code>kubectl apply -f my-app.yml -n production\n</code></pre> <p>Get a list of Pods:</p> <pre><code>kubectl get pods -n production\n</code></pre> <p>Scale the deployment up again in <code>my-app.yml</code>. Change the number of replicas to 5:</p> <pre><code>...\nreplicas: 5\n</code></pre> <p>Apply the changes:</p> <pre><code>kubectl apply -f my-app .yml -n production\n</code></pre> <p>Get a list of Pods:</p> <pre><code>kubectl get pods -n production\n</code></pre>"},{"location":"k8s/#disposability-with-stateless-containers","title":"Disposability with Stateless Containers","text":"<p>IX. Disposability Maximize robustness with fast startup and graceful shutdown</p> <p>Reference Security Context Pod Lifecycle </p> <p>Deployments can be used to maintain a specified number of running replicas automatically replacing pods that fail or are deleted.</p> <p>Get a list of Pods:</p> <pre><code>kubectl get pods -n production\n</code></pre> <p>Locate one of the Pods from the my-app deployment and copy the Pod's name. Delete the Pod using the Pod name:</p> <pre><code>kubectl delete pod &lt;Pod name&gt; -n production\n</code></pre> <p>Get the list of Pods again. You will notice that the deployment is automatically creating a new Pod to replace the one that was deleted:</p> <pre><code>kubectl get pods -n production\n</code></pre>"},{"location":"k8s/#devprod-parity-with-namespaces","title":"Dev/Prod Parity with Namespaces","text":"<p>X. Dev/prod parity Keep development, staging, and production as similar as possible</p> <p>Reference Namespaces </p> <p>k8s namespaces allow us to have multiple environments in the same cluster. A namespace is like a virtual cluster.</p> <p>Create a new namespace:</p> <pre><code>kubectl create namespace dev\n</code></pre> <p>Make a copy of the my-app app YAML:</p> <pre><code>cp my-app.yml my-app-dev.yml\n</code></pre> <p><code>NodePort</code> <code>services</code> need to be unique within the cluster. We need to choose unique ports so dev doesn't conflict with production. Edit the my-app-svc service in the <code>my-app-dev.yml</code> file to select different <code>nodePort</code>s. You will also need to edit the my-app-config ConfigMap to reflect the new port.  Set the nodePorts on the service:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-app-svc\nspec:\n  type: NodePort\n  selector:\n    app: my-app\n  ports:\n  - name: frontend\n    protocol: TCP\n    port: 30082\n    nodePort: 30082\n    targetPort: 5000\n  - name: server\n    protocol: TCP\n    port: 30083\n    nodePort: 30083\n    targetPort: 3001\n</code></pre> <p>Update the configured port in the ConfigMap:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: my-app-config\ndata:\n  mongodb.host: \"my-app-mongodb\"\n  mongodb.port: \"27017\"\n  .env: |\n    REACT_APP_API_PORT=\"30083\"\n</code></pre> <p>Deploy the backing service setup in the new namespace:</p> <pre><code>kubectl apply -f k8s-my-app-mongodb.yml -n dev\nkubectl apply -f my-app-mongodb.yml -n dev\n</code></pre> <p>Deploy the app in the new namespace:</p> <pre><code>kubectl apply -f my-app-dev.yml -n dev\n</code></pre> <p>Check the status of the Pods:</p> <pre><code>kubectl get pods -n dev\n</code></pre> <p>Once all the Pods are up and running, you should be able to test the dev environment in a browser at <code>&lt;Control Plane Node Public IP&gt;:30082</code>.</p>"},{"location":"k8s/#logs-with-k8s-container-logging","title":"Logs with k8s Container Logging","text":"<p>XI. Logs Treat logs as event streams</p> <p>Reference Logging Architecture Kubectl cheatsheet </p> <p>k8s captures log data written to stdout by containers. We can use the k8s API, <code>kubectl logs</code> or external tools to interact with container logs.  </p> <p>Edit the source code for the server e.g. <code>src/server/index.js</code>. There is a log function that writes to a file. Change this function to simply write log data to the console:  </p> <pre><code>log = function(data) {\nconsole.log(data);\n}\n</code></pre> <p>Build a new server image because we changed the source code:</p> <pre><code>docker build -t &lt;Your Docker Hub username&gt;/my-app-server:0.0.4 --target server .\n</code></pre> <p>Push the image:</p> <pre><code>docker push &lt;Your Docker Hub username&gt;/my-app-server:0.0.4\n</code></pre> <p>Deploy the new code. Edit <code>my-app.yml</code>. Change the image version for the server to the new image:</p> <pre><code>containers:\n- name: my-app-server\n  image: &lt;Your Docker Hub username&gt;/my-app-server:0.0.4\n</code></pre> <pre><code>kubectl apply -f my-app.yml -n production\n</code></pre> <p>Get a list of Pods:</p> <pre><code>kubectl get pods -n production\n</code></pre> <p>Copy the name of one of the my-app deployment Pods and view its logs specifying the pod, namespace, and container.</p> <pre><code>kubectl logs &lt;Pod name&gt; -n production -c my-app-server\n</code></pre>"},{"location":"k8s/#admin-processes-with-jobs","title":"Admin Processes with Jobs","text":"<p>XII. Admin processes Run admin/management tasks as one-off processes</p> <p>Reference Jobs </p> <p>A <code>Job</code> e.g. a database migration runs a container until its execution completes. Jobs handle re-trying execution if it fails. Jobs have a <code>restartPolicy</code> of <code>Never</code> because once they complete they don't run again.  This example adds the administrative job to the server image but you could package it into its own image.  </p> <p>Edit the source code for the admin process in e.g. <code>src/jobs/deDeuplicateJob.js</code>. Locate the block of code that begins with // Setup MongoDb backing database . Change the code to make the database connection configurable:  </p> <pre><code>// Setup MongoDb backing database\nconst MongoClient = require('mongodb').MongoClient\n// MongoDB credentials\nconst username = encodeURIComponent(process.env.MONGODB_USER || \"my-app_user\");\nconst password = encodeURIComponent(process.env.MONGODB_PASSWORD || \"ILoveTheList\");\n// MongoDB connection info\nconst mongoPort = process.env.MONGODB_PORT || 27017;\nconst mongoHost = process.env.MONGODB_HOST || 'localhost';\n// MongoDB connection string\nconst mongoURI = `mongodb://${username}:${password}@${mongoHost}:${mongoPort}/my-app`;\nconst mongoURISanitized = `mongodb://${username}:****@${mongoHost}:${mongoPort}/my-app`;\nconsole.log(\"MongoDB connection string %s\", mongoURISanitized);\nconst client = new MongoClient(mongoURI);\n</code></pre> <p>Edit the <code>Dockerfile</code> to include the admin job code in the server image. Add the following line after the other COPY directives for the server image:</p> <pre><code>...\nCOPY --from=build /usr/src/app/src/jobs .\n</code></pre> <p>Build and push the server image:</p> <pre><code>docker build -t &lt;Your Docker Hub username&gt;/my-app-server:0.0.5 --target server .\ndocker push &lt;Your Docker Hub username&gt;/my-app-server:0.0.5\n</code></pre> <p>Create a Kubernetes Job to run the admin job: Create <code>de-duplicate-job.yml</code>. Supply your Docker Hub username in the image tag:</p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: de-duplicate\nspec:\n  template:\n    spec:\n      containers:\n      - name: my-app-server\n        image: &lt;Your Docker Hub username&gt;/my-app-server:0.0.5\n        command: [\"node\", \"deDeuplicateJob.js\"]\n        env:\n        - name: MONGODB_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: my-app-config\n              key: mongodb.host\n        - name: MONGODB_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: my-app-config\n              key: mongodb.port\n        - name: MONGODB_USER\n          valueFrom:\n            secretKeyRef:\n              name: my-app-secure-config\n              key: mongodb.username\n        - name: MONGODB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: my-app-secure-config\n              key: mongodb.password\n      restartPolicy: Never\n  backoffLimit: 4\n</code></pre> <p>Run the Job:</p> <pre><code>kubectl apply -f de-duplicate-job.yml -n production\n</code></pre> <p>Check the Job status:</p> <pre><code>kubectl get jobs -n production\n</code></pre> <p>Get the name of the Job Pod:</p> <pre><code>kubectl get pods -n production\n</code></pre> <p>Use the Pod name to view the logs for the Job Pod:</p> <pre><code>kubectl logs &lt;Pod name&gt; -n production\n</code></pre>"},{"location":"k8s/#microk8s","title":"MicroK8s","text":"<p>On Raspberry Pi Note: Your boot parameters might be in <code>/boot/cmdline.txt</code>. Add these options at the end of the file, then <code>sudo reboot</code>.</p> <pre><code>cgroup_enable=cpuset cgroup_enable=memory cgroup_memory=1\n</code></pre> <p>For Raspberry Pi OS install <code>snap</code> first.</p> <pre><code>sudo apt update\nsudo apt install snapd\nsudo reboot\n# ...reconnect after reboot\nsudo snap install core\n</code></pre> <p>Then install MicroK8s.</p> <pre><code>pi@raspberrypi4:~ $ sudo snap install microk8s --classic\npi@raspberrypi4:~ $ microk8s status --wait-ready\npi@raspberrypi4:~ $ microk8s kubectl get all --all-namespaces\npi@raspberrypi4:~ $ microk8s enable dns dashboard registry hostpath-storage # or any other addons\npi@raspberrypi4:~ $ alias mkctl=\"microk8s kubectl\"\npi@raspberrypi4:~ $ alias mkhelm=\"microk8s helm\"\npi@raspberrypi4:~ $ mkctl create deployment nginx --image nginx\npi@raspberrypi4:~ $ mkctl expose deployment nginx --port 80 --target-port 80 --selector app=nginx --type ClustetIP --name nginx\npi@raspberrypi4:~ $ watch microk8s kubectl get all\npi@raspberrypi4:~ $ microk8s reset\npi@raspberrypi4:~ $ microk8s status\npi@raspberrypi4:~ $ microk8s stop # microk8s start\npi@raspberrypi4:~ $ microk8s kubectl version --output=yaml\n</code></pre> <p>You can update a snap package with <code>sudo snap refresh</code>.</p> <p>Configuration file. These are the arguments you can add regarding log rotation <code>--container-log-max-files</code> and <code>--container-log-max-size</code>. They have default values. More info.</p> <pre><code>cat /var/snap/microk8s/current/args/kubelet\n</code></pre>"},{"location":"k8s/#registry","title":"Registry","text":"<p>Registry doc</p> <pre><code>microk8s enable registry\n</code></pre> <p>The containerd daemon used by MicroK8s is configured to trust this insecure registry. To upload images we have to tag them with <code>localhost:32000/your-image</code> before pushing them.</p>"},{"location":"k8s/#microk8s-dashboard","title":"MicroK8s dashboard","text":"<p>If RBAC is not enabled access the dashboard using the token retrieved with:</p> <pre><code>microk8s kubectl describe secret -n kube-system microk8s-dashboard-token\n</code></pre> <p>Use this token in the https login UI of the <code>kubernetes-dashboard</code> service. In an RBAC enabled setup (<code>microk8s enable rbac</code>) you need to create a user with restricted permissions as shown here.</p> <p>To access remotely from anywhere with <code>port-forward</code>:</p> <pre><code>microk8s kubectl port-forward -n kube-system service/kubernetes-dashboard 10443:443 --address 0.0.0.0\n</code></pre> <p>You can then access the Dashboard with IP or hostname as in https://raspberrypi4.local:10443/</p>"},{"location":"k8s/#troubleshooting","title":"Troubleshooting","text":"<pre><code>microk8s inspect\n</code></pre> <p>MicroK8s might not recognize that cgroup memory is enabled but you can check with <code>cat /proc/cgroups</code>.</p>"},{"location":"k8s/#kubernetes-dashboard","title":"Kubernetes Dashboard","text":"<p>Documentation</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.6.1/aio/deploy/recommended.yaml\n</code></pre> <pre><code>cat &lt;&lt; EOF &gt; dashboard-adminuser.yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: admin-user\n  namespace: kubernetes-dashboard\n\n---\n\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: admin-user\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: admin-user\n  namespace: kubernetes-dashboard\nEOF\n</code></pre> <pre><code>kubectl apply -f dashboard-adminuser.yaml\n\nkubectl proxy\n# Kubectl will make Dashboard available at \n# http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/\n\nkubectl -n kubernetes-dashboard create token admin-user\n# Now copy the token and paste it into the Enter token field on the login screen. \n</code></pre>"},{"location":"mkdocs/","title":"MkDocs","text":"<p>For full documentation visit mkdocs.org.</p> <pre><code>pip install mkdocs python-markdown-math mkdocs-material\n</code></pre>"},{"location":"mkdocs/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"mkdocs/#default-project-layout","title":"Default project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"mkdocs/#themes","title":"Themes","text":"<p>Included: <code>mkdocs</code>, <code>readthedocs</code>. Third-party: <code>material</code>.</p>"},{"location":"mkdocs/#extensions","title":"Extensions","text":"<p>Documentation. Admonition types: <code>attention</code>, <code>caution</code>, <code>danger</code>, <code>error</code>, <code>hint</code>, <code>important</code>, <code>note</code>, <code>tip</code>, and <code>warning</code>.</p>"},{"location":"mkdocs/#mathjax-test","title":"MathJax Test","text":"<p>When , there are two solutions to  and they are  </p>"},{"location":"mysql/","title":"MySQL development","text":"<p>Install MySQL Community Edition  Use your OS package manager or download .dmg installer for macOS. Take note of the root password.</p> <p>Configure your PATH Add the mysql location to your PATH. Typically as part of your ~/.bash_profile</p> <pre><code>export PATH=/usr/local/mysql/bin:$PATH\n</code></pre> <p>Start the MySQL service On macOS</p> <pre><code>sudo launchctl load -F /Library/LaunchDaemons/com.oracle.oss.mysql.mysqld.plist\n</code></pre> <p>Verify it's running On macOS</p> <pre><code>sudo launchctl list | grep mysql\n</code></pre> <p>Connect to your MySQL instance Use MySQL Workbench or other client tool.</p> <p>To stop MySQL On macOS</p> <pre><code>sudo launchctl unload -F /Library/LaunchDaemons/com.oracle.oss.mysql.mysqld.plist\n</code></pre> <p>Exporting data If you need to export data you may need to disable or set a security setting. On macOS:</p> <p>View the variable using your SQL client. If it's NULL you will be restricted regarding file operations.</p> <pre><code>show variables like 'secure_file_priv';\n</code></pre> <p>Open the configuration file.</p> <pre><code>cd /Library/LaunchDaemons\nsudo nano com.oracle.oss.mysql.mysqld.plist\n</code></pre> <p>and set the --secure-file-priv to an empty string (to disable the restriction) or a dir of your choice.</p> <pre><code>&lt;key&gt;ProgramArguments&lt;/key&gt;\n    &lt;array&gt;\n        &lt;string&gt;--secure-file-priv=&lt;/string&gt;\n    &lt;/array&gt;\n</code></pre> <p>Then restart MySQL. Now you can export data:</p> <pre><code>SELECT  *\nINTO    OUTFILE 'your_file.csv'\n        FIELDS TERMINATED BY ',' \n        ENCLOSED BY '\"'\nFROM    `your_db`.`your_table`\n</code></pre> <p>You can find your exported data:</p> <pre><code>sudo find /usr/local/mysql/data -name your_file.csv\n</code></pre>"},{"location":"node/","title":"Node.js","text":"<p>Install</p> <p>macOS Download the Node.js installer from https://nodejs.org/en/download/ or</p> <pre><code>brew install node\n</code></pre> <p>Linux  </p> <pre><code>sudo apt install nodejs\nsudo apt install npm\n</code></pre> <p>[Optional] Install nvm from https://github.com/creationix/nvm to manage multiple versions of node and npm.  </p> <p>Windows Subsystem for Linux On WSL the recommended approach for installing a current version of Node.js is nvm.</p> <pre><code>touch ~/.bashrc\ncurl -o- https://raw.githubusercontent.com/creationix/nvm/v0.33.11/install.sh | bash\n# Reload your configuration\nsource ~/.bashrc\nnvm install node\n# Upgrade npm. You may need to run this twice on WSL.\nnpm install npm@latest -g\n</code></pre> <p>Update npm If you installed npm as part of node you may need to update npm.</p> <pre><code>sudo npm install -g npm\n</code></pre> <p>Initialize a node project by creating a package.json</p> <pre><code>npm init\n</code></pre> <p>Installing dependencies examples</p> <pre><code>sudo npm install --save ask-sdk moment\nsudo npm install --save-dev mocha chai eslint virtual-alexa\n</code></pre> <p>Troubleshooting mocha If you get an error running mocha tests e.g. <code>node_modules/.bin/mocha</code> not having execute permissions or mocha Error: Cannot find module './options' delete your node_modules folder and <code>npm install</code>.</p> <p>Set up ESLint with a configuration file</p> <pre><code>eslint --init\n# you may need to run it as:\n# sudo ./node_modules/.bin/eslint --init\n</code></pre>"},{"location":"package-mgmt/","title":"Package manager","text":""},{"location":"package-mgmt/#macos","title":"macOS","text":"<p>Get Homebrew, a package manager for macOS.</p> <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nbrew doctor\n</code></pre> <p>You can use a more detailed guide if needed.</p>"},{"location":"package-mgmt/#linux","title":"Linux","text":"<p>Depending on your distribution, you may have apt (Ubuntu), yum (Red Hat), or zypper (openSUSE).</p>"},{"location":"python/","title":"Python","text":"<p>Packaging Python Projects </p>"},{"location":"python/#install-python-the-pip-python-package-installer-and-setuptools","title":"Install Python, the pip Python package installer, and Setuptools","text":""},{"location":"python/#with-homebrew","title":"With Homebrew","text":"<p>Even if you already have Python on your OS it might be an outdated version. This installs python and pip. macOS</p> <pre><code>brew install python\npython3 --version\n</code></pre> <p>The python formulae install pip (as pip3) and Setuptools. Setuptools can be updated via pip3 and pip3 can be used to upgrade itself.</p> <pre><code>python3 -m pip install --upgrade setuptools\npython3 -m pip install --upgrade pip\n</code></pre> <p>Add the unversioned symlinks to your <code>$PATH</code> by adding this to your <code>.zshrc</code> file.</p> <pre><code>export PATH=\"$(brew --prefix)/opt/python/libexec/bin:$PATH\"\n</code></pre> <p>See the Homebrew Python documentation for details.  </p> <p><code>site-packages</code> is here. Example for python 3.10 on Homebrew</p> <pre><code>ls -al $(brew --prefix)/lib/python3.10/site-packages\n</code></pre>"},{"location":"python/#without-homebrew","title":"Without Homebrew","text":"<p>Linux (Ubuntu)</p> <pre><code>sudo apt install python3\n</code></pre> <p>macOS Download .pkg installer from pythong.org. Then get pip.</p> <pre><code>curl -o get-pip.py https://bootstrap.pypa.io/get-pip.py\nsudo python get-pip.py\n</code></pre> <p>Linux</p> <pre><code>sudo apt install python-pip\n</code></pre> <pre><code>pip install --upgrade pip setuptools\n</code></pre> <p>On Ubuntu, upgrading to pip 10 may break pip. If this happens you need to:</p> <pre><code>sudo nano /usr/bin/pip\n# change \"from pip import main\" to \"from pip._internal import main\"\n</code></pre>"},{"location":"python/#get-the-pep8-python-style-checker","title":"Get the pep8 python style checker","text":"<pre><code># On Linux you may need to use sudo.\npip install pep8\n</code></pre>"},{"location":"python/#anaconda","title":"Anaconda","text":"<p>For data science and machine learning. Anaconda is a distribution of the Python and R programming languages for scientific computing. See Getting Started.</p> <pre><code>brew install --cask anaconda\n</code></pre> <p>Installing in silent mode </p> <p>Add anaconda to your <code>PATH</code> on your <code>.zshrc</code> and <code>source ~/.zshrc</code> to apply the changes.</p> <pre><code>export PATH=\"$(brew --prefix)/anaconda3/bin:$PATH\"\n</code></pre> <p>To use zsh:</p> <pre><code>conda init zsh\n</code></pre> <p>If <code>conda init zsh</code> messed up the <code>PATH</code> in your <code>~/.zshrc</code> by adding the <code>condabin</code> directory instead of <code>bin</code> you can fix it with a symlink:</p> <pre><code>ln -sf $(brew --prefix)/anaconda3/bin/jupyter-lab $(brew --prefix)/anaconda3/condabin/jupyter-lab\nln -sf $(brew --prefix)/anaconda3/bin/jupyter $(brew --prefix)/anaconda3/condabin/jupyter\n</code></pre> <p>If you don't want to activate the <code>base</code> environment every time you open your terminal:</p> <pre><code>conda config --set auto_activate_base false\n</code></pre> <p>When creating a conda environment you can use optionally use a configuration file. You can also of course, save a config file from an existing environment to back it up.</p> <pre><code>conda env create -f myenv.yml\nconda activate myenv\n\nconda env list\nconda env remove --name ldm\n</code></pre>"},{"location":"python/#notebooks","title":"Notebooks","text":"<p>Fresh installs of Anaconda no longer include notebook extensions in their Jupyter installer. This means that the nb_conda libraries need to be added into your environment separately to access conda environments from your Jupyter notebook. Just run these to add them and you should be able to select your environment as a kernel within a Jupyter notebook.</p> <pre><code>conda activate &lt;myenv&gt;\nconda install ipykernel\nconda install nb_conda_kernels # or defaults::nb_conda_kernels\n</code></pre> <p>Now you can launch the new JupyterLab or the classic Jupyter Notebook.</p> <pre><code>jupyter-lab # or the classic \"jupyter notebook\"\n</code></pre>"},{"location":"rpi/","title":"Raspberry Pi","text":""},{"location":"rpi/#setup","title":"Setup","text":"<ol> <li>Write and preconfigure Raspberry Pi OS on the SD card using Raspberry Pi Imager (<code>brew install --cask raspberry-pi-imager</code>). Make sure you use the correct version of the OS (32-bit or 64-bit).     1.1. Change the default password for the <code>pi</code> user.     1.2. Enable SSH (password or ssh keys).     1.3. Configure WiFi if needed.     1.4. Take note of the hostname.  </li> <li>Insert the SD card in your Raspberry Pi and turn it on. </li> </ol> <p>If you didn't do so during setup, you can still generate and add an ssh key at any time. Example:</p> <pre><code># on your laptop\nssh-keygen\nssh-copy-id -i ~/.ssh/id_rsa pi@raspberrypi4.local\n</code></pre> <p>To remove password authentication:</p> <pre><code># on the Pi\nsudo nano /etc/ssh/sshd_config\n</code></pre> <p>and replace <code>#PasswordAuthentication yes</code> with <code>PasswordAuthentication no</code>. Test the validity of the config file and restart the service (or reboot).</p> <pre><code>sudo sshd -t\nsudo service sshd restart\nsudo service sshd status\n</code></pre>"},{"location":"rpi/#configuration","title":"Configuration","text":"<p>Find the IP of your Raspberry Pi using its hostname or find all devices on your network with <code>arp -a</code>.</p> <pre><code>arp raspberrypi4.local \n</code></pre> <p>SSH into it with the <code>pi</code> user and the IP address or hostname. Examples:</p> <pre><code>ssh pi@192.168.xxx.xxx\nssh pi@raspberrypi4.local\n# You'll be asked for the password if applicable\n</code></pre> <p>Configure it.</p> <pre><code>pi@raspberrypi4:~ $ sudo raspi-config\n# Go to Interface Options, VNC (for graphical remote access)\n# Tab to the Finish option and reboot.\n</code></pre> <p>Update it. <code>upgrade</code> is used to install available upgrades of all packages currently installed on the system. New packages will be installed if required to satisfy dependencies, but existing packages will never be removed. If an upgrade for a package requires the removal of an installed package the upgrade for this package isn't performed. <code>full-upgrade</code> performs the function of upgrade but will remove currently installed packages if this is needed to upgrade the system as a whole.</p> <pre><code>pi@raspberrypi4:~ $ sudo apt update # updates the package list\npi@raspberrypi4:~ $ sudo apt full-upgrade\n</code></pre>"},{"location":"rpi/#to-give-it-a-static-ip","title":"To give it a static IP","text":"<p>Find the IP adddress of your router. It's the address that appears after <code>default via</code>.</p> <pre><code>pi@raspberrypi4:~ $ ip r\ndefault via [IP]\n</code></pre> <p>Get the IP of your DNS server (it may or may not be your router)</p> <pre><code>pi@raspberrypi4:~ $ grep nameserver /etc/resolv.conf\n</code></pre> <p>Open this file:</p> <pre><code>nano /etc/dhcpcd.conf\n</code></pre> <p>and add/edit these lines at the end filling in the correct info.</p> <pre><code>interface [wlan0 for Wi-Fi or eth0 for Ethernet]\nstatic_routers=[ROUTER IP]\nstatic domain_name_servers=[DNS IP]\n[static or inform] ip_address=[STATIC IP ADDRESS YOU WANT]/24\n</code></pre> <p><code>inform</code> means that the Raspberry Pi will attempt to get the IP address you requested, but if it's not available, it will choose another. If you use <code>static</code>, it will have no IP v4 address at all if the requested one is in use.  </p> <p>Save the file and <code>sudo reboot</code>. From now on, upon each boot, the Pi will attempt to obtain the static ip address you requested.  </p> <p>You could also set your router to manually assign the static IP to the Raspberry Pi under its DHCP settings e.g. LAN, DHCP server.</p>"},{"location":"rpi/#to-set-it-up-as-a-dns-server","title":"To set it up as a DNS server","text":"<ol> <li>Install and configure a DNS Server e.g. DNSmasq or Pi-Hole on the Raspberry Pi.</li> <li>Change your router\u2019s DNS settings to point to the Raspberry Pi. Log in to your router's admin interface and look for DNS e.g. in LAN, DHCP Server. Set the primary DNS server to the IP of your Raspberry Pi and make sure it's the only DNS server. The Raspberry Pi will handle upstream DNS services.</li> </ol>"},{"location":"rpi/#remote-gui-access","title":"Remote GUI access","text":"<p>Now you'll need a VNC viewer on your laptop to connect to the Raspberry Pi using the graphical interface.</p> <pre><code>brew install --cask vnc-viewer\n</code></pre> <p>Apparently, on Raspberry Pi pip does not download from the python package index (PyPi), it downloads from PiWheels. PiWheels wheels do not come with pygame's dependencies that are bundled in normal releases.</p> <p>Install Pygame dependencies and Pygame.</p> <pre><code>pi@raspberrypi4:~ $ sudo apt install libvorbisenc2 libwayland-server0 libxi6 libfluidsynth2 libgbm1 libxkbcommon0 libopus0 libwayland-cursor0 libsndfile1 libwayland-client0 libportmidi0 libvorbis0a libopusfile0 libmpg123-0 libflac8 libxcursor1 libxinerama1 libasyncns0 libxrandr2 libdrm2 libpulse0 libxfixes3 libvorbisfile3 libmodplug1 libxrender1 libsdl2-2.0-0 libxxf86vm1 libwayland-egl1 libsdl2-ttf-2.0-0 libsdl2-image-2.0-0 libjack0 libsdl2-mixer-2.0-0 libinstpatch-1.0-2 libxss1 libogg0\npi@raspberrypi4:~ $ sudo pip3 install pygame\n</code></pre> <p>Check that the installation worked by running one of its demos</p> <pre><code>pi@raspberrypi4:~ $ python3 -m pygame.examples.aliens\n</code></pre>"},{"location":"rpi/#copy-files","title":"Copy files","text":"<p>Copy files between Pi and local machine. On local machine run:</p> <pre><code>scp -r pi@raspberrypi2.local:/home/pi/Documents/ ~/Documents/pidocs\n</code></pre>"},{"location":"rpi/#find-info-about-your-pi","title":"Find info about your Pi","text":"<p>32 or 64-bit kernel?</p> <pre><code>getconf LONG_BIT\n# or check machine's hardware name: armv7l is 32-bit and aarch64 is 64-bit\npi@raspberrypi4:~ $ uname -m\n</code></pre> <p>See OS version</p> <pre><code>cat /etc/os-release\n</code></pre> <p>Architecture   If the following returns a <code>Tag_ABI_VFP_args</code> tag of <code>VFP registers</code>, it's an <code>armhf</code> (<code>arm</code>) system. A blank output means <code>armel</code> (<code>arm/v6</code>).</p> <pre><code>pi@raspberrypi2:~ $ readelf -A /proc/self/exe | grep Tag_ABI_VFP_args\n</code></pre> <p>Or check the architecture with:</p> <pre><code>hostnamectl\n</code></pre> <p>You can find info about the hardware like ports, pins, RAM, SoC, connectivity, etc. with:</p> <pre><code>pi@raspberrypi4:~ $ pinout\n</code></pre>"},{"location":"rpi/#learn-about-electronics","title":"Learn about electronics","text":"<p>I've added some sample code from the MagPi Essentials book. Sample code</p>"},{"location":"rpi/#gpio-header","title":"GPIO Header","text":""},{"location":"shell/","title":"Shell","text":""},{"location":"shell/#bash","title":"Bash","text":"<p>If you want to use bash</p> <pre><code>brew install bash # get the latest version of bash\nchsh -s $(which bash)\nnano ~/.bash_profile # and paste from sample dot file\n</code></pre>"},{"location":"shell/#zsh","title":"Zsh","text":"<pre><code>brew install zsh # get the latest version\n\n# You may need to add the Homebrew version of Zsh to the list on /etc/shells\nsudo nano /etc/shells # add the output of \"which zsh\" to the top of the list, save the file.\n\nwhere zsh # you might see both the shell that came with your Mac and the latest from Homebrew\nwhich zsh # make sure this returns the one from Homebrew.\n\nchsh -s $(which zsh)\n# Install Oh My Zsh, a framework for managing your Zsh configuration with plugins and themes\nsh -c \"$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"\n</code></pre> <p>Oh My Zsh Add any built-in plugins you need to your <code>~/.zshrc</code></p> <pre><code>plugins=(git macos python)\n</code></pre> <p>You can also add custom plugins by cloning the repo into the plugins directory or with Homebrew. </p> <p>This plugin auto suggests previous commands. </p> <pre><code>git clone https://github.com/zsh-users/zsh-autosuggestions $ZSH_CUSTOM/plugins/zsh-autosuggestions\n# add zsh-autosuggestions to plugins list in ~/.zshrc and:\nsource ~/.zshrc\n</code></pre> <p>This plugin zsh-syntax-highlighting highlights valid commands green and invalid ones red so you don't have to test the command to see if it will work.</p> <pre><code>git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting\n</code></pre> <p>Or just use Homebrew:</p> <pre><code>brew install zsh-syntax-highlighting\n# To activate the syntax highlighting, add the following at the end of your ~/.zshrc:\nsource $(brew --prefix)/share/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh\n</code></pre> <p>Some Oh My Zsh themes like Spaceship have requirements like the Powerline Fonts. Get them with:</p> <pre><code>git clone https://github.com/powerline/fonts.git --depth=1\n./fonts/install.sh\nrm -rf fonts\n</code></pre> <p>You can also install fonts with Homebrew by adding the fonts repository. Fira Code is a good one for programming and computer science.</p> <pre><code>brew tap homebrew/cask-fonts\nbrew install --cask font-fira-code\n</code></pre> <p>Install your Oh My Zsh theme e.g. Spaceship</p> <pre><code>brew install spaceship\n# If the theme is not copied to your themes folder, sim link from the Homebrew dir to your custom themes folder e.g.\nln -sf $(brew --prefix)/Cellar/spaceship/4.4.1/spaceship.zsh $ZSH_CUSTOM/themes/spaceship.zsh-theme\n\ntouch ~/.spaceshiprc.zsh\n</code></pre> <p>Any time you edit your zsh configuration file you can reload it to apply changes.</p> <pre><code>source ~/.zshrc\n</code></pre>"},{"location":"shell/#terminal-replacement","title":"Terminal replacement","text":"<p>You can use iTerm2</p> <pre><code>brew install --cask iterm2\n</code></pre> <p>After you have installed the font(s) required by your Oh My Zsh theme set your iTerm preferences like default shell and font. Verify that you're using the shell you want. In the output of the <code>env</code> command look for something like <code>zsh=/opt/homebrew/bin/zsh</code>.</p> <p>Install iTerm 2 color schemes</p> <pre><code>git clone https://github.com/mbadolato/iTerm2-Color-Schemes.git\ncd iTerm2-Color-Schemes\n\n# Import all color schemes\ntools/import-scheme.sh schemes/*\n</code></pre> <p>Restart iTerm 2 (need to quit iTerm 2 to reload the configuration file). iTerm2 &gt; Preferences &gt; Profile &gt; Colors &gt; Color Presets </p>"},{"location":"utilities/","title":"Install utilities","text":"<p>These will vary for each person. Some examples on a Mac:  </p> <pre><code>brew install wget\nbrew install gcc\nbrew install jq\nbrew install kompose # translate Docker Compose files to Kubernetes resources\nbrew install helm\nbrew install --cask docker\nbrew install --cask drawio # Diagrams (UML, AWS, etc.)\nbrew install --cask 1password\nbrew install --cask nordvpn\nbrew install --cask google-chrome # or chromium\nbrew install --cask firefox\nbrew install --cask visual-studio-code # or vscodium \nbrew install --cask kindle\nbrew install --cask authy\nbrew install --cask teamviewer\nbrew install --cask raspberry-pi-imager\nbrew install --cask balenaetcher # to make bootable media\nbrew install --cask zoom\nbrew install --cask spotify\nbrew install --cask whatsapp\nbrew install --cask messenger # Facebook messenger\nbrew install --cask rectangle # to snap windows\nbrew install --cask avast-security # if it fails, download from website\nbrew install --cask handbrake # if you need to rip DVDs\nbrew install --cask virtualbox # currently Intel Macs only. ARM installer in beta.\nbrew install --cask multipass # run Ubuntu VMs in a local mini-cloud.\nbrew install --cask steam # if you want games\nbrew tap homebrew/cask-drivers # add repository of drivers\nbrew install logitech-options # driver for Logitech mouse\n</code></pre> <p>On macOS you can also install gcc by installing the xcode developer tools if you're not using Homebrew.</p> <pre><code>xcode-select -v # check if tools are installed\nxcode-select --install\n</code></pre>"}]}