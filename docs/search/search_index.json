{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"My notes","text":"<p>Select a section from the sidebar.  </p>"},{"location":"automated/","title":"Automated setup","text":""},{"location":"automated/#macos","title":"macOS","text":"<p>Get the Homebrew package manager. <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nbrew doctor\n</code></pre> You can use a more detailed guide if needed.</p> <p>Install chezmoi to manage your dotfiles.  <pre><code>brew install chezmoi\n</code></pre></p> <p>On your current machine, backup your configuration, edit as needed, and push to a GitHub repo. Add mas to your Brewfile to automate App Store installs. <pre><code>chezmoi init\nchezmoi add ~/.gitconfig\nchezmoi add ~/.zshrc\nchezmoi add ~/.spaceshiprc.zsh\n\ncd ~/.local/share/chezmoi\nbrew bundle dump # generates your Brewfile\n# add and commit everything, then\ngh repo create dotfiles --public -d \"My dotfiles\" -y\ngit remote add origin git@github.com:$GITHUB_USERNAME/dotfiles.git\ngit branch -M main\ngit push -u origin main\n</code></pre></p> <p>On your new machine <pre><code>chezmoi init --apply $GITHUB_USERNAME\ngit remote -v # is it https? Change to ssh\ngit remote remove origin\ngit remote add origin git@github.com:$GITHUB_USERNAME/dotfiles.git\n# To base your work on an upstream branch that already exists at the remote, you may need to retrieve it.\ngit fetch\ngit branch -u origin/main # branch 'main' set up to track 'origin/main'.\n\nbrew bundle --file=\"~/.local/share/chezmoi/Brewfile\" # At any time\nchezmoi update -v\nmas list\nmas upgrade\n</code></pre></p> <p>Follow the Shell instructions that are not covered by this <code>chezmoi</code>/<code>brew bundle</code> automation. </p>"},{"location":"aws/","title":"Amazon Web Services","text":""},{"location":"aws/#aws-cli","title":"AWS CLI","text":""},{"location":"aws/#install-the-aws-command-line-interface","title":"Install the AWS command line interface","text":"<pre><code>curl \"https://awscli.amazonaws.com/AWSCLIV2.pkg\" -o \"AWSCLIV2.pkg\"\nsudo installer -pkg AWSCLIV2.pkg -target /\n</code></pre>"},{"location":"aws/#check-if-the-aws-cli-is-installed-correctly","title":"Check if the AWS CLI is installed correctly","text":"<pre><code># you may need to reload your shell's rc file first.\naws --version\n</code></pre>"},{"location":"aws/#configure-the-aws-cli","title":"Configure the AWS CLI","text":"<pre><code>aws configure\n</code></pre>"},{"location":"aws/#alexa-skills-kit-ask-cli","title":"Alexa Skills Kit (ASK) CLI","text":"<p>Set up credentials for AWS account Quick Start ASK CLI</p>"},{"location":"aws/#aws-amplify","title":"AWS Amplify","text":"<pre><code>npm install -g @aws-amplify/cli\namplify configure\n</code></pre>"},{"location":"docker/","title":"Docker","text":""},{"location":"docker/#install","title":"Install","text":""},{"location":"docker/#macos","title":"macOS","text":"<pre><code>brew install --cask docker\n</code></pre> <p>Important</p> <p>You must use the --cask version. Otherwise only the client is included and can't run the Docker daemon. Then open the Docker app and grant privileged access when asked. Only then will you be able to use <code>docker</code>.</p>"},{"location":"docker/#linux-raspberry-pi-os","title":"Linux / Raspberry Pi OS:","text":"<p>If you just ran <code>apt upgrade</code> on your Raspberry Pi, reboot before installing Docker. Follow the appropriate installation method.  </p> <p>If you don't want to have to prefix commands with <code>sudo</code> add your user to the <code>docker</code> group. This is equivalent to giving that user root privileges. <pre><code>cat /etc/group | grep docker # see if docker group exists\nsudo usermod -aG docker $USER\n</code></pre></p> <p>Follow Post-installation steps to configure log rotation.  </p>"},{"location":"docker/#use","title":"Use","text":""},{"location":"docker/#architecture","title":"Architecture","text":"<p>Supported Architectures Platform specifiers </p> Value Normalized Examples aarch64 arm64 Apple M1/M2, Raspberry Pi 4 armhf arm Raspberry Pi 2 armel arm/v6 i386 386 x86_64 amd64 Intel (Default) x86-64 amd64 Intel (Default) <p>Docker Desktop for Apple silicon can run (or build) an Intel image using emulation. The <code>--platform</code> option sets the platform if server is multi-platform capable.  </p> <p>Macbook M1/M2 is based on ARM. The Docker host is detected as linux/arm64/v8 by Docker.  </p> <p>On a Macbook M1/M2 running a native arm64 linux image instead of an amd64 (x86-64 Intel) image with emulation: When installing apps on the container either install the package for the ARM platform if they provide one, or run an amd64 docker image (Docker will use emulation). </p> <p>See Docker host info. You need to use the real field names (not display names) so we'll grab the info in json to get the real field names and use <code>jq</code> to display it nicely. Then we'll render them using Go templates.</p> <pre><code>docker info --format '{{json .}}' | jq .\n\ndocker info --format \"{{.Plugins.Volume}}\"\ndocker info --format \"{{.OSType}} - {{.Architecture}} CPUs: {{.NCPU}}\"\n</code></pre> <p>With the default images, Docker Desktop for Apple silicon warns us the requested image's platform (linux/amd64) is different from the detected host platform (linux/arm64/v8) and no specific platform was requested. <code>--platform linux/amd64</code> is the default and the same as  <code>--platform linux/x86_64</code>. It runs (or builds) an Intel image. <code>--platform linux/arm64</code> runs (or builds) an aarch64 (arm64) image. Architecture used by Appple M1/M2.  </p> <pre><code>docker run -it --name container1 debian # WARNING. uname -m -&gt; x86_64 (amd64)\ndocker run -it --platform linux/amd64 --name container1 debian # NO warning. uname -m -&gt; x86_64 (amd64). On Mac it emulates Intel.\ndocker run -it --platform linux/arm64 --name container1 debian # NO warning. uname -m -&gt; aarch64 (arm64). Mac native.\n\n# On a Mac M2 you can also use this image\ndocker run -it --name mycontainer arm64v8/debian bash # NO warning. uname -m -&gt; aarch64 (arm64). Mac native.\n</code></pre>"},{"location":"docker/#examples","title":"Examples","text":"<p>Dockerfile instruction to keep a container running <pre><code>CMD tail -f /dev/null\n</code></pre></p> <p>Running containers <pre><code># sanity check\ndocker version\ndocker --help\n\n# \"docker run\" = \"docker container run\"\n\ndocker run --name container1 debian # created and Exited\ndocker run -it --name container2 debian # created and Up. Interactive tty\ndocker run --detach --name container3 debian # created and Exited\ndocker run -it --detach --name container4 debian # created and Up. Running in background\n\ndocker stop container4 # Exited\ndocker restart container4 # Up\ndocker attach container4 # Interactive tty\n\n# To have it removed automatically when it's stopped \ndocker run -it --rm --name mycontainer debian # created and Up. Interactive tty. \n# Run bash in a new container with interactive tty, based on debian image.\ndocker run -it --name mycontainer debian bash\n# Get the IDs of all stopped containers. Options: all, quiet (only IDs), filter.\ndocker ps -aq -f status=exited\n</code></pre></p> <p>Working with images and volumes <pre><code># Show all images (including intermediate images)\ndocker image ls -a\n\n# Build an image from a Dockerfile and give it a name (or name:tag).\ndocker build -f mydockerfile -t santisbon/myimage .\ndocker builder prune -a     # Remove all unused build cache, not just dangling ones\ndocker image rm santisbon/myimage\n\n# Volumes\ndocker volume create my-vol\ndocker volume ls\ndocker volume inspect my-vol\ndocker volume prune\ndocker volume rm my-vol\n\n# Start a container from an image.\ndocker run \\\n--name mycontainer \\\n--hostname mycontainer \\\n--mount source=my-vol,target=/data \\\nsantisbon/myimage\n\n# Get rid of all stopped containers. Options: remove **anonymous volumes** associated with the container.\ndocker rm -v $(docker ps -aq -f status=exited)\n# You might want to make an alias:\nalias drm='docker rm -v $(docker ps -aq -f status=exited)'\n# or\nalias drm='sudo docker rm -v $(sudo docker ps -aq -f status=exited)'\n\n# on macOS the volume mountpoint (/var/lib/docker/volumes/) is in the VM that Docker Desktop uses to provide the Linux environment.\n# You can read it through a container given extended privileges. \n# Use: \n# - the \"host\" PID namespace, \n# - an image e.g. debian, \n# - the nsenter command to run a program (sh) in a different namespace\n# - the target process with PID 1\n# - enter following namspaces of the target process: mount, UTS, network, IPC.\ndocker run -it --privileged --pid=host debian nsenter -t 1 -m -u -n -i sh\n</code></pre></p> <p>If you want to build multi-platform docker images: <pre><code>docker buildx create --name mybuilder --driver docker-container --bootstrap\ndocker buildx use mybuilder\n\ndocker buildx inspect\ndocker buildx ls\n</code></pre></p> <p>Docker Compose. Multi-container deployments in a compose.yaml file. Using Compose in Production </p> <pre><code>docker compose -p mastodon-bot-project up --detach\n# or\ndocker compose -p mastodon-bot-project create\ndocker compose -p mastodon-bot-project start\n\n# connect to a running container\ndocker exec -it bot-app bash\n\ndocker compose -p mastodon-bot-project down\n# or\ndocker compose -p mastodon-bot-project stop\n</code></pre> <p>Kubernetes</p> <pre><code># Convert the Docker Compose file to k8s files\nkompose --file compose.yaml convert\n\n# Make sure the container image is available in a repository. \n# You can build it with `docker build` or `docker compose create` and push it to a public repository.\ndocker image push user/repo\n# multiple -f filenames or a folder \nkubectl apply -f ./k8s\nkubectl get pods\nkubectl delete -f ./k8s\n</code></pre>"},{"location":"editor/","title":"Code Editor (VSCode/VSCodium)","text":"<p>Tip</p> <p>If you get the message VSCodium cannot be opened because Apple cannot check it for malicious software: Press the mouse Right Button on the .app and then hold the Option key while clicking Open (needed on first launch only).</p> <p>Disable telemetry in: Code -&gt; Settings -&gt; Telemetry Settings -&gt; Telemetry Level -&gt; off.</p> <p>Useful extensions:  </p> <ul> <li>Better TOML</li> <li>Compare Folders</li> <li>Draw.io Integration</li> <li>Remote - SSH</li> </ul>"},{"location":"git/","title":"Git","text":""},{"location":"git/#install","title":"Install","text":"<p>macOS <pre><code>brew install git\ngit --version\n\n# GitHub CLI\nbrew install gh \ngh auth login\ngh config set editor \"codium -w\" # or code, nano, etc\n</code></pre> GitHub CLI reference.</p> <p>Linux <pre><code>sudo yum install git # or git-all\ngit --version\n</code></pre></p>"},{"location":"git/#configure","title":"Configure","text":"<pre><code>git config --global core.editor 'codium --wait'\n\ngit config --global diff.tool codium\ngit config --global difftool.codium.cmd 'codium --wait --diff $LOCAL $REMOTE'\n\ngit config --global merge.tool codium\ngit config --global mergetool.codium.cmd 'codium --wait $MERGED'\n\n# or\ncodium ~/.gitconfig # and paste from sample dot file. Also: codium, vscode, nano\n</code></pre> <p>Then you can <code>git difftool main feature-branch</code>.  </p> <p>If using AWS CodeCommit do this after configuring the AWS CLI: <pre><code>git config --global credential.helper '!aws codecommit credential-helper $@'\ngit config --global credential.UseHttpPath true\n</code></pre> Troubleshooting CodeCommit</p> <p>To create a squash function: <pre><code>git config --global alias.squash-all '!f(){ git reset $(git commit-tree \"HEAD^{tree}\" \"$@\");};f'\n</code></pre></p> <p>Note</p> <ul> <li>Git allows you to escape to a shell (like bash or zsh) using the ! (bang). Learn more.</li> <li><code>commit-tree</code> creates a new commit object based on the provided tree object and emits the new commit object id on stdout.</li> <li><code>tree</code> objects correspond to UNIX directory entries. </li> <li><code>reset</code> resets current HEAD to the specified state e.g a commit.</li> <li>The <code>master^{tree}</code> syntax specifies the tree object that is pointed to by the last commit on your <code>master</code> branch. So <code>HEAD^{tree}</code> is the tree object pointed to by the last commit on your current branch.</li> <li>If you\u2019re using ZSH, the <code>^</code> character is used for globbing, so you have to enclose the whole expression in quotes: <code>\"HEAD^{tree}\"</code>.</li> </ul> <p>Then just run: <pre><code>git squash-all -m \"a brand new start\"\ngit push -f\n</code></pre></p> <p>Change a remote repo's URL <pre><code>git remote set-url origin git@github.com:OWNER/REPOSITORY.git\n</code></pre> Or edit <code>.git/config</code> and change the URLs there.</p>"},{"location":"git/#set-up-git-autocompletion-bash","title":"Set up git autocompletion (bash)","text":"<pre><code>cd ~\ncurl -OL https://github.com/git/git/raw/master/contrib/completion/git-completion.bash\nmv ~/git-completion.bash ~/.git-completion.bash\n</code></pre>"},{"location":"git/#set-up-ssh-key","title":"Set up SSH key","text":"<ol> <li>Check for existing keys.</li> <li>Generate a new SSH key and add it to ssh-agent. You may need to set permissions to your key file with <code>chmod 600</code>.</li> <li>Add the public key to your GitHub account.</li> </ol> <p>Tip</p> <p>If the terminal is no longer authenticating you: <pre><code>git remote -v # is it https or ssh? Should be ssh\ngit remote remove origin\ngit remote add origin git@github.com:user/repo.git\n</code></pre></p> <p>Still having issues? Start the <code>ssh-agent</code> in the background and add your SSH private key to it. <pre><code># is it running?\nps -ax | grep ssh-agent\n# which identities have been added?\nssh-add -l\n\n# start the agent and add your identity\neval \"$(ssh-agent -s)\"\nssh-add --apple-use-keychain ~/.ssh/id_ed25519\n</code></pre></p> <p>If you need to add your public key to Github again copy and paste it on your Settings page on Github: <pre><code>pbcopy &lt; ~/.ssh/id_ed25519.pub\n</code></pre></p>"},{"location":"install-os/","title":"Do a clean install of the OS","text":"<p>On macOS download the new version from System Preferences, Software Update (or the Mac App Store) and create the bootable media with: <pre><code>sudo /Applications/Install\\ macOS\\ Sonoma.app/Contents/Resources/createinstallmedia --volume /Volumes/MyVolume/\n</code></pre> using the volume that matches the name of the external drive you are using.  </p> <p>If you have an Apple silicon Mac here's how to install macOS from a bootable installer:</p> <ol> <li>Plug in your bootable media.</li> <li>Turn off your Mac.</li> <li>Depending on your CPU:<ol> <li>If you have an Apple silicon Mac press Power to turn on the Mac - but keep it pressed until you see the startup options window including your bootable volume.</li> <li>If you have an Intel-powered Mac press and hold Option while the Mac starts up - keep pressing the key until you see a screen showing the bootable volume.</li> </ol> </li> </ol>"},{"location":"k8s/","title":"Kubernetes","text":""},{"location":"k8s/#kubernetes-essentials","title":"Kubernetes Essentials","text":"<p>Interactive Diagram Working with K8s objects Log rotation </p> <p>In order for Kubernetes to pull your container image you need to first push it to an image repository like Docker Hub. To avoid storing your Docker Hub password unencrypted in $HOME/.docker/config.json when you <code>docker login</code> to your account, use a credentials store. A helper program lets you interact with such a keychain or external store. </p> <p>If you're doing this on your laptop with Docker Desktop, it already provides a store. Otherwise, use one of the stores supported by the <code>docker-credential-helper</code>. Now <code>docker login</code> on your terminal or on the Docker Desktop app.  </p>"},{"location":"k8s/#designing-applications-for-kubernetes","title":"Designing Applications for Kubernetes","text":"<p>Implements the 12-Factor App Design Methodology and based on a Cloud Guru course. It uses Ubuntu 20.04 Focal Fossa LTS and the Calico network plugin instead of Flannel. Example with 1 control plane node and 2 worker nodes.</p>"},{"location":"k8s/#prerequisites","title":"Prerequisites","text":""},{"location":"k8s/#building-a-kubernetes-cluster","title":"Building a Kubernetes Cluster","text":"<p>Reference Installing kubeadm Creating a cluster with kubeadm </p> <p>Warning</p> <p><code>kubeadm</code> sometimes doesn't work with the latest and greatest version of Docker right away.</p> <p><code>kubeadm</code> simplifies the process of setting up a K8s cluster. <code>containerd</code> manages the complete container lifecycle of its host system, from image transfer and storage to container execution and supervision to low-level storage to network attachments. <code>kubelet</code> handles running containers on a node. <code>kubectl</code> is a tool for managing the cluster.  </p> <p>If you wish, you can set an appropriate hostname for each node. On the control plane node: <pre><code>sudo hostnamectl set-hostname k8s-control\n</code></pre></p> <p>On the first worker node: <pre><code>sudo hostnamectl set-hostname k8s-worker1\n</code></pre> On the second worker node: <pre><code>sudo hostnamectl set-hostname k8s-worker2\n</code></pre></p> <p>On all nodes, set up the hosts file to enable all the nodes to reach each other using these hostnames. <pre><code>sudo nano /etc/hosts\n</code></pre></p> <p>On all nodes, add the following at the end of the file. You will need to supply the actual private IP address for each node. <pre><code>&lt;control plane node private IP&gt; k8s-control\n&lt;worker node 1 private IP&gt; k8s-worker1\n&lt;worker node 2 private IP&gt; k8s-worker2\n</code></pre></p> <p>Log out of all three servers and log back in to see these changes take effect.  </p> <p>On all nodes, set up containerd. You will need to load some kernel modules and modify some system settings as part of this process. <pre><code># Enable them when the server start up\ncat &lt;&lt; EOF | sudo tee /etc/modules-load.d/containerd.conf\noverlay\nbr_netfilter\nEOF\n\n# Enable them right now without having to restart the server\nsudo modprobe overlay\nsudo modprobe br_netfilter\n\n# Add network configurations K8s will need\ncat &lt;&lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf\nnet.bridge.bridge-nf-call-iptables = 1\nnet.ipv4.ip_forward = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nEOF\n\n# Apply them immediately\nsudo sysctl --system\n</code></pre></p> <p>Install and configure containerd. <pre><code>sudo apt-get update &amp;&amp; sudo apt-get install -y containerd\nsudo mkdir -p /etc/containerd\n# Generate the contents of a default config file and save it\nsudo containerd config default | sudo tee /etc/containerd/config.toml\n# Restart containerd to make sure it's using that configuration\nsudo systemctl restart containerd\n</code></pre></p> <p>On all nodes, disable swap. <pre><code>sudo swapoff -a\n</code></pre> On all nodes, install kubeadm, kubelet, and kubectl. <pre><code># Some required packages\nsudo apt-get update &amp;&amp; sudo apt-get install -y apt-transport-https curl\n# Set up the package repo for K8s packages. Download the key for the repo and add it\ncurl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n# Configure the repo\ncat &lt;&lt; EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list\ndeb https://apt.kubernetes.io/ kubernetes-xenial main\nEOF\nsudo apt-get update\n# Install the K8s packages. Make sure the versions for all 3 are the same.\nsudo apt-get install -y kubelet=1.24.0-00 kubeadm=1.24.0-00 kubectl=1.24.0-00\n# Make sure they're not automatically upgraded. Have manual control over when to update K8s\nsudo apt-mark hold kubelet kubeadm kubectl\n</code></pre></p> <p>On the control plane node only, initialize the cluster and set up kubectl access. <pre><code>sudo kubeadm init --pod-network-cidr 192.168.0.0/16 --kubernetes-version 1.24.0\n# Config File to authenticate and interact with the cluster with kubectl commands\n# These are in the output of the previous step\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n</code></pre></p> <p>Verify the cluster is working. It will be in Not Ready status because we haven't configured the networking plugin. <pre><code>kubectl get nodes\n</code></pre></p> <p>Install the Calico network add-on. <pre><code>kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml\n</code></pre></p> <p>Get the join command (this command is also printed during kubeadm init . Feel free to simply copy it from there). <pre><code>kubeadm token create --print-join-command\n</code></pre></p> <p>Copy the join command from the control plane node. Run it on each worker node as root (i.e. with sudo ). <pre><code>sudo kubeadm join ...\n</code></pre></p> <p>On the control plane node, verify all nodes in your cluster are ready. Note that it may take a few moments for all of the nodes to enter the READY state. <pre><code>kubectl get nodes\n</code></pre></p>"},{"location":"k8s/#installing-docker","title":"Installing Docker","text":"<p>Reference Install Docker Engine on Ubuntu Docker credentials store  to avoid storing your Docker Hub password unencrypted in <code>$HOME/.docker/config.json</code> when you <code>docker login</code> and <code>docker push</code> your images.  </p> <p>On the system that will build Docker images from source code e.g. a CI server, install and configure Docker. For simplicity we'll use the control plane server just so we don't have to create another server for this exercise.  </p> <p>Create a docker group. Users in this group will have permission to use Docker on the system: <pre><code>sudo groupadd docker\n</code></pre></p> <p>Install required packages. Note: Some of these packages may already be present on the system, but including them here will not cause any problems: <pre><code>sudo apt-get update &amp;&amp; sudo apt-get install -y apt-transport-https ca-certificates curl gnupg lsb-release\n</code></pre></p> <p>Set up the Docker GPG key and package repository: <pre><code>curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\necho \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n</code></pre></p> <p>Install the Docker Engine: <pre><code>sudo apt-get update &amp;&amp; sudo apt-get install -y docker-ce docker-ce-cli\n# Type N (default) or enter to keep your current containerd configuration\n</code></pre></p> <p>Test the Docker setup: <pre><code>sudo docker version\n</code></pre></p> <p>Add cloud_user to the docker group in order to give cloud_user access to use Docker: <pre><code>sudo usermod -aG docker cloud_user\n</code></pre> Log out of the server and log back in. Test your setup: <pre><code>docker version\n</code></pre></p>"},{"location":"k8s/#i-codebase","title":"I. Codebase","text":"<p>One codebase tracked in revision control, many deploys </p> <p>Keep your codebase in a version control system like Git. There's a one-to-one relationship between the codebase and the app. If there are multiple codebases it's not an app - it's a distributed system where each component is an app. Factor shared code into libraries which can be included through the dependency manager. A deploy is a running instance of the app.</p> <p>Your apps can be implemented as containers/pods, built and deployed independently of other apps.</p>"},{"location":"k8s/#ii-dependencies","title":"II. Dependencies","text":"<p>Explicitly declare and isolate dependencies</p> <p>Don't rely on implicit existence of system-wide packages. Declare all dependencies, completely and exactly, via a dependency declaration manifest. With containers, your app and its dependencies are deployed as a unit, allowing it to run almost anywhere - a desktop, a traditional IT infrastructure, or the cloud.</p>"},{"location":"k8s/#iii-config","title":"III. Config","text":"<p>Store config in the environment</p>"},{"location":"k8s/#configmaps-and-secrets","title":"ConfigMaps and Secrets","text":"<p>Reference </p> <p>Encrypting Secret Data at Rest ConfigMaps Secrets </p> <p>Create a production Namespace: <pre><code>kubectl create namespace production\n</code></pre></p> <p>Get base64-encoded strings for a db username and password: <pre><code>echo -n my_user | base64\necho -n my_password | base64\n</code></pre></p> <p>Example: Create a <code>ConfigMap</code> and <code>Secret</code> to configure the backing service connection information for the app, including the base64-encoded credentials: <pre><code>cat &gt; my-app-config.yml &lt;&lt;End-of-message \napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: my-app-config\ndata:\n  mongodb.host: \"my-app-mongodb\"\n  mongodb.port: \"27017\"\n\n---\n\napiVersion: v1\nkind: Secret\nmetadata:\n  name: my-app-secure-config\ntype: Opaque\ndata:\n  mongodb.username: dWxvZV91c2Vy\n  mongodb.password: SUxvdmVUaGVMaXN0\n\nEnd-of-message\n</code></pre></p> <pre><code>kubectl apply -f my-app-config.yml -n production\n</code></pre> <p>Create a temporary <code>Pod</code> to test the configuration setup. Note that you need to supply your Docker Hub username as part of the image name in this file. This passes configuration data in env variables but you could also do it in files that will show up on the containers filesystem. <pre><code>cat &gt; test-pod.yml &lt;&lt;End-of-message\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: test-pod\nspec:\n  containers:\n  - name: my-app-server\n    image: &lt;YOUR_DOCKER_HUB_USERNAME&gt;/my-app-server:0.0.1\n    ports:\n    - name: web\n      containerPort: 3001\n      protocol: TCP\n    env:\n    - name: MONGODB_HOST\n      valueFrom:\n        configMapKeyRef:\n          name: my-app-config\n          key: mongodb.host\n    - name: MONGODB_PORT\n      valueFrom:\n        configMapKeyRef:\n          name: my-app-config\n          key: mongodb.port\n    - name: MONGODB_USER\n      valueFrom:\n        secretKeyRef:\n          name: my-app-secure-config\n          key: mongodb.username\n    - name: MONGODB_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: my-app-secure-config\n          key: mongodb.password\n\nEnd-of-message\n</code></pre></p> <pre><code>kubectl apply -f test-pod.yml -n production\n</code></pre> <p>Check the logs to verify the config data is being passed to the container: <pre><code>kubectl logs test-pod -n production\n</code></pre></p> <p>Clean up the test pod: <pre><code>kubectl delete pod test-pod -n production --force\n</code></pre></p>"},{"location":"k8s/#iv-backing-services","title":"IV. Backing services","text":"<p>Treat backing services as attached resources</p> <p>Makes no distinction between local and third party services. To the app, both are attached resources, accessed via a URL or other locator/credentials stored in the config. You should be able to swap out a local MySQL database with one managed by a third party (such as Amazon RDS) without any changes to the app\u2019s code.</p> <p>By implementing attached resources as containers/pods you achieve loose coupling between those resources and the deploy they are attached to.</p>"},{"location":"k8s/#v-build-release-run","title":"V. Build, release, run","text":"<p>Strictly separate build and run stages</p>"},{"location":"k8s/#build-release-run-with-docker-and-deployments","title":"Build, Release, Run with Docker and Deployments","text":"<p>Reference Labels and Selectors Deployments </p> <p>Example: After you <code>docker build</code> and <code>docker push</code> your image to a repository, create a deployment file for your app. The <code>selector</code> selects pods that have the specified label name and value. <code>template</code> is the pod template. This example puts 2 containers in the same pod for simplicity but in the real world you'll want separate deployments to scale them independently.</p> <pre><code>cat &gt; my-app.yml &lt;&lt;End-of-message\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: my-app-config\ndata:\n  mongodb.host: \"my-app-mongodb\"\n  mongodb.port: \"27017\"\n  .env: |\n    REACT_APP_API_PORT=\"30081\"\n\n---\n\napiVersion: v1\nkind: Secret\nmetadata:\n  name: my-app-secure-config\ntype: Opaque\ndata:\n  mongodb.username: dWxvZV91c2Vy\n  mongodb.password: SUxvdmVUaGVMaXN0\n\n---\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-app-svc\nspec:\n  type: NodePort\n  selector:\n    app: my-app\n  ports:\n  - name: frontend\n    protocol: TCP\n    port: 30080\n    nodePort: 30080\n    targetPort: 5000\n  - name: server\n    protocol: TCP\n    port: 30081\n    nodePort: 30081\n    targetPort: 3001\n\n---\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: my-app\n  template:\n    metadata:\n      labels:\n        app: my-app\n    spec:\n      containers:\n      - name: my-app-server\n        image: &lt;Your Docker Hub username&gt;/my-app-server:0.0.1\n        ports:\n        - name: web\n          containerPort: 3001\n          protocol: TCP\n        env:\n        - name: MONGODB_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: my-app-config\n              key: mongodb.host\n        - name: MONGODB_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: my-app-config\n              key: mongodb.port\n        - name: MONGODB_USER\n          valueFrom:\n            secretKeyRef:\n              name: my-app-secure-config\n              key: mongodb.username\n        - name: MONGODB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: my-app-secure-config\n              key: mongodb.password\n      - name: my-app-frontend\n        image: &lt;Your Docker Hub username&gt;/my-app-frontend:0.0.1\n        ports:\n        - name: web\n          containerPort: 5000\n          protocol: TCP\n        volumeMounts:\n        - name: frontend-config\n          mountPath: /usr/src/app/.env\n          subPath: .env\n          readOnly: true\n      volumes:\n      - name: frontend-config\n        configMap:\n          name: my-app-config\n          items:\n          - key: .env\n            path: .env\nEnd-of-message\n</code></pre> <p>Deploy the app. <pre><code>kubectl apply -f my-app.yml -n production\n</code></pre></p> <p>Create a new container image version to test the rollout process: <pre><code>docker tag &lt;Your Docker Hub username&gt;/my-app-frontend:0.0.1 &lt;Your Docker Hub username&gt;/my-app-frontend:0.0.2\ndocker push &lt;Your Docker Hub username&gt;/my-app-frontend:0.0.2\n</code></pre></p> <p>Edit the app manifest <code>my-app.yml</code> to use the <code>0.0.2</code> image version and then: <pre><code>kubectl apply -f my-app.yml -n production\n</code></pre></p> <p>Get the list of Pods to see the new version rollout: <pre><code>kubectl get pods -n production\n</code></pre></p>"},{"location":"k8s/#vi-processes","title":"VI. Processes","text":"<p>Execute the app as one or more stateless processes</p>"},{"location":"k8s/#processes-with-stateless-containers","title":"Processes with stateless containers","text":"<p>Reference Security Context </p> <p>Edit the app deployment <code>my-app.yml</code>. In the deployment Pod spec, add a new <code>emptyDir</code> volume under <code>volumes</code>: <pre><code>volumes:\n- name: added-items-log\nemptyDir: {}\n...\n</code></pre></p> <p>Mount the volume to the server container: <pre><code>containers:\n...\n- name: my-app-server\n...\nvolumeMounts:\n- name: added-items-log\nmountPath: /usr/src/app/added_items.log\nsubPath: added_items.log\nreadOnly: false\n...\n</code></pre></p> <p>Make the container file system read only: <pre><code>containers:\n...\n- name: my-app-server\nsecurityContext:\nreadOnlyRootFilesystem: true\n...\n</code></pre></p> <p>Deploy the changes: <pre><code>kubectl apply -f my-app.yml -n production\n</code></pre></p>"},{"location":"k8s/#persistent-volumes","title":"Persistent Volumes","text":"<p>Reference Persistent Volumes (PV) <code>local</code> PV </p> <p>Create a <code>StorageClass</code> that supports volume expansion as <code>localdisk-sc.yml</code> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\nname: localdisk\nprovisioner: kubernetes.io/no-provisioner\nallowVolumeExpansion: true\n</code></pre> <pre><code>kubectl create -f localdisk-sc.yml\n</code></pre></p> <p><code>persistentVolumeReclaimPolicy</code> says how storage can be reused when the volume's associated claims are deleted.  </p> <ul> <li>Retain: Keeps all data. An admin must manually clean up and prepare the resource for reuse.</li> <li>Recycle: Automatically deletes all data, allowing  the volume to be reused.</li> <li>Delete: Deletes underlying storage resource automatically (applies to cloud only).  </li> </ul> <p><code>accessModes</code> can be:</p> <ul> <li>ReadWriteOnce: The volume can be mounted as read-write by a single node. Still can allow multiple pods to access the volume when the pods are running on the same node.  </li> <li>ReadOnlyMany: Can be mounted as read-only by many nodes.</li> <li>ReadWriteMany: Can be mounted as read-write by many nodes.</li> <li>ReadWriteOncePod: Can be mounted as read-write by a single Pod. Use ReadWriteOncePod access mode if you want to ensure that only one pod across whole cluster can read that PVC or write to it. This is only supported for CSI volumes.</li> </ul> <p>Create a <code>PersistentVolume</code> in <code>my-pv.yml</code>. <pre><code>kind: PersistentVolume\napiVersion: v1\nmetadata:\nname: my-pv\nspec:\nstorageClassName: localdisk\npersistentVolumeReclaimPolicy: Recycle\ncapacity:\nstorage: 1Gi\naccessModes:\n- ReadWriteOnce\nhostPath:\npath: /var/output\n</code></pre> <pre><code>kubectl create -f my-pv.yml\n</code></pre></p> <p>Check the status of the <code>PersistentVolume</code>. <pre><code>kubectl get pv\n</code></pre></p> <p>Create a <code>PersistentVolumeClaim</code> that will bind to the <code>PersistentVolume</code> as <code>my-pvc.yml</code> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\nname: my-pvc\nspec:\nstorageClassName: localdisk\naccessModes:\n- ReadWriteOnce\nresources:\nrequests:\nstorage: 100Mi\n</code></pre> <pre><code>kubectl create -f my-pvc.yml\n</code></pre></p> <p>Check the status of the <code>PersistentVolume</code> and <code>PersistentVolumeClaim</code> to verify that they have been bound. <pre><code>kubectl get pv\nkubectl get pvc\n</code></pre></p> <p>Create a <code>Pod</code> that uses the <code>PersistentVolumeClaim</code> as <code>pv-pod.yml</code>. <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\nname: pv-pod\nspec:\nrestartPolicy: Never\ncontainers:\n- name: busybox\nimage: busybox\ncommand: ['sh', '-c', 'echo Success! &gt; /output/success.txt']\nvolumeMounts:\n- name: pv-storage\nmountPath: /output\nvolumes:\n- name: pv-storage\npersistentVolumeClaim:\nclaimName: my-pvc\n</code></pre> <pre><code>kubectl create -f pv-pod.yml\n</code></pre></p> <p>Expand the <code>PersistentVolumeClaim</code> and record the process. <pre><code>kubectl edit pvc my-pvc --record\n</code></pre> <pre><code>...\nspec:\n...\nresources:\nrequests:\nstorage: 200Mi\n</code></pre></p> <p>Delete the <code>Pod</code> and the <code>PersistentVolumeClaim</code>. <pre><code>kubectl delete pod pv-pod\nkubectl delete pvc my-pvc\n</code></pre></p> <p>Check the status of the <code>PersistentVolume</code> to verify that it has been successfully recycled and is available again. <pre><code>kubectl get pv\n</code></pre></p>"},{"location":"k8s/#vii-port-binding","title":"VII. Port binding","text":"<p>Export services via port binding</p>"},{"location":"k8s/#port-binding-with-pods","title":"Port Binding with Pods","text":"<p>Reference Cluster Networking </p> <p>Note</p> <p>Challenge: Only 1 process can listen on a port per host. So how do all apps on the host use a unique port?</p> <ul> <li>In K8s, each pod has its own network namespace and cluster IP address.  </li> <li>That IP address is unique within the cluster even if there are multiple worker nodes in the cluster. That means ports only need to be unique within each pod.  </li> <li>Two pods can listen on the same port because they each have their own unique IP address within the cluster network.  </li> <li>The pods can communicate across nodes simply using the unique IPs.</li> </ul> <p>Get a list of Pods in the production namespace: <pre><code>kubectl get pods -n production -o wide\n</code></pre></p> <p>Copy the name of the IP address of the application Pod. Example: Use the IP address to make a request to the port on the Pod that serves the frontend content: <pre><code>curl &lt;Pod Cluster IP address&gt;:5000\n</code></pre></p>"},{"location":"k8s/#viii-concurrency","title":"VIII. Concurrency","text":"<p>Scale out via the process model</p>"},{"location":"k8s/#concurrency-with-containers-and-scaling","title":"Concurrency with Containers and Scaling","text":"<p>Reference Deployments </p> <p>By using <code>services</code> to manage access to the app, the service automaticaly picks up the additional pods created during scaling and route traffic to those pods. When you have <code>services</code> alongside <code>deployments</code> you can dynamically change the number of replicas that you have and K8s will take care of everything.</p> <p>Edit the application deployment <code>my-app.yml</code>. Change the number of replicas to 3: <pre><code>...\nreplicas: 3\n</code></pre></p> <p>Apply the changes: <pre><code>kubectl apply -f my-app.yml -n production\n</code></pre></p> <p>Get a list of Pods: <pre><code>kubectl get pods -n production\n</code></pre></p> <p>Scale the deployment up again in <code>my-app.yml</code>. Change the number of replicas to 5: <pre><code>...\nreplicas: 5\n</code></pre></p> <p>Apply the changes: <pre><code>kubectl apply -f my-app .yml -n production\n</code></pre></p> <p>Get a list of Pods: <pre><code>kubectl get pods -n production\n</code></pre></p>"},{"location":"k8s/#ix-disposability","title":"IX. Disposability","text":"<p>Maximize robustness with fast startup and graceful shutdown</p>"},{"location":"k8s/#disposability-with-stateless-containers","title":"Disposability with Stateless Containers","text":"<p>Reference Security Context Pod Lifecycle </p> <p>Deployments can be used to maintain a specified number of running replicas automatically replacing pods that fail or are deleted.</p> <p>Get a list of Pods: <pre><code>kubectl get pods -n production\n</code></pre></p> <p>Locate one of the Pods from the <code>my-app</code> deployment and copy the Pod's name. Delete the Pod using the Pod name: <pre><code>kubectl delete pod &lt;Pod name&gt; -n production\n</code></pre></p> <p>Get the list of Pods again. You will notice that the deployment is automatically creating a new Pod to replace the one that was deleted: <pre><code>kubectl get pods -n production\n</code></pre></p>"},{"location":"k8s/#x-devprod-parity","title":"X. Dev/prod parity","text":"<p>Keep development, staging, and production as similar as possible</p>"},{"location":"k8s/#devprod-parity-with-namespaces","title":"Dev/Prod Parity with Namespaces","text":"<p>Reference Namespaces </p> <p>K8s namespaces allow us to have multiple environments in the same cluster. A namespace is like a virtual cluster.</p> <p>Create a new <code>namespace</code>: <pre><code>kubectl create namespace dev\n</code></pre></p> <p>Make a copy of the my-app app YAML: <pre><code>cp my-app.yml my-app-dev.yml\n</code></pre></p> <p><code>NodePort</code> <code>services</code> need to be unique within the cluster. We need to choose unique ports so dev doesn't conflict with production. Edit the <code>my-app-svc</code> service in the <code>my-app-dev.yml</code> file to select different <code>nodePort</code>s. You will also need to edit the <code>my-app-config</code> <code>ConfigMap</code> to reflect the new port.  Set the <code>nodePort</code>s on the service: <pre><code>apiVersion: v1\nkind: Service\nmetadata:\nname: my-app-svc\nspec:\ntype: NodePort\nselector:\napp: my-app\nports:\n- name: frontend\nprotocol: TCP\nport: 30082\nnodePort: 30082\ntargetPort: 5000\n- name: server\nprotocol: TCP\nport: 30083\nnodePort: 30083\ntargetPort: 3001\n</code></pre></p> <p>Update the configured port in the <code>ConfigMap</code>: <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\nname: my-app-config\ndata:\nmongodb.host: \"my-app-mongodb\"\nmongodb.port: \"27017\"\n.env: |\nREACT_APP_API_PORT=\"30083\"\n</code></pre></p> <p>Deploy the backing service setup in the new namespace: <pre><code>kubectl apply -f k8s-my-app-mongodb.yml -n dev\nkubectl apply -f my-app-mongodb.yml -n dev\n</code></pre></p> <p>Deploy the app in the new namespace: <pre><code>kubectl apply -f my-app-dev.yml -n dev\n</code></pre> Check the status of the Pods: <pre><code>kubectl get pods -n dev\n</code></pre></p> <p>Once all the Pods are up and running, you should be able to test the dev environment in a browser at <pre><code>&lt;Control Plane Node Public IP&gt;:30082\n</code></pre></p>"},{"location":"k8s/#xi-logs","title":"XI. Logs","text":"<p>Treat logs as event streams</p>"},{"location":"k8s/#logs-with-k8s-container-logging","title":"Logs with K8s Container Logging","text":"<p>Reference Logging Architecture Kubectl cheatsheet </p> <p>K8s captures log data written to stdout by containers. We can use the K8s API, <code>kubectl logs</code> or external tools to interact with container logs.  </p> <p>Edit the source code for the server e.g. <code>src/server/index.js</code>. There is a log function that writes to a file. Change this function to simply write log data to the console: <pre><code>log = function(data) {\nconsole.log(data);\n}\n</code></pre></p> <p>Build a new server image because we changed the source code: <pre><code>docker build -t &lt;Your Docker Hub username&gt;/my-app-server:0.0.4 --target server .\n</code></pre></p> <p>Push the image: <pre><code>docker push &lt;Your Docker Hub username&gt;/my-app-server:0.0.4\n</code></pre></p> <p>Deploy the new code. Edit <code>my-app.yml</code>. Change the image version for the server to the new image: <pre><code>containers:\n- name: my-app-server\nimage: &lt;Your Docker Hub username&gt;/my-app-server:0.0.4\n</code></pre></p> <pre><code>kubectl apply -f my-app.yml -n production\n</code></pre> <p>Get a list of Pods: <pre><code>kubectl get pods -n production\n</code></pre></p> <p>Copy the name of one of the my-app deployment Pods and view its logs specifying the pod, namespace, and container. <pre><code>kubectl logs &lt;Pod name&gt; -n production -c my-app-server\n</code></pre></p>"},{"location":"k8s/#xii-admin-processes","title":"XII. Admin processes","text":"<p>Run admin/management tasks as one-off processes</p>"},{"location":"k8s/#admin-processes-with-jobs","title":"Admin Processes with Jobs","text":"<p>Reference Jobs </p> <p>A <code>Job</code> e.g. a database migration runs a container until its execution completes. Jobs handle re-trying execution if it fails. Jobs have a <code>restartPolicy</code> of <code>Never</code> because once they complete they don't run again.  This example adds the administrative job to the server image but you could package it into its own image.  </p> <p>Edit the source code for the admin process in e.g. <code>src/jobs/deDeuplicateJob.js</code>. Locate the block of code that begins with // Setup MongoDb backing database . Change the code to make the database connection configurable:  </p> <pre><code>// Setup MongoDb backing database\nconst MongoClient = require('mongodb').MongoClient\n// MongoDB credentials\nconst username = encodeURIComponent(process.env.MONGODB_USER || \"my-app_user\");\nconst password = encodeURIComponent(process.env.MONGODB_PASSWORD || \"ILoveTheList\");\n// MongoDB connection info\nconst mongoPort = process.env.MONGODB_PORT || 27017;\nconst mongoHost = process.env.MONGODB_HOST || 'localhost';\n// MongoDB connection string\nconst mongoURI = `mongodb://${username}:${password}@${mongoHost}:${mongoPort}/my-app`;\nconst mongoURISanitized = `mongodb://${username}:****@${mongoHost}:${mongoPort}/my-app`;\nconsole.log(\"MongoDB connection string %s\", mongoURISanitized);\nconst client = new MongoClient(mongoURI);\n</code></pre> <p>Edit the <code>Dockerfile</code> to include the admin job code in the server image. Add the following line after the other COPY directives for the server image: <pre><code>...\nCOPY --from=build /usr/src/app/src/jobs .\n</code></pre></p> <p>Build and push the server image: <pre><code>docker build -t &lt;Your Docker Hub username&gt;/my-app-server:0.0.5 --target server .\ndocker push &lt;Your Docker Hub username&gt;/my-app-server:0.0.5\n</code></pre></p> <p>Create a Kubernetes Job to run the admin job: Create <code>de-duplicate-job.yml</code>. Supply your Docker Hub username in the image tag: <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\nname: de-duplicate\nspec:\ntemplate:\nspec:\ncontainers:\n- name: my-app-server\nimage: &lt;Your Docker Hub username&gt;/my-app-server:0.0.5\ncommand: [\"node\", \"deDeuplicateJob.js\"]\nenv:\n- name: MONGODB_HOST\nvalueFrom:\nconfigMapKeyRef:\nname: my-app-config\nkey: mongodb.host\n- name: MONGODB_PORT\nvalueFrom:\nconfigMapKeyRef:\nname: my-app-config\nkey: mongodb.port\n- name: MONGODB_USER\nvalueFrom:\nsecretKeyRef:\nname: my-app-secure-config\nkey: mongodb.username\n- name: MONGODB_PASSWORD\nvalueFrom:\nsecretKeyRef:\nname: my-app-secure-config\nkey: mongodb.password\nrestartPolicy: Never\nbackoffLimit: 4\n</code></pre></p> <p>Run the Job: <pre><code>kubectl apply -f de-duplicate-job.yml -n production\n</code></pre></p> <p>Check the Job status: <pre><code>kubectl get jobs -n production\n</code></pre> Get the name of the Job Pod: <pre><code>kubectl get pods -n production\n</code></pre></p> <p>Use the Pod name to view the logs for the Job Pod: <pre><code>kubectl logs &lt;Pod name&gt; -n production\n</code></pre></p>"},{"location":"k8s/#kubernetes-dashboard","title":"Kubernetes Dashboard","text":"<p><pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.6.1/aio/deploy/recommended.yaml\n</code></pre> <pre><code>cat &lt;&lt; EOF &gt; dashboard-adminuser.yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: admin-user\n  namespace: kubernetes-dashboard\n\n---\n\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: admin-user\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: admin-user\n  namespace: kubernetes-dashboard\nEOF\n</code></pre></p> <pre><code>kubectl apply -f dashboard-adminuser.yaml\n\nkubectl proxy\n# Kubectl will make Dashboard available at \n# http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/\n\nkubectl -n kubernetes-dashboard create token admin-user\n# Now copy the token and paste it into the Enter token field on the login screen. \n</code></pre>"},{"location":"k8s/#microk8s","title":"MicroK8s","text":"<p>Suitable for Raspberry Pi and other development boards.  </p>"},{"location":"k8s/#setup","title":"Setup","text":"<p>Attention</p> <p>MicroK8s is not available for 32-bit architectures like <code>armhf</code>(<code>arm/v7</code>), only on 64-bit architectures like <code>arm64</code> and <code>amd64</code>.</p> <p>Attention</p> <p>On Raspberry Pi your boot parameters file might be in <code>/boot/cmdline.txt</code> or in <code>/boot/firmware/cmdline.txt</code>. Find it with <code>sudo find /boot -name cmdline.txt</code>.  </p> <p>On Raspberry Pi you need to enable c-groups so the kubelet will work out of the box. Add these options at the end of the file, then <code>sudo reboot</code>. Some users report needing <code>cgroup_enable=cpuset</code> as well but try adding only these: cmdline.txt<pre><code>cgroup_enable=memory cgroup_memory=1\n</code></pre></p> <p>Attention</p> <p>On Orange Pi boards these parameters are handled in <code>/boot/boot.cmd</code> by checking: <code>if test \"${docker_optimizations}\" = \"on\"</code>.  </p> <p>Don't edit this file, instead <code>sudo nano /boot/orangepiEnv.txt</code> and set <code>docker_optimizations</code> to <code>on</code>.</p> <p>If your image doesn't already include it, install <code>snap</code>. On your Pi<pre><code>sudo apt update\nsudo apt install snapd\nsudo reboot\n# ...reconnect after reboot\nsudo snap install core\n</code></pre></p> <p>Then install MicroK8s. On your Pi<pre><code>sudo snap install microk8s --classic\n</code></pre></p> <p>To run commands without <code>sudo</code> add the user to the <code>microk8s</code> group. Example: On your Pi<pre><code>sudo usermod -a -G microk8s pi\nsudo chown -R pi ~/.kube\nnewgrp microk8s\n</code></pre></p> <p>Usage: On your Pi<pre><code>microk8s status --wait-ready\nmicrok8s kubectl get all --all-namespaces\n# default addons are: dns ha-cluster helm helm3. You can enable more\n# but preferably enable them one by one\nmicrok8s enable dashboard ingress metrics-server alias mkctl=\"microk8s kubectl\"\nalias mkhelm=\"microk8s helm\"\nmkctl version --output=yaml\nwatch microk8s kubectl get all\nmicrok8s reset\nmicrok8s status\nmicrok8s stop # microk8s start\n</code></pre></p> <p>You can update a snap package with <code>sudo snap refresh</code>.</p> <p>Configuration file. These are the arguments you can add regarding log rotation <code>--container-log-max-files</code> and <code>--container-log-max-size</code>. They have default values. More info. <pre><code>cat /var/snap/microk8s/current/args/kubelet\n</code></pre></p>"},{"location":"k8s/#registry","title":"Registry","text":"<p>Registry doc <pre><code>microk8s enable registry\n</code></pre> The containerd daemon used by MicroK8s is configured to trust this insecure registry. To upload images we have to tag them with <code>localhost:32000/your-image</code> before pushing them.</p>"},{"location":"k8s/#dashboard","title":"Dashboard","text":"<p>If RBAC is not enabled access the dashboard using the token retrieved with: <pre><code>microk8s kubectl describe secret -n kube-system microk8s-dashboard-token\n</code></pre> Use this token in the https login UI of the <code>kubernetes-dashboard</code> service. In an RBAC enabled setup (<code>microk8s enable rbac</code>) you need to create a user with restricted permissions as shown here.</p> <p>To access remotely from outside the cluster:</p>"},{"location":"k8s/#option-a-port-forward","title":"Option A: <code>port-forward</code>","text":"<p>Note: <code>kubectl port-forward</code> does not return. To type other commands, you'll need to open another terminal. <pre><code>microk8s kubectl port-forward -n kube-system service/kubernetes-dashboard 10443:443 --address 0.0.0.0\n</code></pre> You can then access the Dashboard with IP or hostname and the forwarded port as in <code>https://raspberrypi4.local:10443/</code></p>"},{"location":"k8s/#option-b-nodeport","title":"Option B: <code>NodePort</code>","text":"<p>Make the dashboard service a <code>NodePort</code> service by changing <code>type: ClusterIP</code> to <code>type: NodePort</code>. <pre><code>KUBE_EDITOR=nano microk8s kubectl -n kube-system edit service kubernetes-dashboard\n# If using vim:\n# Enter insert mode with i\n# Enter command mode with esc \n# :q! to abort changes \n# :wq to save and exit\n\nmicrok8s kubectl -n kube-system get service kubernetes-dashboard\n</code></pre> You can then access the Dashboard with IP or hostname and the automatically assigned <code>NodePort</code> as in <code>https://raspberrypi4.local:30772</code></p>"},{"location":"k8s/#troubleshooting","title":"Troubleshooting","text":"<p><pre><code>microk8s inspect\n</code></pre> MicroK8s might not recognize that cgroup memory is enabled but you can check with <code>cat /proc/cgroups</code>.</p>"},{"location":"k8s/#clustering","title":"Clustering","text":"<ol> <li>Assign static IPs to all the nodes. </li> <li>Edit <code>/etc/hosts</code> on each node with the IP and hostnames of the other nodes so they can resolve during the join process.</li> <li>Follow the instructions on https://microk8s.io/docs/clustering</li> </ol>"},{"location":"k8s/#openebs","title":"OpenEBS","text":"<p>Prerequisite knowledge: Huge Pages NVMe over Fabrics (NVMe-oF)</p> <p>Enable: Mayastor</p>"},{"location":"k8s/#minio","title":"MinIO","text":"<p>Concepts: https://min.io/docs/minio/linux/operations/concepts.html</p> <p>Enable: MinIO</p> <p>Filesystem type to use: https://min.io/docs/minio/linux/operations/install-deploy-manage/deploy-minio-single-node-multi-drive.html</p> <p>Check status for tenant named microk8s <pre><code>sudo microk8s kubectl-minio tenant status microk8s\n</code></pre></p> <p>Check all endpoints <pre><code>microk8s kubectl get endpoints -A\n</code></pre></p>"},{"location":"k8s/#helm","title":"Helm","text":"<p>You can package your charts and publish them on a repository, which can be any HTTP server. Use the name of the folder containing your <code>Chart.yaml</code> file. <pre><code>helm package ./mychart\n</code></pre> Move the package to your HTTP server e.g. a GitHub Pages site based on a <code>charts</code> repo. <pre><code>mv *.tgz ~/github/USER/charts\nhelm repo index ~/github/USER/charts\n# [commit and push to GitHub]\n</code></pre></p> <p>You can then list your charts repo on the CNCF Artifact Hub. You can search for charts on the website or via the command line: <pre><code>helm search hub [KEYWORD] --list-repo-url --max-col-width [uint] -o [table|json|yaml]\n</code></pre> For example, to search for any <code>speedtest</code> charts in the default <code>hub</code> (Artifact Hub): <pre><code>helm search hub speedtest --list-repo-url --max-col-width 55 -o table\n</code></pre></p>"},{"location":"k8s/#referencing-services","title":"Referencing services","text":"<p>\"Normal\" (not headless) Services are assigned DNS A and/or AAAA records, depending on the IP family or families of the Service, with a name of the form: <code>my-svc.my-namespace.svc.cluster-domain.example</code>. This resolves to the cluster IP of the Service.</p> <p>Headless Services (without a cluster IP) Services are also assigned DNS A and/or AAAA records, with a name of the form: <code>my-svc.my-namespace.svc.cluster-domain.example</code>. Unlike normal Services, this resolves to the set of IPs of all of the Pods selected by the Service. Clients are expected to consume the set or else use standard round-robin selection from the set.  </p> <p>By default, a client Pod's DNS search list includes the Pod's own namespace and the cluster's default domain. A pod in another namespace can resolve either: <code>&lt;service-name&gt;.&lt;namespace&gt;</code> or <code>&lt;service-name&gt;.&lt;namespace&gt;.svc.cluster.local</code></p>"},{"location":"k8s/#about-bitnami-charts","title":"About Bitnami charts","text":"<p>They create a default fully qualified app name <code>common.names.fullname</code>. The RabbitMQ <code>Service</code> uses it as its name. They truncate at 63 chars because some Kubernetes name fields are limited to this (by the DNS naming spec). If the release name contains the chart name it will be used as a full name. By default it's <code>&lt;Release Name&gt;-&lt;Chart Name&gt;</code>.</p>"},{"location":"mkdocs/","title":"MkDocs","text":"<p>For full documentation visit mkdocs.org.</p> <pre><code>pip install mkdocs python-markdown-math mkdocs-material\n</code></pre>"},{"location":"mkdocs/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"mkdocs/#default-project-layout","title":"Default project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"mkdocs/#themes","title":"Themes","text":"<p>Included: <code>mkdocs</code>, <code>readthedocs</code>. Third-party: <code>material</code>.</p>"},{"location":"mkdocs/#extensions","title":"Extensions","text":""},{"location":"mkdocs/#admonition","title":"Admonition","text":"<ul> <li>Pencil: <code>attention</code>, <code>caution</code>, <code>error</code>, <code>hint</code>, <code>important</code>,<code>note</code>.</li> <li>Fire: <code>tip</code>.</li> <li>Exclamation mark: <code>warning</code>.</li> <li>Lightning bolt: <code>danger</code>.</li> </ul> <p>The text must be indented. <pre><code>!!! attention\n    Text goes here.\n</code></pre></p> <p>Attention</p> <p>Text goes here.</p> <p>Tip</p> <p>Text goes here.</p> <p>Warning</p> <p>Text goes here.</p> <p>Danger</p> <p>Text goes here.</p>"},{"location":"mkdocs/#keys","title":"Keys","text":"<p><pre><code>++command+shift+n++\n</code></pre> Cmd+Shift+N</p>"},{"location":"mkdocs/#mathjax","title":"MathJax","text":"<p><pre><code>When \\(a \\ne 0\\), there are two solutions to \\(ax^2 + bx + c = 0\\) and they are\n$$x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}$$\n</code></pre> When , there are two solutions to  and they are  </p>"},{"location":"mysql/","title":"MySQL development","text":"<p>Install MySQL Community Edition  Use your OS package manager or download .dmg installer for macOS. Take note of the root password.</p> <p>Configure your PATH Add the mysql location to your PATH. Typically as part of your ~/.bash_profile <pre><code>export PATH=/usr/local/mysql/bin:$PATH\n</code></pre></p> <p>Start the MySQL service On macOS <pre><code>sudo launchctl load -F /Library/LaunchDaemons/com.oracle.oss.mysql.mysqld.plist\n</code></pre></p> <p>Verify it's running On macOS <pre><code>sudo launchctl list | grep mysql\n</code></pre></p> <p>Connect to your MySQL instance Use MySQL Workbench or other client tool.</p> <p>To stop MySQL On macOS <pre><code>sudo launchctl unload -F /Library/LaunchDaemons/com.oracle.oss.mysql.mysqld.plist\n</code></pre></p> <p>Exporting data If you need to export data you may need to disable or set a security setting. On macOS:</p> <p>View the variable using your SQL client. If it's NULL you will be restricted regarding file operations. <pre><code>show variables like 'secure_file_priv';\n</code></pre></p> <p>Open the configuration file. <pre><code>cd /Library/LaunchDaemons\nsudo nano com.oracle.oss.mysql.mysqld.plist\n</code></pre> and set the <code>--secure-file-priv</code> to an empty string (to disable the restriction) or a dir of your choice. <pre><code>&lt;key&gt;ProgramArguments&lt;/key&gt;\n&lt;array&gt;\n&lt;string&gt;--secure-file-priv=&lt;/string&gt;\n&lt;/array&gt;\n</code></pre></p> <p>Then restart MySQL. Now you can export data: <pre><code>SELECT  *\nINTO    OUTFILE 'your_file.csv'\nFIELDS TERMINATED BY ',' ENCLOSED BY '\"'\nFROM    `your_db`.`your_table`\n</code></pre></p> <p>You can find your exported data: <pre><code>sudo find /usr/local/mysql/data -name your_file.csv\n</code></pre></p>"},{"location":"node/","title":"Node.js","text":"<p>Install</p> <p>macOS Download the Node.js installer from https://nodejs.org/en/download/ or <pre><code>brew install node\n</code></pre></p> <p>Linux <pre><code>sudo apt install nodejs\nsudo apt install npm\n</code></pre></p> <p>Tip</p> <p>Install nvm to manage multiple versions of node and npm.  </p> <p>Windows Subsystem for Linux On WSL the recommended approach for installing a current version of Node.js is nvm. <pre><code>touch ~/.bashrc\ncurl -o- https://raw.githubusercontent.com/creationix/nvm/v0.33.11/install.sh | bash\n# Reload your configuration\nsource ~/.bashrc\nnvm install node\n# Upgrade npm. You may need to run this twice on WSL.\nnpm install npm@latest -g\n</code></pre></p> <p>Update npm If you installed npm as part of node you may need to update npm. <pre><code>sudo npm install -g npm\n</code></pre></p> <p>Initialize a node project by creating a package.json <pre><code>npm init\n</code></pre></p> <p>Installing dependencies examples <pre><code>sudo npm install --save ask-sdk moment\nsudo npm install --save-dev mocha chai eslint virtual-alexa\n</code></pre> Troubleshooting mocha If you get an error running mocha tests e.g. <code>node_modules/.bin/mocha</code> not having execute permissions or mocha Error: Cannot find module './options' delete your <code>node_modules</code> folder and <code>npm install</code>.</p> <p>Set up ESLint with a configuration file <pre><code>eslint --init\n# you may need to run it as:\n# sudo ./node_modules/.bin/eslint --init\n</code></pre></p>"},{"location":"package-mgmt/","title":"Package manager","text":""},{"location":"package-mgmt/#macos","title":"macOS","text":"<p>Get Homebrew, a package manager for macOS. <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nbrew doctor\n</code></pre> You can use a more detailed guide if needed.</p>"},{"location":"package-mgmt/#linux","title":"Linux","text":"<p>Depending on your distribution, you may have apt (Ubuntu), yum (Red Hat), or zypper (openSUSE).</p> <p>Many apps use a different distribution method like Snap or Flatpak.</p>"},{"location":"package-mgmt/#snap","title":"Snap","text":"<p>If you need to install a snap and your distro doens't have snap pre-installed e.g. in a remote system with Raspberry Pi OS: <pre><code>sudo apt update\nsudo apt install snapd\nsudo reboot\n# ...reconnect after reboot\nsudo snap install core\n</code></pre></p>"},{"location":"package-mgmt/#flatpak","title":"Flatpak","text":"<p>Set up instructions for your distribution.</p>"},{"location":"python/","title":"Python","text":"<p>Packaging Python Projects </p>"},{"location":"python/#install-python-the-pip-python-package-installer-and-setuptools","title":"Install Python, the pip Python package installer, and Setuptools","text":""},{"location":"python/#with-homebrew","title":"With Homebrew","text":"<p>Even if you already have Python on your OS it might be an outdated version. This installs python and pip. macOS <pre><code>brew install python\npython3 --version\n</code></pre> The python formulae install pip (as pip3) and Setuptools. Setuptools can be updated via pip3 and pip3 can be used to upgrade itself. <pre><code>python3 -m pip install --upgrade setuptools\npython3 -m pip install --upgrade pip\n</code></pre> Add the unversioned symlinks to your <code>$PATH</code> by adding this to your <code>.zshrc</code> file. <pre><code>export PATH=\"$(brew --prefix)/opt/python/libexec/bin:$PATH\"\n</code></pre> See Homebrew Python for details.  </p> <p><code>site-packages</code> is here. Example for python 3.10 on Homebrew <pre><code>ls -al $(brew --prefix)/lib/python3.10/site-packages\n</code></pre></p>"},{"location":"python/#without-homebrew","title":"Without Homebrew","text":"<p>Linux (Ubuntu) <pre><code>sudo apt install python3\n</code></pre></p> <p>macOS Download .pkg installer from pythong.org. Then get pip. <pre><code>curl -o get-pip.py https://bootstrap.pypa.io/get-pip.py\nsudo python get-pip.py\n</code></pre></p> <p>Linux <pre><code>sudo apt install python-pip\n</code></pre></p> <pre><code>pip install --upgrade pip setuptools\n</code></pre> <p>Warning</p> <p>On Ubuntu, upgrading to pip 10 may break pip. If this happens you need to: <pre><code>sudo nano /usr/bin/pip\n# change \"from pip import main\" to \"from pip._internal import main\"\n</code></pre></p>"},{"location":"python/#get-the-pep8-python-style-checker","title":"Get the pep8 python style checker","text":"<pre><code># On Linux you may need to use sudo.\npip install pep8\n</code></pre>"},{"location":"python/#anaconda","title":"Anaconda","text":"<p>For data science and machine learning. Anaconda is a distribution of the Python and R programming languages for scientific computing. See Getting Started. <pre><code>brew install --cask anaconda\n</code></pre></p> <p>Installing in silent mode </p> <p>Add anaconda to your <code>PATH</code> on your <code>.zshrc</code> and <code>source ~/.zshrc</code> to apply the changes. <pre><code>export PATH=\"$(brew --prefix)/anaconda3/bin:$PATH\"\n</code></pre> To use zsh: <pre><code>conda init zsh\n</code></pre></p> <p>Warning</p> <p>If <code>conda init zsh</code> messed up the <code>PATH</code> in your <code>~/.zshrc</code> by adding the <code>condabin</code> directory instead of <code>bin</code> you can fix it with a symlink: <pre><code>ln -sf $(brew --prefix)/anaconda3/bin/jupyter-lab $(brew --prefix)/anaconda3/condabin/jupyter-lab\nln -sf $(brew --prefix)/anaconda3/bin/jupyter $(brew --prefix)/anaconda3/condabin/jupyter\n</code></pre></p> <p>If you don't want to activate the <code>base</code> environment every time you open your terminal: <pre><code>conda config --set auto_activate_base false\n</code></pre></p> <p>When creating a conda environment you can use optionally use a configuration file. You can also of course, save a config file from an existing environment to back it up. <pre><code>conda env create -f myenv.yml\nconda activate myenv\n\nconda env list\nconda env remove --name ldm\n</code></pre></p>"},{"location":"python/#notebooks","title":"Notebooks","text":"<p>Fresh installs of Anaconda no longer include notebook extensions in their Jupyter installer. This means that the <code>nb_conda</code> libraries need to be added into your environment separately to access conda environments from your Jupyter notebook. Just run these to add them and you should be able to select your environment as a kernel within a Jupyter notebook. <pre><code>conda activate &lt;myenv&gt;\nconda install ipykernel\nconda install nb_conda_kernels # or defaults::nb_conda_kernels\n</code></pre></p> <p>Now you can launch the new JupyterLab or the classic Jupyter Notebook. <pre><code>jupyter-lab # or the classic \"jupyter notebook\"\n</code></pre></p>"},{"location":"rpi/","title":"Single-board Computers (SBCs)","text":"<p>Raspberry Pi, Orange Pi, and many others.</p> <p></p>"},{"location":"rpi/#get-os-image","title":"Get OS image","text":"<p>You could use an imager program that flashes an operating system on the SD card for you but you'll be very limited in the ways you can pre-configure the OS. It's better to download the OS image and flash it yourself.  </p> <p>I recommend using an Ubuntu image made specifically for your board instead of a manufaturer's custom OS like Raspberry Pi OS or Orange Pi OS. Those are designed for simplicity and ease of use at the expense of functionality that is important when building a cloud native homelab.</p> <ul> <li>\u274c Raspberry Pi OS doesn't come with <code>cloud-init</code>.</li> <li>\u274c Orange Pi OS doesn't support the GPU, Neural Processing Unit (NPU), and Vision Processing Unit (VPU) that some Orange Pi boards have for Artificial Intelligence and graphics-intensive workloads.</li> <li>\u2705 Ubuntu comes with <code>cloud-init</code>.</li> <li>\u2705 Ubuntu comes with Snap support preinstalled.</li> <li>\u2705 Ubuntu, Debian, and Android support the Orange Pi's GPU, NPU, and VPU.</li> </ul> <p>Make sure you grab the image corresponding to your Pi's CPU architecture (32-bit or 64-bit). We'll use the Server version because we don't need the graphical environment that comes with the Desktop version.  </p> <p>Official Ubuntu images for all Raspberry Pis (recommended): Download Checksums</p> <p>Manufacturer's images for Orange Pi 3B:   Download </p> <p>The rest of this guide assumes you downloaded your image to <code>~/Downloads</code>.</p> <p>Examples of verifying the integrity of the image file: On your laptop<pre><code># Check the sum from the Ubuntu website and decompress\necho \"f3842efb3be1be4243c24203bd16e335f155fdbe104b1ed8c5efc548ea478ab0 *ubuntu-22.04.3-preinstalled-server-arm64+raspi.img.xz\" | shasum -a 256 --check\nxz -d ubuntu-22.04.3-preinstalled-server-arm64+raspi.img.xz\n\n# Extract Orange Pi image and check the sum\n7zz x Orangepi3b_1.0.0_ubuntu_jammy_server_linux5.10.160.7z\nshasum -c Orangepi3b_1.0.0_ubuntu_jammy_server_linux5.10.160.img.sha\n</code></pre></p>"},{"location":"rpi/#option-a-cloud-native","title":"Option A: Cloud native","text":"<p>This is the repeatable, flexible option.</p> <p>Tip</p> <p>This lets you launch your Pi just like a cloud instance. Automatically upgrade the system, configure users, modify boot parameters, install software, run commands on first boot, among other things. Recommended when setting up multiple Pis or installing Kubernetes.</p> <p>Important</p> <p>This requires the OS image to:</p> <ul> <li>Have <code>cloud-init</code> preinstalled.</li> <li>Be mountable on the OS you're using (e.g. macOS) so you can inject <code>user-data</code> and boot parameters.</li> </ul> <p>\u2705 Supported by the Ubuntu images for Raspberry Pi. \u274c Not supoprted by Orange Pi images.</p> <p>If you're setting up an Orange Pi, download the image and see the one-off instructions.</p> <p>We'll flash a pre-configured SD card with:  </p> <ul> <li>A hostname.</li> <li>A user with ssh key authentication (and password authentication disabled).</li> <li>Upgraded packages.</li> <li>Additional packages installed.</li> <li>Wi-Fi configuration.</li> <li>Boot parameters modified to support Kubernetes.</li> <li>Kubernetes (MicroK8s) installed with some addons including one for cluster-ready replicated storage based on OpenEBS.</li> </ul> <p>If you don't have an ssh key, generate one with <code>ssh-keygen -t ed25519</code>.  </p> <p>Tip</p> <p>Install <code>pv</code> to see a progress bar and percentage of completion as the image is being flashed: <code>brew install pv</code></p> <p>Attention</p> <p>This example uses macOS. Adjust the commands with your OS's tools to copy/paste, view disks, <code>dd</code> options, and <code>sed</code> version. It has been tested with the image provided by Ubuntu for Raspberry Pi.  </p>"},{"location":"rpi/#flash-the-image","title":"Flash the image","text":"<p>Insert the SD card and check the device name e.g. <code>/dev/disk4</code> On your laptop<pre><code>diskutil list\n</code></pre></p> <p>Use that device name to flash the OS image to the SD card. On your laptop<pre><code>###################################################################\n# REPLACE WITH YOUR VALUES\n###################################################################\nIMAGE='ubuntu-22.04.3-preinstalled-server-arm64+raspi.img' DEVICE='/dev/disk4' ###################################################################\n\n# Unmount the card\ndiskutil unmountDisk $DEVICE\n\ncd ~/Downloads\n# sudo dd if=$IMAGE of=$DEVICE bs=1m status=progress\npv $IMAGE | sudo dd bs=1m of=$DEVICE\n</code></pre></p> <p>When it's done you'll see a volume mounted on your desktop called <code>system-boot</code> or something similar. Modify the volume to inject <code>user-data</code> and set boot parameters needed by Kubernetes (enable c-groups so the kubelet will work out of the box). On your laptop<pre><code>###################################################################\n# REPLACE WITH YOUR VALUES\n###################################################################\nVOLUME='system-boot'\n\nHOSTNAME='raspberrypi4b' LOCALE='en_US' TIMEZONE='US/Central' WIFI_NAME='MySSID' WIFI_PASSWORD='MyPassword' \n\npbcopy &lt; ~/.ssh/id_ed25519.pub\nKEY=$(pbpaste)\n###################################################################\n\n# create file for `cloud-init`\ncat &lt;&lt; EOF &gt; /Volumes/$VOLUME/user-data\n#cloud-config\n\nhostname: ${HOSTNAME}\nmanage_etc_hosts: false\nlocale: ${LOCALE}\ntimezone: ${TIMEZONE}\n\nusers:\n  - name: pi\n    shell: /bin/bash\n    lock_passwd: true\n    sudo: ALL=(ALL) NOPASSWD:ALL\n    ssh_authorized_keys:\n      - ${KEY}\n\npackage_upgrade: true\npackages:\n  - avahi-daemon \n  - lshw\n  - net-tools\n  # For OpenEBS\n  # - linux-modules-extra-$(uname -r)\n  # For Rook Ceph\n  - lvm2\n\nruncmd:\n  - sudo ifconfig wlan0 up\n  - sudo snap refresh\n  - sudo snap install microk8s --classic\n  - sudo usermod -a -G microk8s pi\n  - sudo chown -f -R pi ~/.kube\n  - newgrp microk8s\n  - microk8s enable metrics-server\n  - microk8s enable ingress\n  - microk8s enable dashboard\n  # For OpenEBS\n  # - sudo sysctl vm.nr_hugepages=1024\n  # - echo 'vm.nr_hugepages=1024' | sudo tee -a /etc/sysctl.conf\n  # - sudo modprobe nvme_tcp\n  # - echo 'nvme-tcp' | sudo tee -a /etc/modules-load.d/microk8s-mayastor.conf\n  # - microk8s stop\n  # - microk8s start\n  # - microk8s enable core/mayastor --default-pool-size 20G\n  # For Rook Ceph\n  - microk8s kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.11.1/cert-manager.yaml\n\n\nwrite_files:\n- content: |\n    # This file is generated from information provided by the datasource.  Changes\n    # to it will not persist across an instance reboot.  To disable cloud-init's\n    # network configuration capabilities, write a file\n    # /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg with the following:\n    # network: {config: disabled}\n    network:\n        ethernets:\n            eth0:\n                dhcp4: true\n                optional: true\n        version: 2\n        wifis:\n            wlan0:\n                optional: true\n                access-points:\n                    \"${WIFI_NAME}\":\n                        password: \"${WIFI_PASSWORD}\"\n                dhcp4: true\n  path: /etc/netplan/50-cloud-init.yaml\n- content: |\n    network: {config: disabled}\n  path: /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg\nEOF\n\n# Add these options at the end of the file as Kubernetes will need them to run on the Pi.\nsed -i \"\" \"$ s/$/ cgroup_enable=memory cgroup_memory=1/\" /Volumes/$VOLUME/cmdline.txt\n</code></pre></p> <p>You can verify that the files were written correctly On your laptop<pre><code>cat /Volumes/$VOLUME/user-data\ncat /Volumes/$VOLUME/cmdline.txt\n</code></pre></p> <p>Unmount the SD card On your laptop<pre><code>diskutil unmountDisk $DEVICE\n</code></pre></p> <p>\ud83c\udf89 You're done! \ud83c\udf7e  </p> <p>Now remove the SD card from your laptop and insert it into the Pi which should be connected to your router with an ethernet cable. The Wi-Fi won't be available until everything has finished configuring and you manually do a required system restart.</p> <p>Since we pre-configured everything it has a lot of work to do on the first boot and it takes several minutes. You'll be able to go in as soon as the SSH service is up but it will probably still be in the process of upgrading packages as well as installing and configuring our software.  </p> <p>Go make yourself a cup of tea before connecting for the first time.</p>"},{"location":"rpi/#verify","title":"Verify","text":"<p>Once cloud-init is done launching our instance, you can check a few things to make sure everything went smoothly. On your Pi<pre><code># If using Ceph storage, verify your kernel is built with the RBD module.\n# If 'not found', rebuild the kernel to include the rbd module, install a newer kernel, or choose a different Linux distribution.\nmodprobe rbd\n\n# Check if there were any cloud-init errors\nsudo cat /var/log/cloud-init.log | grep failures\nsudo cat /var/log/cloud-init-output.log\n\n# Check if packages were installed\nsudo apt list | grep avahi-daemon\nsudo apt list | grep lshw\nsudo cat /var/log/apt/history.log\n\n# Check if a service is running\nsystemctl status 'avahi*'\n\n# Check if MicroK8s is intalled and running with the addons enabled\nmicrok8s status --wait-ready\nmicrok8s kubectl cluster-info\n# On first boot, the etcd-operator-mayastor pod \n# may be stuck in CrashLoopBackOff state until you reboot\nmicrok8s kubectl get pod -n mayastor \nmicrok8s kubectl get diskpool -n mayastor\n\n# Check cloud-init's network configuration \ncat /etc/netplan/50-cloud-init.yaml\ncat /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg\n\n# Look at the state of the network.\n# You may need to `sudo reboot` for Wi-Fi to be enabled but \n# make sure you've allowed enough time for cloud-init to \n# finish configuring your instance and MicroK8s mayastor pods to come up!\nsudo lshw -c network\nip addr show | grep wlan0\n</code></pre></p>"},{"location":"rpi/#option-b-one-off","title":"Option B: One-off","text":"<p>Tip</p> <p>This is the easiest option but is limited in how much you can pre-configure the OS. Recommended if you're setting up only one board or if your OS image doesn't have <code>cloud-init</code>.</p>"},{"location":"rpi/#flash-the-image_1","title":"Flash the image","text":"<p>Use Raspberry Pi Imager or Balena Etcher (<code>brew install --cask [raspberry-pi-imager | balenaetcher]</code>) to flash the image to the SD card. If you use Pi Imager you can:  </p> <ul> <li>Change the password for the default user or disable passwords altogether.  </li> <li>Enable SSH (password or ssh keys); remove password authentication if using keys.  </li> <li>Configure Wi-Fi if needed.  </li> <li>Set the hostname.  </li> </ul> <p>Attention</p> <p>The Orange Pi images have to be configured manually after first boot.</p> <p>Access your Pi </p>"},{"location":"rpi/#upgrade","title":"Upgrade","text":"<ul> <li><code>upgrade</code> is used to install available upgrades of all packages currently installed on the system. New packages will be installed if required to satisfy dependencies, but existing packages will never be removed. If an upgrade for a package requires the removal of an installed package the upgrade for this package isn't performed.  </li> <li><code>full-upgrade</code> performs the function of upgrade but will remove currently installed packages if this is needed to upgrade the system as a whole.</li> </ul> On your Pi<pre><code>sudo apt update # updates the package list\nsudo apt full-upgrade\n</code></pre>"},{"location":"rpi/#configure","title":"Configure","text":"<p>Make the board reachable using its hostname, not only its IP. Also install tool to view hardware info. On your Pi<pre><code>sudo apt install avahi-daemon lshw\n</code></pre></p>"},{"location":"rpi/#authentication","title":"Authentication","text":"<p>If you didn't do so during setup, generate and add an ssh key. On your laptop<pre><code># Specify the type of key to create e.g. `ed25519` or `rsa`.\nssh-keygen -t ed25519\n# Add it on the remote machine (if the `-i` filename does not end in `.pub` this is added)\n# Examples:\nssh-copy-id -i ~/.ssh/id_rsa pi@raspberrypi4b.local\nssh-copy-id -i ~/.ssh/id_ed25519 orangepi@orangepi3b.local\n</code></pre></p> <p>To remove password authentication: On your Pi<pre><code>sudo nano /etc/ssh/sshd_config\n</code></pre> and replace <code>#PasswordAuthentication yes</code> with <code>PasswordAuthentication no</code>. Test the validity of the config file and restart the service (or reboot). On your Pi<pre><code>sudo sshd -t\nsudo service sshd restart\nsudo service sshd status\n</code></pre></p>"},{"location":"rpi/#board-specific-tools","title":"Board-specific tools","text":""},{"location":"rpi/#raspberry-pi","title":"Raspberry Pi","text":"<p>On Raspberry Pi OS On your Pi<pre><code>sudo raspi-config\n# Go to Interface Options, VNC (for graphical remote access)\n# Tab to the Finish option and reboot.\n</code></pre></p>"},{"location":"rpi/#orange-pi","title":"Orange Pi","text":"<p>In Ubuntu for Orange Pi, to reach other machines by hostname you need to add an entry to the hosts file even if <code>avahi-daemon</code> is running on the hosts. Example: /etc/hosts<pre><code>192.168.x.x  thathostname.local\n</code></pre></p> <p>Orange Pi has a config tool as well On your Pi<pre><code>sudo orangepi-config\n</code></pre> If you just need to connect to Wi-Fi on Orange Pi, use: On your Pi<pre><code>nmcli dev wifi connect wifi_name password wifi_passwd\n</code></pre></p> <p>To set a static IP on Orange Pi see the user manual for instructions on using the <code>nmtui</code> command.</p>"},{"location":"rpi/#kubernetes","title":"Kubernetes","text":"<p>If you went the one-off route, install MicroK8s.</p>"},{"location":"rpi/#usage","title":"Usage","text":""},{"location":"rpi/#access-your-pi","title":"Access your Pi","text":"<p>Tip</p> <p>Before connecting to your Pi If it's not already running, start the <code>ssh-agent</code> in the background and add your private key to it so you're not asked for your passphrase every time. On your laptop<pre><code># is it running?\nps -ax | grep ssh-agent\n# which identities have been added?\nssh-add -l\n\n# start the agent and add your identity\neval \"$(ssh-agent -s)\"\nssh-add --apple-use-keychain ~/.ssh/id_ed25519\n</code></pre></p> <p>If you're reinstalling the OS you might need to remove old key fingerprints belonging to that hostname from your <code>known_hosts</code> file. Example: On your laptop<pre><code>ssh-keygen  -f ~/.ssh/known_hosts -R raspberrypi4b.local\n</code></pre></p> <p>If your board is already running <code>avahi-daemon</code> you can reach it using its hostname. On your laptop<pre><code>arp raspberrypi4b.local </code></pre> Otherwise, find your board's IP in your router's admin UI or by going over the list of all devices on your network with <code>arp -a</code>.</p> <p>SSH into it with the configured user e.g. <code>pi</code> and the IP address or hostname. Examples: On your laptop<pre><code>ssh pi@raspberrypi4b.local\n# or\nssh pi@192.168.xxx.xxx\n</code></pre></p> <p>Update the password for default users like <code>pi</code>, <code>orangepi</code>, <code>ubuntu</code>, etc. Examples: On your Pi<pre><code>sudo passwd root\nsudo passwd pi\n</code></pre></p>"},{"location":"rpi/#remote-gui-access","title":"Remote GUI access","text":"<p>If you installed a graphical desktop</p> <p>You'll need a VNC viewer on your laptop to connect to the Pi using the graphical interface. On your laptop<pre><code>brew install --cask vnc-viewer\n</code></pre></p> <p>Attention</p> <p>Apparently, on Raspberry Pi OS <code>pip</code> does not download from the Python Package Index (PyPI), it downloads from PiWheels. PiWheels wheels do not come with <code>pygame</code>'s dependencies that are bundled in normal releases.</p> <p>Install Pygame dependencies and Pygame. On your Pi<pre><code>sudo apt install libvorbisenc2 libwayland-server0 libxi6 libfluidsynth2 libgbm1 libxkbcommon0 libopus0 libwayland-cursor0 libsndfile1 libwayland-client0 libportmidi0 libvorbis0a libopusfile0 libmpg123-0 libflac8 libxcursor1 libxinerama1 libasyncns0 libxrandr2 libdrm2 libpulse0 libxfixes3 libvorbisfile3 libmodplug1 libxrender1 libsdl2-2.0-0 libxxf86vm1 libwayland-egl1 libsdl2-ttf-2.0-0 libsdl2-image-2.0-0 libjack0 libsdl2-mixer-2.0-0 libinstpatch-1.0-2 libxss1 libogg0\nsudo pip3 install pygame\n\n# Check that the installation worked by running one of its demos\npython3 -m pygame.examples.aliens\n</code></pre></p>"},{"location":"rpi/#to-give-it-a-static-ip","title":"To give it a static IP","text":""},{"location":"rpi/#option-a-the-router","title":"Option A: The router","text":"<p>Log in to your router's admin interface and go to LAN -&gt; DHCP Server (or similar wording). There will be an option to manually assign an IP to an item on the list of client MAC addresses.</p>"},{"location":"rpi/#option-b-dhcpcdconf","title":"Option B: <code>dhcpcd.conf</code>","text":"<p>Find the IP adddress of your router. It's the address that appears after <code>default via</code>. On your Pi<pre><code>ip r\ndefault via [IP]\n</code></pre> Get the IP of your DNS server (it may or may not be your router) On your Pi<pre><code>grep nameserver /etc/resolv.conf\n</code></pre></p> <p>Open this file: On your Pi<pre><code>nano /etc/dhcpcd.conf\n</code></pre> and add/edit these lines at the end filling in the correct info. <pre><code>interface [wlan0 for Wi-Fi or eth0 for Ethernet]\nstatic_routers=[ROUTER IP]\nstatic domain_name_servers=[DNS IP]\n[static or inform] ip_address=[STATIC IP ADDRESS YOU WANT]/24\n</code></pre> <code>inform</code> means that the Pi will attempt to get the IP address you requested, but if it's not available, it will choose another. If you use <code>static</code>, it will have no IP v4 address at all if the requested one is in use.  </p> <p>Save the file and <code>sudo reboot</code>. From now on, upon each boot, the Pi will attempt to obtain the static ip address you requested.  </p>"},{"location":"rpi/#to-set-it-up-as-a-dns-server","title":"To set it up as a DNS server","text":"<ol> <li>Install and configure a DNS Server e.g. DNSmasq or Pi-Hole on the Pi.</li> <li>Change your router\u2019s DNS settings to point to the Pi. Log in to your router's admin interface and look for DNS e.g. in LAN -&gt; DHCP Server. Set the primary DNS server to the IP of your Pi and make sure it's the only DNS server. The Pi will handle upstream DNS services.</li> </ol>"},{"location":"rpi/#copying-files","title":"Copying files","text":"<p>To copy files between the Pi and local machine On your laptop<pre><code>scp -r pi@raspberrypi2.local:/home/pi/Documents/ ~/Documents/pidocs\n</code></pre></p>"},{"location":"rpi/#find-info-about-your-pi","title":"Find info about your Pi","text":"<p>View number of processing units and system load On your Pi<pre><code>nproc\nuptime\n</code></pre></p> <p>CPU temperature in millidegree Celsius (one thousandth of a degree) On your Pi<pre><code>cat /sys/class/thermal/thermal_zone*/temp\n</code></pre> On the Orange Pi On your Pi<pre><code># For CPU and GPU\nsensors\n# For NVMe SSD\nsudo smartctl -a /dev/nvme0 | grep \"Temperature:\"\n</code></pre></p> <p>What model do you have? On your Pi<pre><code>cat /sys/firmware/devicetree/base/model ;echo\n</code></pre></p> <p>What's the connection speed of the ethernet port? On your Pi<pre><code>ethtool eth0\n</code></pre></p> <p>32 or 64-bit kernel? On your Pi<pre><code>getconf LONG_BIT\n# or check machine's hardware name: armv7l is 32-bit and aarch64 is 64-bit\nuname -m\n</code></pre></p> <p>See OS version On your Pi<pre><code>cat /etc/os-release\n</code></pre></p> <p>Architecture   If the following returns a <code>Tag_ABI_VFP_args</code> tag of <code>VFP registers</code>, it's an <code>armhf</code> (<code>arm</code>) system. A blank output means <code>armel</code> (<code>arm/v6</code>). On your Pi<pre><code>readelf -A /proc/self/exe | grep Tag_ABI_VFP_args\n</code></pre> Or check the architecture with: On your Pi<pre><code>hostnamectl\n</code></pre></p> <p>You can find info about the hardware like ports, pins, RAM, SoC, connectivity, etc. with: On your Pi<pre><code>pinout\n</code></pre></p>"},{"location":"rpi/#troubleshooting","title":"Troubleshooting","text":"<p>If your Pi's ethernet port is capable of 1Gbps, you're using a cat5e cable or better, your router and switch support 1Gbps,  and you're still only getting 100Mbps first try with another cable. A faulty cable is the most common cause of problems like this. If that doesn't work you can try disabling EEE (Energy Efficient Ethernet) although it will be reenabled at reboot. You could also try setting the speed manually. On your Pi<pre><code>ethtool --show-eee eth0\nsudo ethtool --set-eee eth0 eee off\n\n# set speed manually and disable autonegotiation\nethtool -s eth0 speed 1000 duplex full autoneg off\n</code></pre></p> <p>If not using <code>cloud-init</code> or an imager program, enable ssh with On your Pi<pre><code>touch /Volumes/$VOLUME/ssh\n</code></pre></p> <p>DNS issues On your Pi<pre><code>cat /etc/resolv.conf\nresolvectl status\n</code></pre></p>"},{"location":"rpi/#learn-about-electronics","title":"Learn about electronics","text":"<p>I've added some sample code from the MagPi Essentials book. Sample code</p>"},{"location":"rpi/#gpio-header","title":"GPIO Header","text":""},{"location":"rpi/#boot-from-ssd","title":"Boot from SSD","text":"<p>These instructions are for Raspberry Pi 4. Other boards may need additional steps.</p> <ol> <li>Use Raspberry Pi Imager to flash the USB Boot bootloader utility image onto a microSD card.</li> <li>Insert the card into the Pi, turn it on so the Pi flashes the bootloader from the card. When it's done the green LED will start blinking.</li> <li>Turn off the Pi and remove the card.</li> <li>Connect a raw device (no partitions or formatted filesystems) SSD to your laptop.</li> <li>Flash your desired OS image to the SSD with <code>dd</code> or preferred tool e.g. as explained here but using the SSD instead of a microSD card.</li> <li>Unmount and remove the SSD drive from your laptop and connect it to a USB 3.0 port on your Pi.</li> <li>Turn on your Pi.</li> <li>Connect to your Pi over ssh and check available storage space with <code>df -h</code>.</li> <li>Use <code>parted</code> to set up/resize the SSD partitions and file systems as desired. For example, leave a raw partition (no formatted filesystem) for use by a Ceph storage cluster. See examples of using <code>parted</code> here or in the section SSD as Additional Storage.</li> </ol>"},{"location":"rpi/#ssd-as-additional-storage","title":"SSD as Additional Storage","text":"<p>If you have an additional SSD you'll need to:</p> <ul> <li>Choose a partition manipulation program<ul> <li>GNU Parted (<code>parted</code>) is probably already installed and ready to use from the command line. </li> <li>If you have a desktop environment you can use the graphical frontend GParted. Install with <code>sudo apt install gparted</code>.</li> </ul> </li> <li>Create a partition table (aka disklabel). The default partition table type is <code>msdos</code> for disks smaller than 2 Tebibytes in size (assuming a 512 byte sector size) and <code>gpt</code> for disks 2 Tebibytes and larger.</li> <li>Create partition(s) and file system(s).</li> <li>Find the file system's UUID.</li> <li>Create a directory for mounting the SSD.</li> <li>Set up automatic SSD mounting, mount the SSD, reboot to test.</li> </ul> <p>Example: <pre><code>cat /sys/block/sda/queue/optimal_io_size\n# 33553920\ncat /sys/block/sda/queue/minimum_io_size\n# 512\ncat /sys/block/sda/alignment_offset\n# 0\ncat /sys/block/sda/queue/physical_block_size\n# 512\n\nsudo parted\n\n(parted) print devices # you should see your SSD e.g. /dev/sda (240GB)\n(parted) select /dev/sda # whatever name your SSD device has\n(parted) mklabel msdos # Add optimal_io_size to alignment_offset and divide the result by physical_block_size.\n# This number is the sector at which the partition should start. Here it ends in the last sector.\n# Example:\n(parted) mkpart primary ext4 65535s -1s\n\n(parted) print list\n(parted) align-check optimal 1 # or whatever number your partition has\n# 1 aligned \n(parted) quit\n\n# Make the filesystem with a volume label on partition 1 (or whatever number yours has)\nsudo mkfs.ext4 -L WDSSD -c /dev/sda1\n# Filesystem UUID is displayed but you can also find it with:\nsudo lsblk -o UUID,NAME,FSTYPE,SIZE,MOUNTPOINT,LABEL,MODEL\n\nmkdir wdssd\nsudo chown pi:pi -R /home/pi/wdssd/\nsudo chmod a+rwx /home/pi/wdssd/\nsudo nano /etc/fstab\n# At the end of the file that opens, add a new line containing the UUID and mounting directory\n# UUID=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx /home/pi/wdssd/ ext4 defaults,auto,users,rw,nofail 0 0\n</code></pre></p>"},{"location":"shell/","title":"Shell","text":"<p>Open Finder and show hidden files with Cmd+Shift+. or with <pre><code>defaults write com.apple.finder AppleShowAllFiles TRUE; killall Finder\n</code></pre></p>"},{"location":"shell/#zsh","title":"Zsh","text":"<pre><code>brew install zsh # get the latest version\n\n# You may need to add the Homebrew version of Zsh to the list on /etc/shells\nsudo nano /etc/shells # add the output of \"which zsh\" to the top of the list, save the file.\n\nwhere zsh # you might see both the shell that came with your Mac and the latest from Homebrew\nwhich zsh # make sure this returns the one from Homebrew.\n\nchsh -s $(which zsh)\n# Install Oh My Zsh, a framework for managing your Zsh configuration with plugins and themes\nsh -c \"$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"\n</code></pre> <p>Oh My Zsh Add any built-in plugins you need to your <code>~/.zshrc</code> <pre><code>plugins=(git macos python)\n</code></pre></p> <p>You can also add custom plugins by cloning the repo into the plugins directory or with Homebrew. </p> <p>This plugin auto suggests previous commands.  <pre><code>git clone https://github.com/zsh-users/zsh-autosuggestions $ZSH_CUSTOM/plugins/zsh-autosuggestions\n# add zsh-autosuggestions to plugins list in ~/.zshrc and:\nsource ~/.zshrc\n</code></pre></p> <p>The zsh-syntax-highlighting plugin highlights valid commands green and invalid ones red so you don't have to test the command to see if it will work. <pre><code>brew install zsh-syntax-highlighting\n# To activate the syntax highlighting, add the following at the end of your ~/.zshrc:\nsource $(brew --prefix)/share/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh\n</code></pre></p> <p>Some Oh My Zsh themes like Spaceship have font requirements. You can install fonts with Homebrew by adding the fonts repository. Fira Code is a good one for programming and computer science. <pre><code>brew tap homebrew/cask-fonts\nbrew install --cask font-fira-code\n</code></pre></p> <p>Or get a ton of fonts with: <pre><code>git clone https://github.com/powerline/fonts.git --depth=1\n./fonts/install.sh\nrm -rf fonts\n</code></pre></p> <p>Install your Oh My Zsh theme e.g. Spaceship (requires at least the Fire Code font) <pre><code>brew install spaceship\n# If the theme is not copied to your themes folder, sim link from the Homebrew dir to your custom themes folder e.g.\nln -sf $(brew --prefix)/Cellar/spaceship/4.4.1/spaceship.zsh $ZSH_CUSTOM/themes/spaceship.zsh-theme\n\ntouch ~/.spaceshiprc.zsh\n</code></pre></p> <p>Any time you edit your zsh configuration file you can reload it to apply changes. <pre><code>source ~/.zshrc\n</code></pre></p>"},{"location":"shell/#terminal-replacement","title":"Terminal replacement","text":"<p>You can use iTerm2 <pre><code>brew install --cask iterm2\n</code></pre></p> <p>After you have installed the font(s) required by your Oh My Zsh theme set your iTerm preferences like default shell and font. Verify that you're using the shell you want. In the output of the <code>env</code> command look for something like <code>zsh=/opt/homebrew/bin/zsh</code>.</p> <p>Install iTerm 2 color schemes <pre><code>git clone https://github.com/mbadolato/iTerm2-Color-Schemes.git\ncd iTerm2-Color-Schemes\n\n# Import all color schemes\ntools/import-scheme.sh schemes/*\n</code></pre> Restart iTerm 2 (need to quit iTerm 2 to reload the configuration file). iTerm2 &gt; Preferences &gt; Profile &gt; Colors &gt; Color Presets </p>"},{"location":"shell/#bash","title":"Bash","text":"<p>If you want to use bash <pre><code>brew install bash # get the latest version of bash\nchsh -s $(which bash)\nnano ~/.bash_profile # and paste from sample dot file\n</code></pre></p>"},{"location":"utilities/","title":"Install utilities","text":"<p>These will vary for each person. Some examples on a Mac:  </p> <pre><code>brew install wget\nbrew install gcc\nbrew install jq\nbrew install kompose # translate Docker Compose files to Kubernetes resources\nbrew install helm\nbrew install --cask docker\nbrew install --cask drawio # Diagrams (UML, AWS, etc.)\nbrew install --cask 1password\nbrew install --cask nordvpn\nbrew install --cask google-chrome # or chromium\nbrew install --cask firefox\nbrew install --cask visual-studio-code # or vscodium \nbrew install --cask kindle\nbrew install --cask authy\nbrew install --cask teamviewer\nbrew install --cask raspberry-pi-imager\nbrew install --cask balenaetcher # to make bootable media\nbrew install --cask zoom\nbrew install --cask spotify\nbrew install --cask whatsapp\nbrew install --cask messenger # Facebook messenger\nbrew install --cask rectangle # to snap windows\nbrew install --cask avast-security # if it fails, download from website\nbrew install --cask handbrake # if you need to rip DVDs\nbrew install --cask virtualbox # currently Intel Macs only. ARM installer in beta.\nbrew install --cask multipass # run Ubuntu VMs in a local mini-cloud.\nbrew install --cask steam # if you want games\nbrew tap homebrew/cask-drivers # add repository of drivers\nbrew install logitech-options # driver for Logitech mouse\n</code></pre> <p>On macOS you can also install gcc by installing the xcode developer tools if you're not using Homebrew. <pre><code>xcode-select -v # check if tools are installed\nxcode-select --install\n</code></pre></p>"}]}