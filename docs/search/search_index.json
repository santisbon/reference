{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Reference documentation Select a section from the sidebar for instructions on setting up a new development workstation.","title":"Home"},{"location":"#reference-documentation","text":"Select a section from the sidebar for instructions on setting up a new development workstation.","title":"Reference documentation"},{"location":"aws/","text":"Amazon Web Services AWS CLI Install the AWS command line interface curl \"https://awscli.amazonaws.com/AWSCLIV2.pkg\" -o \"AWSCLIV2.pkg\" sudo installer -pkg AWSCLIV2.pkg -target / Check if the AWS CLI is installed correctly # you may need to reload your shell's rc file first. aws --version Configure the AWS CLI aws configure Alexa Skills Kit (ASK) CLI https://developer.amazon.com/docs/smapi/set-up-credentials-for-an-amazon-web-services-account.html https://developer.amazon.com/docs/smapi/quick-start-alexa-skills-kit-command-line-interface.html AWS Amplify npm install -g @aws-amplify/cli amplify configure","title":"AWS"},{"location":"aws/#amazon-web-services","text":"","title":"Amazon Web Services"},{"location":"aws/#aws-cli","text":"","title":"AWS CLI"},{"location":"aws/#install-the-aws-command-line-interface","text":"curl \"https://awscli.amazonaws.com/AWSCLIV2.pkg\" -o \"AWSCLIV2.pkg\" sudo installer -pkg AWSCLIV2.pkg -target /","title":"Install the AWS command line interface"},{"location":"aws/#check-if-the-aws-cli-is-installed-correctly","text":"# you may need to reload your shell's rc file first. aws --version","title":"Check if the AWS CLI is installed correctly"},{"location":"aws/#configure-the-aws-cli","text":"aws configure","title":"Configure the AWS CLI"},{"location":"aws/#alexa-skills-kit-ask-cli","text":"https://developer.amazon.com/docs/smapi/set-up-credentials-for-an-amazon-web-services-account.html https://developer.amazon.com/docs/smapi/quick-start-alexa-skills-kit-command-line-interface.html","title":"Alexa Skills Kit (ASK) CLI"},{"location":"aws/#aws-amplify","text":"npm install -g @aws-amplify/cli amplify configure","title":"AWS Amplify"},{"location":"docker/","text":"Docker Install macOS brew install --cask docker You must use the --cask version. Otherwise only the client is included and can't run the Docker daemon. Then open the Docker app and grant privileged access when asked. Only then will you be able to use docker. Linux / Raspberry Pi OS: If you just ran apt upgrade on your Raspberry Pi, reboot before installing Docker. Follow the appropriate installation method . If you don't want to have to prefix commands with sudo add your user to the docker group. This is equivalent to giving that user root privileges. cat /etc/group | grep docker # see if docker group exists sudo usermod -aG docker $USER Follow Post-installation steps to configure log rotation. Use Architecture Supported Architectures Platform specifiers Value Normalized Examples aarch64 arm64 Apple M1/M2, Raspberry Pi 4 armhf arm Raspberry Pi 2 armel arm/v6 i386 386 x86_64 amd64 Intel (Default) x86-64 amd64 Intel (Default) Docker Desktop for Apple silicon can run (or build) an Intel image using emulation. The --platform option sets the platform if server is multi-platform capable. Macbook M1/M2 is based on ARM . The Docker host is detected as linux/arm64/v8 by Docker. On a Macbook M1/M2 running a native arm64 linux image instead of an amd64 (x86-64 Intel) image with emulation: When installing apps on the container either install the package for the ARM platform if they provide one, or run an amd64 docker image (Docker will use emulation). See Docker host info. You need to use the real field names (not display names) so we'll grab the info in json to get the real field names and use jq to display it nicely. Then we'll render them using Go templates. docker info --format '{{json .}}' | jq . docker info --format \"{{.Plugins.Volume}}\" docker info --format \"{{.OSType}} - {{.Architecture}} CPUs: {{.NCPU}}\" With the default images, Docker Desktop for Apple silicon warns us the requested image's platform (linux/amd64) is different from the detected host platform (linux/arm64/v8) and no specific platform was requested. --platform linux/amd64 is the default and the same as --platform linux/x86_64 . It runs (or builds) an Intel image. --platform linux/arm64 runs (or builds) an aarch64 (arm64) image. Architecture used by Appple M1/M2. docker run -it --name container1 debian # WARNING. uname -m -> x86_64 (amd64) docker run -it --platform linux/amd64 --name container1 debian # NO warning. uname -m -> x86_64 (amd64). On Mac it emulates Intel. docker run -it --platform linux/arm64 --name container1 debian # NO warning. uname -m -> aarch64 (arm64). Mac native. # On a Mac M2 you can also use this image docker run -it --name mycontainer arm64v8/debian bash # NO warning. uname -m -> aarch64 (arm64). Mac native. Examples Dockerfile instruction to keep a container running CMD tail -f /dev/null Running containers # sanity check docker version docker --help # \"docker run\" = \"docker container run\" docker run --name container1 debian # created and Exited docker run -it --name container2 debian # created and Up. Interactive tty docker run --detach --name container3 debian # created and Exited docker run -it --detach --name container4 debian # created and Up. Running in background docker stop container4 # Exited docker restart container4 # Up docker attach container4 # Interactive tty # To have it removed automatically when it's stopped docker run -it --rm --name mycontainer debian # created and Up. Interactive tty. # Run bash in a new container with interactive tty, based on debian image. docker run -it --name mycontainer debian bash # Get the IDs of all stopped containers. Options: all, quiet (only IDs), filter. docker ps -aq -f status=exited Working with images and volumes # Show all images (including intermediate images) docker image ls -a # Build an image from a Dockerfile and give it a name (or name:tag). docker build -f mydockerfile -t santisbon/myimage . docker builder prune -a # Remove all unused build cache, not just dangling ones docker image rm santisbon/myimage # Volumes docker volume create my-vol docker volume ls docker volume inspect my-vol docker volume prune docker volume rm my-vol # Start a container from an image. docker run \\ --name mycontainer \\ --hostname mycontainer \\ --mount source=my-vol,target=/data \\ santisbon/myimage # Get rid of all stopped containers. Options: remove **anonymous volumes** associated with the container. docker rm -v $(docker ps -aq -f status=exited) # You might want to make an alias: alias drm='docker rm -v $(docker ps -aq -f status=exited)' # or alias drm='sudo docker rm -v $(sudo docker ps -aq -f status=exited)' # on macOS the volume mountpoint (/var/lib/docker/volumes/) is in the VM that Docker Desktop uses to provide the Linux environment. # You can read it through a container given extended privileges. # Use: # - the \"host\" PID namespace, # - an image e.g. debian, # - the nsenter command to run a program (sh) in a different namespace # - the target process with PID 1 # - enter following namspaces of the target process: mount, UTS, network, IPC. docker run -it --privileged --pid=host debian nsenter -t 1 -m -u -n -i sh If you want to build multi-platform docker images: docker buildx create --name mybuilder --driver docker-container --bootstrap docker buildx use mybuilder docker buildx inspect docker buildx ls Docker Compose. Multi-container deployments in a compose.yaml file. Using Compose in Production docker compose -p mastodon-bot-project up --detach # or docker compose -p mastodon-bot-project create docker compose -p mastodon-bot-project start # connect to a running container docker exec -it bot-app bash docker compose -p mastodon-bot-project down # or docker compose -p mastodon-bot-project stop Kubernetes # Convert the Docker Compose file to k8s files kompose --file compose.yaml convert # Make sure the container image is available in a repository. # You can build it with ```docker build``` or ```docker compose create``` and push it to a public repository. docker image push user/repo # multiple -f filenames or a folder kubectl apply -f ./k8s kubectl get pods kubectl delete -f ./k8s","title":"Docker"},{"location":"docker/#docker","text":"","title":"Docker"},{"location":"docker/#install","text":"","title":"Install"},{"location":"docker/#macos","text":"brew install --cask docker You must use the --cask version. Otherwise only the client is included and can't run the Docker daemon. Then open the Docker app and grant privileged access when asked. Only then will you be able to use docker.","title":"macOS"},{"location":"docker/#linux-raspberry-pi-os","text":"If you just ran apt upgrade on your Raspberry Pi, reboot before installing Docker. Follow the appropriate installation method . If you don't want to have to prefix commands with sudo add your user to the docker group. This is equivalent to giving that user root privileges. cat /etc/group | grep docker # see if docker group exists sudo usermod -aG docker $USER Follow Post-installation steps to configure log rotation.","title":"Linux / Raspberry Pi OS:"},{"location":"docker/#use","text":"","title":"Use"},{"location":"docker/#architecture","text":"Supported Architectures Platform specifiers Value Normalized Examples aarch64 arm64 Apple M1/M2, Raspberry Pi 4 armhf arm Raspberry Pi 2 armel arm/v6 i386 386 x86_64 amd64 Intel (Default) x86-64 amd64 Intel (Default) Docker Desktop for Apple silicon can run (or build) an Intel image using emulation. The --platform option sets the platform if server is multi-platform capable. Macbook M1/M2 is based on ARM . The Docker host is detected as linux/arm64/v8 by Docker. On a Macbook M1/M2 running a native arm64 linux image instead of an amd64 (x86-64 Intel) image with emulation: When installing apps on the container either install the package for the ARM platform if they provide one, or run an amd64 docker image (Docker will use emulation). See Docker host info. You need to use the real field names (not display names) so we'll grab the info in json to get the real field names and use jq to display it nicely. Then we'll render them using Go templates. docker info --format '{{json .}}' | jq . docker info --format \"{{.Plugins.Volume}}\" docker info --format \"{{.OSType}} - {{.Architecture}} CPUs: {{.NCPU}}\" With the default images, Docker Desktop for Apple silicon warns us the requested image's platform (linux/amd64) is different from the detected host platform (linux/arm64/v8) and no specific platform was requested. --platform linux/amd64 is the default and the same as --platform linux/x86_64 . It runs (or builds) an Intel image. --platform linux/arm64 runs (or builds) an aarch64 (arm64) image. Architecture used by Appple M1/M2. docker run -it --name container1 debian # WARNING. uname -m -> x86_64 (amd64) docker run -it --platform linux/amd64 --name container1 debian # NO warning. uname -m -> x86_64 (amd64). On Mac it emulates Intel. docker run -it --platform linux/arm64 --name container1 debian # NO warning. uname -m -> aarch64 (arm64). Mac native. # On a Mac M2 you can also use this image docker run -it --name mycontainer arm64v8/debian bash # NO warning. uname -m -> aarch64 (arm64). Mac native.","title":"Architecture"},{"location":"docker/#examples","text":"Dockerfile instruction to keep a container running CMD tail -f /dev/null Running containers # sanity check docker version docker --help # \"docker run\" = \"docker container run\" docker run --name container1 debian # created and Exited docker run -it --name container2 debian # created and Up. Interactive tty docker run --detach --name container3 debian # created and Exited docker run -it --detach --name container4 debian # created and Up. Running in background docker stop container4 # Exited docker restart container4 # Up docker attach container4 # Interactive tty # To have it removed automatically when it's stopped docker run -it --rm --name mycontainer debian # created and Up. Interactive tty. # Run bash in a new container with interactive tty, based on debian image. docker run -it --name mycontainer debian bash # Get the IDs of all stopped containers. Options: all, quiet (only IDs), filter. docker ps -aq -f status=exited Working with images and volumes # Show all images (including intermediate images) docker image ls -a # Build an image from a Dockerfile and give it a name (or name:tag). docker build -f mydockerfile -t santisbon/myimage . docker builder prune -a # Remove all unused build cache, not just dangling ones docker image rm santisbon/myimage # Volumes docker volume create my-vol docker volume ls docker volume inspect my-vol docker volume prune docker volume rm my-vol # Start a container from an image. docker run \\ --name mycontainer \\ --hostname mycontainer \\ --mount source=my-vol,target=/data \\ santisbon/myimage # Get rid of all stopped containers. Options: remove **anonymous volumes** associated with the container. docker rm -v $(docker ps -aq -f status=exited) # You might want to make an alias: alias drm='docker rm -v $(docker ps -aq -f status=exited)' # or alias drm='sudo docker rm -v $(sudo docker ps -aq -f status=exited)' # on macOS the volume mountpoint (/var/lib/docker/volumes/) is in the VM that Docker Desktop uses to provide the Linux environment. # You can read it through a container given extended privileges. # Use: # - the \"host\" PID namespace, # - an image e.g. debian, # - the nsenter command to run a program (sh) in a different namespace # - the target process with PID 1 # - enter following namspaces of the target process: mount, UTS, network, IPC. docker run -it --privileged --pid=host debian nsenter -t 1 -m -u -n -i sh If you want to build multi-platform docker images: docker buildx create --name mybuilder --driver docker-container --bootstrap docker buildx use mybuilder docker buildx inspect docker buildx ls Docker Compose. Multi-container deployments in a compose.yaml file. Using Compose in Production docker compose -p mastodon-bot-project up --detach # or docker compose -p mastodon-bot-project create docker compose -p mastodon-bot-project start # connect to a running container docker exec -it bot-app bash docker compose -p mastodon-bot-project down # or docker compose -p mastodon-bot-project stop Kubernetes # Convert the Docker Compose file to k8s files kompose --file compose.yaml convert # Make sure the container image is available in a repository. # You can build it with ```docker build``` or ```docker compose create``` and push it to a public repository. docker image push user/repo # multiple -f filenames or a folder kubectl apply -f ./k8s kubectl get pods kubectl delete -f ./k8s","title":"Examples"},{"location":"editor/","text":"Code Editor (VSCode/VSCodium) Q. VSCodium cannot be opened because Apple cannot check it for malicious software. A. You have to \"right click\" the .app and then hold the alt/option key, while clicking \"Open\" (only on first launch). Disable telemetry in: Code -> Settings -> Telemetry Settings -> Telemetry Level -> off . Useful extensions: Better TOML Compare Folders Draw.io Integration Remote - SSH","title":"Code Editor"},{"location":"editor/#code-editor-vscodevscodium","text":"Q. VSCodium cannot be opened because Apple cannot check it for malicious software. A. You have to \"right click\" the .app and then hold the alt/option key, while clicking \"Open\" (only on first launch). Disable telemetry in: Code -> Settings -> Telemetry Settings -> Telemetry Level -> off . Useful extensions: Better TOML Compare Folders Draw.io Integration Remote - SSH","title":"Code Editor (VSCode/VSCodium)"},{"location":"git/","text":"Git Install macOS brew install git git --version # GitHub CLI brew install gh gh auth login gh config set editor \"codium -w\" # or code, nano, etc GitHub CLI reference . Linux sudo yum install git # or git-all git --version Configure git config --global core.editor 'codium --wait' git config --global diff.tool codium git config --global difftool.codium.cmd 'codium --wait --diff $LOCAL $REMOTE' git config --global merge.tool codium git config --global mergetool.codium.cmd 'codium --wait $MERGED' # or codium ~/.gitconfig # and paste from sample dot file. Also: codium, vscode, nano Then you can git difftool main feature-branch . If using AWS CodeCommit do this after configuring the AWS CLI: git config --global credential.helper '!aws codecommit credential-helper $@' git config --global credential.UseHttpPath true Troubleshooting CodeCommit https://docs.aws.amazon.com/codecommit/latest/userguide/troubleshooting-ch.html#troubleshooting-macoshttps To create a squash function: git config --global alias.squash-all '!f(){ git reset $(git commit-tree \"HEAD^{tree}\" \"$@\");};f' :::info :information_source: - Git allows you to escape to a shell (like bash or zsh) using the ! (bang). Learn more . - commit-tree creates a new commit object based on the provided tree object and emits the new commit object id on stdout. - tree objects correspond to UNIX directory entries. - reset resets current HEAD to the specified state e.g a commit. - The master^{tree} syntax specifies the tree object that is pointed to by the last commit on your master branch. So HEAD^{tree} is the tree object pointed to by the last commit on your current branch. - If you\u2019re using ZSH, the ^ character is used for globbing, so you have to enclose the whole expression in quotes: \"HEAD^{tree}\" ::: Then just run: git squash-all -m \"a brand new start\" git push -f Set up git autocompletion (bash) cd ~ curl -OL https://github.com/git/git/raw/master/contrib/completion/git-completion.bash mv ~/git-completion.bash ~/.git-completion.bash Set up SSH key Check for existing keys. Generate a new SSH key and add it to ssh-agent. You may need to set permissions to your key file with chmod 600 . Add the public key to your GitHub account. If the terminal is no longer authenticating you: git remote -v # is it https or ssh? Should be ssh git remote remove origin git remote add origin git@github.com:user/repo.git Still having issues? Start the ssh-agent in the background and add your SSH private key to the ssh-agent. ps -ax | grep ssh-agent eval \"$(ssh-agent -s)\" ssh-add --apple-use-keychain ~/.ssh/id_ed25519 If you need to add your public key to Github again copy and paste it on your Settings page on Github: pbcopy < ~/.ssh/id_ed25519.pub","title":"Git"},{"location":"git/#git","text":"","title":"Git"},{"location":"git/#install","text":"macOS brew install git git --version # GitHub CLI brew install gh gh auth login gh config set editor \"codium -w\" # or code, nano, etc GitHub CLI reference . Linux sudo yum install git # or git-all git --version","title":"Install"},{"location":"git/#configure","text":"git config --global core.editor 'codium --wait' git config --global diff.tool codium git config --global difftool.codium.cmd 'codium --wait --diff $LOCAL $REMOTE' git config --global merge.tool codium git config --global mergetool.codium.cmd 'codium --wait $MERGED' # or codium ~/.gitconfig # and paste from sample dot file. Also: codium, vscode, nano Then you can git difftool main feature-branch . If using AWS CodeCommit do this after configuring the AWS CLI: git config --global credential.helper '!aws codecommit credential-helper $@' git config --global credential.UseHttpPath true Troubleshooting CodeCommit https://docs.aws.amazon.com/codecommit/latest/userguide/troubleshooting-ch.html#troubleshooting-macoshttps To create a squash function: git config --global alias.squash-all '!f(){ git reset $(git commit-tree \"HEAD^{tree}\" \"$@\");};f' :::info :information_source: - Git allows you to escape to a shell (like bash or zsh) using the ! (bang). Learn more . - commit-tree creates a new commit object based on the provided tree object and emits the new commit object id on stdout. - tree objects correspond to UNIX directory entries. - reset resets current HEAD to the specified state e.g a commit. - The master^{tree} syntax specifies the tree object that is pointed to by the last commit on your master branch. So HEAD^{tree} is the tree object pointed to by the last commit on your current branch. - If you\u2019re using ZSH, the ^ character is used for globbing, so you have to enclose the whole expression in quotes: \"HEAD^{tree}\" ::: Then just run: git squash-all -m \"a brand new start\" git push -f","title":"Configure"},{"location":"git/#set-up-git-autocompletion-bash","text":"cd ~ curl -OL https://github.com/git/git/raw/master/contrib/completion/git-completion.bash mv ~/git-completion.bash ~/.git-completion.bash","title":"Set up git autocompletion (bash)"},{"location":"git/#set-up-ssh-key","text":"Check for existing keys. Generate a new SSH key and add it to ssh-agent. You may need to set permissions to your key file with chmod 600 . Add the public key to your GitHub account. If the terminal is no longer authenticating you: git remote -v # is it https or ssh? Should be ssh git remote remove origin git remote add origin git@github.com:user/repo.git Still having issues? Start the ssh-agent in the background and add your SSH private key to the ssh-agent. ps -ax | grep ssh-agent eval \"$(ssh-agent -s)\" ssh-add --apple-use-keychain ~/.ssh/id_ed25519 If you need to add your public key to Github again copy and paste it on your Settings page on Github: pbcopy < ~/.ssh/id_ed25519.pub","title":"Set up SSH key"},{"location":"install-os/","text":"Do a clean install of the OS On macOS download the new version from System Preferences, Software Update (or the Mac App Store) and create the bootable media with: sudo /Applications/Install\\ macOS\\ Ventura.app/Contents/Resources/createinstallmedia --volume /Volumes/MyVolume/ using the volume that matches the name of the external drive you are using. If you have an Intel-powered Mac here's how to install macOS from a bootable installer: 1. Plug in your bootable media. 2. Turn off your Mac. 3. Press and hold Option/Alt while the Mac starts up - keep pressing the key until you see a screen showing the bootable volume. If you have an Apple silicon Mac here's how to install macOS from a bootable installer: 1. Plug in your bootable media. 2. Turn off your Mac. 3. Press the power button to turn on the Mac - but keep it pressed until you see the startup options window including your bootable volume. Open Finder and show hidden files with Command + Shift + . (period) or with defaults write com.apple.finder AppleShowAllFiles TRUE; killall Finder","title":"Install the OS"},{"location":"install-os/#do-a-clean-install-of-the-os","text":"On macOS download the new version from System Preferences, Software Update (or the Mac App Store) and create the bootable media with: sudo /Applications/Install\\ macOS\\ Ventura.app/Contents/Resources/createinstallmedia --volume /Volumes/MyVolume/ using the volume that matches the name of the external drive you are using. If you have an Intel-powered Mac here's how to install macOS from a bootable installer: 1. Plug in your bootable media. 2. Turn off your Mac. 3. Press and hold Option/Alt while the Mac starts up - keep pressing the key until you see a screen showing the bootable volume. If you have an Apple silicon Mac here's how to install macOS from a bootable installer: 1. Plug in your bootable media. 2. Turn off your Mac. 3. Press the power button to turn on the Mac - but keep it pressed until you see the startup options window including your bootable volume. Open Finder and show hidden files with Command + Shift + . (period) or with defaults write com.apple.finder AppleShowAllFiles TRUE; killall Finder","title":"Do a clean install of the OS"},{"location":"k8s/","text":"Kubernetes Kubernetes Essentials Interactive Diagram Working with k8s objects Log rotation In order for Kubernetes to pull your container image you need to first push it to an image repository like Docker Hub. To avoid storing your Docker Hub password unencrypted in $HOME/.docker/config.json when you docker login to your account, use a credentials store . A helper program lets you interact with such a keychain or external store. If you're doing this on your laptop with Docker Desktop it already provides a store . Otherwise , use one of the stores supported by the docker-credential-helper . Now docker login on your terminal or on the Docker Desktop app. Designing Applications for Kubernetes Based on the 12-Factor App Design Methodology A Cloud Guru course. It uses Ubuntu 20.04 Focal Fossa LTS and the Calico network plugin instead of Flannel. Example with 1 control plane node and 2 worker nodes. Building a Kubernetes Cluster Reference Installing kubeadm Creating a cluster with kubeadm * kubeadm sometimes doesn't work with the latest and greatest version of docker right away. kubeadm simplifies the process of setting up a k8s cluster. containerd manages the complete container lifecycle of its host system, from image transfer and storage to container execution and supervision to low-level storage to network attachments. kubelet handles running containers on a node. kubectl is a tool for managing the cluster. If you wish, you can set an appropriate hostname for each node. On the control plane node : sudo hostnamectl set-hostname k8s-control On the first worker node : sudo hostnamectl set-hostname k8s-worker1 On the second worker node : sudo hostnamectl set-hostname k8s-worker2 On all nodes , set up the hosts file to enable all the nodes to reach each other using these hostnames. sudo nano /etc/hosts On all nodes , add the following at the end of the file. You will need to supply the actual private IP address for each node. <control plane node private IP> k8s-control <worker node 1 private IP> k8s-worker1 <worker node 2 private IP> k8s-worker2 Log out of all three servers and log back in to see these changes take effect. On all nodes, set up containerd . You will need to load some kernel modules and modify some system settings as part of this process. # Enable them when the server start up cat << EOF | sudo tee /etc/modules-load.d/containerd.conf overlay br_netfilter EOF # Enable them right now without having to restart the server sudo modprobe overlay sudo modprobe br_netfilter # Add network configurations k8s will need cat <<EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 EOF # Apply them immediately sudo sysctl --system Install and configure containerd. sudo apt-get update && sudo apt-get install -y containerd sudo mkdir -p /etc/containerd # Generate the contents of a default config file and save it sudo containerd config default | sudo tee /etc/containerd/config.toml # Restart containerd to make sure it's using that configuration sudo systemctl restart containerd On all nodes, disable swap . sudo swapoff -a On all nodes, install kubeadm, kubelet, and kubectl . # Some required packages sudo apt-get update && sudo apt-get install -y apt-transport-https curl # Set up the package repo for k8s packages. Download the key for the repo and add it curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - # Configure the repo cat << EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF sudo apt-get update # Install the k8s packages. Make sure the versions for all 3 are the same. sudo apt-get install -y kubelet=1.24.0-00 kubeadm=1.24.0-00 kubectl=1.24.0-00 # Make sure they're not automatically upgraded. Have manual control over when to update k8s sudo apt-mark hold kubelet kubeadm kubectl On the control plane node only , initialize the cluster and set up kubectl access. sudo kubeadm init --pod-network-cidr 192.168.0.0/16 --kubernetes-version 1.24.0 # Config File to authenticate and interact with the cluster with kubectl commands # These are in the output of the previous step mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Verify the cluster is working. It will be in Not Ready status because we haven't configured the networking plugin. kubectl get nodes Install the Calico network add-on. kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml Get the join command (this command is also printed during kubeadm init . Feel free to simply copy it from there). kubeadm token create --print-join-command Copy the join command from the control plane node. Run it on each worker node as root (i.e. with sudo ). sudo kubeadm join ... On the control plane node , verify all nodes in your cluster are ready. Note that it may take a few moments for all of the nodes to enter the READY state. kubectl get nodes Installing Docker Reference Install Docker Engine on Ubuntu Docker credentials store to avoid storing your Docker Hub password unencrypted in $HOME/.docker/config.json when you docker login and docker push your images. On the system that will build Docker images from source code e.g. a CI server, install and configure Docker. For simplicity we'll use the control plane server just so we don't have to create another server for this exercise. Create a docker group. Users in this group will have permission to use Docker on the system: sudo groupadd docker Install required packages. Note: Some of these packages may already be present on the system, but including them here will not cause any problems: sudo apt-get update && sudo apt-get install -y apt-transport-https ca-certificates curl gnupg lsb-release Set up the Docker GPG key and package repository: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg echo \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null Install the Docker Engine: sudo apt-get update && sudo apt-get install -y docker-ce docker-ce-cli # Type N (default) or enter to keep your current containerd configuration Test the Docker setup: sudo docker version Add cloud_user to the docker group in order to give cloud_user access to use Docker: sudo usermod -aG docker cloud_user Log out of the server and log back in. Test your setup: docker version Kubernetes configuration with ConfigMaps and Secrets III. Config Store config in the environment Reference Encrypting Secret Data at Rest ConfigMaps Secrets Create a production Namespace: kubectl create namespace production Get base64-encoded strings for a db username and password: echo -n my_user | base64 echo -n my_password | base64 Example: Create a ConfigMap and Secret to configure the backing service connection information for the app, including the base64-encoded credentials: cat > my-app-config.yml <<End-of-message apiVersion: v1 kind: ConfigMap metadata: name: my-app-config data: mongodb.host: \"my-app-mongodb\" mongodb.port: \"27017\" --- apiVersion: v1 kind: Secret metadata: name: my-app-secure-config type: Opaque data: mongodb.username: dWxvZV91c2Vy mongodb.password: SUxvdmVUaGVMaXN0 End-of-message kubectl apply -f my-app-config.yml -n production Create a temporary Pod to test the configuration setup. Note that you need to supply your Docker Hub username as part of the image name in this file. This passes configuration data in env variables but you could also do it in files that will show up on the containers filesystem. cat > test-pod.yml <<End-of-message apiVersion: v1 kind: Pod metadata: name: test-pod spec: containers: - name: my-app-server image: <YOUR_DOCKER_HUB_USERNAME>/my-app-server:0.0.1 ports: - name: web containerPort: 3001 protocol: TCP env: - name: MONGODB_HOST valueFrom: configMapKeyRef: name: my-app-config key: mongodb.host - name: MONGODB_PORT valueFrom: configMapKeyRef: name: my-app-config key: mongodb.port - name: MONGODB_USER valueFrom: secretKeyRef: name: my-app-secure-config key: mongodb.username - name: MONGODB_PASSWORD valueFrom: secretKeyRef: name: my-app-secure-config key: mongodb.password End-of-message kubectl apply -f test-pod.yml -n production Check the logs to verify the config data is being passed to the container: kubectl logs test-pod -n production Clean up the test pod: kubectl delete pod test-pod -n production --force Build, Release, Run with Docker and Deployments V. Build, release, run Strictly separate build and run stages Reference Labels and Selectors Deployments Example: After you docker build and docker push your image to a repository, create a deployment file for your app. The selector selects pods that have the specified label name and value. template is the pod template. This example puts 2 containers in the same pod for simplicity but in the real world you'll want separate deployments to scale them independently. cat > my-app.yml <<End-of-message apiVersion: v1 kind: ConfigMap metadata: name: my-app-config data: mongodb.host: \"my-app-mongodb\" mongodb.port: \"27017\" .env: | REACT_APP_API_PORT=\"30081\" --- apiVersion: v1 kind: Secret metadata: name: my-app-secure-config type: Opaque data: mongodb.username: dWxvZV91c2Vy mongodb.password: SUxvdmVUaGVMaXN0 --- apiVersion: v1 kind: Service metadata: name: my-app-svc spec: type: NodePort selector: app: my-app ports: - name: frontend protocol: TCP port: 30080 nodePort: 30080 targetPort: 5000 - name: server protocol: TCP port: 30081 nodePort: 30081 targetPort: 3001 --- apiVersion: apps/v1 kind: Deployment metadata: name: my-app spec: replicas: 1 selector: matchLabels: app: my-app template: metadata: labels: app: my-app spec: containers: - name: my-app-server image: <Your Docker Hub username>/my-app-server:0.0.1 ports: - name: web containerPort: 3001 protocol: TCP env: - name: MONGODB_HOST valueFrom: configMapKeyRef: name: my-app-config key: mongodb.host - name: MONGODB_PORT valueFrom: configMapKeyRef: name: my-app-config key: mongodb.port - name: MONGODB_USER valueFrom: secretKeyRef: name: my-app-secure-config key: mongodb.username - name: MONGODB_PASSWORD valueFrom: secretKeyRef: name: my-app-secure-config key: mongodb.password - name: my-app-frontend image: <Your Docker Hub username>/my-app-frontend:0.0.1 ports: - name: web containerPort: 5000 protocol: TCP volumeMounts: - name: frontend-config mountPath: /usr/src/app/.env subPath: .env readOnly: true volumes: - name: frontend-config configMap: name: my-app-config items: - key: .env path: .env End-of-message Deploy the app. kubectl apply -f my-app.yml -n production Create a new container image version to test the rollout process: docker tag <Your Docker Hub username>/my-app-frontend:0.0.1 <Your Docker Hub username>/my-app-frontend:0.0.2 docker push <Your Docker Hub username>/my-app-frontend:0.0.2 Edit the app manifest my-app.yml to use the 0.0.2 image version and then: kubectl apply -f my-app.yml -n production Get the list of Pods to see the new version rollout: kubectl get pods -n production Processes with stateless containers VI. Processes Execute the app as one or more stateless processes Reference Security Context Edit the app deployment YAML my-app.yml . In the deployment Pod spec, add a new emptyDir volume under volumes: volumes: - name: added-items-log emptyDir: {} ... Mount the volume to the server container: containers: ... - name: my-app-server ... volumeMounts: - name: added-items-log mountPath: /usr/src/app/added_items.log subPath: added_items.log readOnly: false ... Make the container file system read only: containers: ... - name: my-app-server securityContext: readOnlyRootFilesystem: true ... Deploy the changes: kubectl apply -f my-app.yml -n production Persistent Volumes Reference Persistent Volumes (PV) local PV Create a StorageClass that supports volume expansion as localdisk-sc.yml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: localdisk provisioner: kubernetes.io/no-provisioner allowVolumeExpansion: true kubectl create -f localdisk-sc.yml persistentVolumeReclaimPolicy says how storage can be reused when the volume's associated claims are deleted. - Retain: Keeps all data. An admin must manually clean up and prepare the resource for reuse. - Recycle: Automatically deletes all data, allowing the volume to be reused. - Delete: Deletes underlying storage resource automatically (applies to cloud only). accessModes can be: - ReadWriteOnce: The volume can be mounted as read-write by a single node. Still can allow multiple pods to access the volume when the pods are running on the same node. - ReadOnlyMany: Can be mounted as read-only by many nodes. - ReadWriteMany: Can be mounted as read-write by many nodes. - ReadWriteOncePod: Can be mounted as read-write by a single Pod. Use ReadWriteOncePod access mode if you want to ensure that only one pod across whole cluster can read that PVC or write to it. This is only supported for CSI volumes. Create a PersistentVolume in my-pv.yml . kind: PersistentVolume apiVersion: v1 metadata: name: my-pv spec: storageClassName: localdisk persistentVolumeReclaimPolicy: Recycle capacity: storage: 1Gi accessModes: - ReadWriteOnce hostPath: path: /var/output kubectl create -f my-pv.yml Check the status of the PersistentVolume. kubectl get pv Create a PersistentVolumeClaim that will bind to the PersistentVolume as my-pvc.yml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-pvc spec: storageClassName: localdisk accessModes: - ReadWriteOnce resources: requests: storage: 100Mi kubectl create -f my-pvc.yml Check the status of the PersistentVolume and PersistentVolumeClaim to verify that they have been bound. kubectl get pv kubectl get pvc Create a Pod that uses the PersistentVolumeClaim as pv-pod.yml . apiVersion: v1 kind: Pod metadata: name: pv-pod spec: restartPolicy: Never containers: - name: busybox image: busybox command: ['sh', '-c', 'echo Success! > /output/success.txt'] volumeMounts: - name: pv-storage mountPath: /output volumes: - name: pv-storage persistentVolumeClaim: claimName: my-pvc kubectl create -f pv-pod.yml Expand the PersistentVolumeClaim and record the process. kubectl edit pvc my-pvc --record ... spec: ... resources: requests: storage: 200Mi Delete the Pod and the PersistentVolumeClaim. kubectl delete pod pv-pod kubectl delete pvc my-pvc Check the status of the PersistentVolume to verify that it has been successfully recycled and is available again. kubectl get pv Port Binding with Pods VII. Port binding Export services via port binding Reference Cluster Networking Challenge: only 1 process can listen on a port per host. So how do all apps on the host use a unique port? In k8s, each pod has its own network namespace and cluster IP address. That IP address is unique within the cluster even if there are multiple worker nodes in the cluster. Tht means ports only need to be unique within each pod. 2 pods can listen on the same port because they each have their own unique IP address within the cluster network. The pods can communicate across nodes simply using the unique IPs. Get a list of Pods in the production namespace: kubectl get pods -n production -o wide Copy the name of the IP address of the application Pod. Example: Use the IP address to make a request to the port on the Pod that serves the frontend content: curl <Pod Cluster IP address>:5000 Concurrency with Containers and Scaling VIII. Concurrency Scale out via the process model Reference Deployments By using services to manage access to the app, the service automaticaly picks up the additional pods created during scaling and route traffic to those pods. When you have services alongside deployments you can dynamically change the number of replicas that you have and k8s will take care of everything. Edit the application deployment my-app.yml . Change the number of replicas to 3: ... replicas: 3 Apply the changes: kubectl apply -f my-app.yml -n production Get a list of Pods: kubectl get pods -n production Scale the deployment up again in my-app.yml . Change the number of replicas to 5: ... replicas: 5 Apply the changes: kubectl apply -f my-app .yml -n production Get a list of Pods: kubectl get pods -n production Disposability with Stateless Containers IX. Disposability Maximize robustness with fast startup and graceful shutdown Reference Security Context Pod Lifecycle Deployments can be used to maintain a specified number of running replicas automatically replacing pods that fail or are deleted. Get a list of Pods: kubectl get pods -n production Locate one of the Pods from the my-app deployment and copy the Pod's name. Delete the Pod using the Pod name: kubectl delete pod <Pod name> -n production Get the list of Pods again. You will notice that the deployment is automatically creating a new Pod to replace the one that was deleted: kubectl get pods -n production Dev/Prod Parity with Namespaces X. Dev/prod parity Keep development, staging, and production as similar as possible Reference Namespaces k8s namespaces allow us to have multiple environments in the same cluster. A namespace is like a virtual cluster. Create a new namespace: kubectl create namespace dev Make a copy of the my-app app YAML: cp my-app.yml my-app-dev.yml NodePort services need to be unique within the cluster. We need to choose unique ports so dev doesn't conflict with production. Edit the my-app-svc service in the my-app-dev.yml file to select different nodePort s. You will also need to edit the my-app-config ConfigMap to reflect the new port. Set the nodePorts on the service: apiVersion: v1 kind: Service metadata: name: my-app-svc spec: type: NodePort selector: app: my-app ports: - name: frontend protocol: TCP port: 30082 nodePort: 30082 targetPort: 5000 - name: server protocol: TCP port: 30083 nodePort: 30083 targetPort: 3001 Update the configured port in the ConfigMap: apiVersion: v1 kind: ConfigMap metadata: name: my-app-config data: mongodb.host: \"my-app-mongodb\" mongodb.port: \"27017\" .env: | REACT_APP_API_PORT=\"30083\" Deploy the backing service setup in the new namespace: kubectl apply -f k8s-my-app-mongodb.yml -n dev kubectl apply -f my-app-mongodb.yml -n dev Deploy the app in the new namespace: kubectl apply -f my-app-dev.yml -n dev Check the status of the Pods: kubectl get pods -n dev Once all the Pods are up and running, you should be able to test the dev environment in a browser at <Control Plane Node Public IP>:30082 . Logs with k8s Container Logging XI. Logs Treat logs as event streams Reference Logging Architecture Kubectl cheatsheet k8s captures log data written to stdout by containers. We can use the k8s API, kubectl logs or external tools to interact with container logs. Edit the source code for the server e.g. src/server/index.js . There is a log function that writes to a file. Change this function to simply write log data to the console: log = function(data) { console.log(data); } Build a new server image because we changed the source code: docker build -t <Your Docker Hub username>/my-app-server:0.0.4 --target server . Push the image: docker push <Your Docker Hub username>/my-app-server:0.0.4 Deploy the new code. Edit my-app.yml . Change the image version for the server to the new image: containers: - name: my-app-server image: <Your Docker Hub username>/my-app-server:0.0.4 kubectl apply -f my-app.yml -n production Get a list of Pods: kubectl get pods -n production Copy the name of one of the my-app deployment Pods and view its logs specifying the pod, namespace, and container. kubectl logs <Pod name> -n production -c my-app-server Admin Processes with Jobs XII. Admin processes Run admin/management tasks as one-off processes Reference Jobs A Job e.g. a database migration runs a container until its execution completes. Jobs handle re-trying execution if it fails. Jobs have a restartPolicy of Never because once they complete they don't run again. This example adds the administrative job to the server image but you could package it into its own image. Edit the source code for the admin process in e.g. src/jobs/deDeuplicateJob.js . Locate the block of code that begins with // Setup MongoDb backing database . Change the code to make the database connection configurable: // Setup MongoDb backing database const MongoClient = require('mongodb').MongoClient // MongoDB credentials const username = encodeURIComponent(process.env.MONGODB_USER || \"my-app_user\"); const password = encodeURIComponent(process.env.MONGODB_PASSWORD || \"ILoveTheList\"); // MongoDB connection info const mongoPort = process.env.MONGODB_PORT || 27017; const mongoHost = process.env.MONGODB_HOST || 'localhost'; // MongoDB connection string const mongoURI = `mongodb://${username}:${password}@${mongoHost}:${mongoPort}/my-app`; const mongoURISanitized = `mongodb://${username}:****@${mongoHost}:${mongoPort}/my-app`; console.log(\"MongoDB connection string %s\", mongoURISanitized); const client = new MongoClient(mongoURI); Edit the Dockerfile to include the admin job code in the server image. Add the following line after the other COPY directives for the server image: ... COPY --from=build /usr/src/app/src/jobs . Build and push the server image: docker build -t <Your Docker Hub username>/my-app-server:0.0.5 --target server . docker push <Your Docker Hub username>/my-app-server:0.0.5 Create a Kubernetes Job to run the admin job: Create de-duplicate-job.yml . Supply your Docker Hub username in the image tag: apiVersion: batch/v1 kind: Job metadata: name: de-duplicate spec: template: spec: containers: - name: my-app-server image: <Your Docker Hub username>/my-app-server:0.0.5 command: [\"node\", \"deDeuplicateJob.js\"] env: - name: MONGODB_HOST valueFrom: configMapKeyRef: name: my-app-config key: mongodb.host - name: MONGODB_PORT valueFrom: configMapKeyRef: name: my-app-config key: mongodb.port - name: MONGODB_USER valueFrom: secretKeyRef: name: my-app-secure-config key: mongodb.username - name: MONGODB_PASSWORD valueFrom: secretKeyRef: name: my-app-secure-config key: mongodb.password restartPolicy: Never backoffLimit: 4 Run the Job: kubectl apply -f de-duplicate-job.yml -n production Check the Job status: kubectl get jobs -n production Get the name of the Job Pod: kubectl get pods -n production Use the Pod name to view the logs for the Job Pod: kubectl logs <Pod name> -n production MicroK8s On Raspberry Pi Note: Your boot parameters might be in /boot/cmdline.txt . Add these options at the end of the file, then sudo reboot . cgroup_enable=cpuset cgroup_enable=memory cgroup_memory=1 For Raspberry Pi OS install snap first. sudo apt update sudo apt install snapd sudo reboot # ...reconnect after reboot sudo snap install core Then install MicroK8s. pi@raspberrypi4:~ $ sudo snap install microk8s --classic pi@raspberrypi4:~ $ microk8s status --wait-ready pi@raspberrypi4:~ $ microk8s kubectl get all --all-namespaces pi@raspberrypi4:~ $ microk8s enable dns dashboard registry hostpath-storage # or any other addons pi@raspberrypi4:~ $ alias mkctl=\"microk8s kubectl\" pi@raspberrypi4:~ $ alias mkhelm=\"microk8s helm\" pi@raspberrypi4:~ $ mkctl create deployment nginx --image nginx pi@raspberrypi4:~ $ mkctl expose deployment nginx --port 80 --target-port 80 --selector app=nginx --type ClustetIP --name nginx pi@raspberrypi4:~ $ watch microk8s kubectl get all pi@raspberrypi4:~ $ microk8s reset pi@raspberrypi4:~ $ microk8s status pi@raspberrypi4:~ $ microk8s stop # microk8s start pi@raspberrypi4:~ $ microk8s kubectl version --output=yaml You can update a snap package with sudo snap refresh . Configuration file. These are the arguments you can add regarding log rotation --container-log-max-files and --container-log-max-size . They have default values. More info . cat /var/snap/microk8s/current/args/kubelet Registry Registry doc microk8s enable registry The containerd daemon used by MicroK8s is configured to trust this insecure registry. To upload images we have to tag them with localhost:32000/your-image before pushing them. MicroK8s dashboard If RBAC is not enabled access the dashboard using the token retrieved with: microk8s kubectl describe secret -n kube-system microk8s-dashboard-token Use this token in the https login UI of the kubernetes-dashboard service. In an RBAC enabled setup ( microk8s enable rbac ) you need to create a user with restricted permissions as shown here . To access remotely from anywhere with port-forward : microk8s kubectl port-forward -n kube-system service/kubernetes-dashboard 10443:443 --address 0.0.0.0 You can then access the Dashboard with IP or hostname as in https://raspberrypi4.local:10443/ Troubleshooting microk8s inspect MicroK8s might not recognize that cgroup memory is enabled but you can check with cat /proc/cgroups . Kubernetes Dashboard Documentation kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.6.1/aio/deploy/recommended.yaml cat << EOF > dashboard-adminuser.yaml apiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kubernetes-dashboard --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kubernetes-dashboard EOF kubectl apply -f dashboard-adminuser.yaml kubectl proxy # Kubectl will make Dashboard available at # http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/ kubectl -n kubernetes-dashboard create token admin-user # Now copy the token and paste it into the Enter token field on the login screen.","title":"Kubernetes"},{"location":"k8s/#kubernetes","text":"","title":"Kubernetes"},{"location":"k8s/#kubernetes-essentials","text":"Interactive Diagram Working with k8s objects Log rotation In order for Kubernetes to pull your container image you need to first push it to an image repository like Docker Hub. To avoid storing your Docker Hub password unencrypted in $HOME/.docker/config.json when you docker login to your account, use a credentials store . A helper program lets you interact with such a keychain or external store. If you're doing this on your laptop with Docker Desktop it already provides a store . Otherwise , use one of the stores supported by the docker-credential-helper . Now docker login on your terminal or on the Docker Desktop app.","title":"Kubernetes Essentials"},{"location":"k8s/#designing-applications-for-kubernetes","text":"Based on the 12-Factor App Design Methodology A Cloud Guru course. It uses Ubuntu 20.04 Focal Fossa LTS and the Calico network plugin instead of Flannel. Example with 1 control plane node and 2 worker nodes.","title":"Designing Applications for Kubernetes"},{"location":"k8s/#building-a-kubernetes-cluster","text":"Reference Installing kubeadm Creating a cluster with kubeadm * kubeadm sometimes doesn't work with the latest and greatest version of docker right away. kubeadm simplifies the process of setting up a k8s cluster. containerd manages the complete container lifecycle of its host system, from image transfer and storage to container execution and supervision to low-level storage to network attachments. kubelet handles running containers on a node. kubectl is a tool for managing the cluster. If you wish, you can set an appropriate hostname for each node. On the control plane node : sudo hostnamectl set-hostname k8s-control On the first worker node : sudo hostnamectl set-hostname k8s-worker1 On the second worker node : sudo hostnamectl set-hostname k8s-worker2 On all nodes , set up the hosts file to enable all the nodes to reach each other using these hostnames. sudo nano /etc/hosts On all nodes , add the following at the end of the file. You will need to supply the actual private IP address for each node. <control plane node private IP> k8s-control <worker node 1 private IP> k8s-worker1 <worker node 2 private IP> k8s-worker2 Log out of all three servers and log back in to see these changes take effect. On all nodes, set up containerd . You will need to load some kernel modules and modify some system settings as part of this process. # Enable them when the server start up cat << EOF | sudo tee /etc/modules-load.d/containerd.conf overlay br_netfilter EOF # Enable them right now without having to restart the server sudo modprobe overlay sudo modprobe br_netfilter # Add network configurations k8s will need cat <<EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 EOF # Apply them immediately sudo sysctl --system Install and configure containerd. sudo apt-get update && sudo apt-get install -y containerd sudo mkdir -p /etc/containerd # Generate the contents of a default config file and save it sudo containerd config default | sudo tee /etc/containerd/config.toml # Restart containerd to make sure it's using that configuration sudo systemctl restart containerd On all nodes, disable swap . sudo swapoff -a On all nodes, install kubeadm, kubelet, and kubectl . # Some required packages sudo apt-get update && sudo apt-get install -y apt-transport-https curl # Set up the package repo for k8s packages. Download the key for the repo and add it curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - # Configure the repo cat << EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF sudo apt-get update # Install the k8s packages. Make sure the versions for all 3 are the same. sudo apt-get install -y kubelet=1.24.0-00 kubeadm=1.24.0-00 kubectl=1.24.0-00 # Make sure they're not automatically upgraded. Have manual control over when to update k8s sudo apt-mark hold kubelet kubeadm kubectl On the control plane node only , initialize the cluster and set up kubectl access. sudo kubeadm init --pod-network-cidr 192.168.0.0/16 --kubernetes-version 1.24.0 # Config File to authenticate and interact with the cluster with kubectl commands # These are in the output of the previous step mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Verify the cluster is working. It will be in Not Ready status because we haven't configured the networking plugin. kubectl get nodes Install the Calico network add-on. kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml Get the join command (this command is also printed during kubeadm init . Feel free to simply copy it from there). kubeadm token create --print-join-command Copy the join command from the control plane node. Run it on each worker node as root (i.e. with sudo ). sudo kubeadm join ... On the control plane node , verify all nodes in your cluster are ready. Note that it may take a few moments for all of the nodes to enter the READY state. kubectl get nodes","title":"Building a Kubernetes Cluster"},{"location":"k8s/#installing-docker","text":"Reference Install Docker Engine on Ubuntu Docker credentials store to avoid storing your Docker Hub password unencrypted in $HOME/.docker/config.json when you docker login and docker push your images. On the system that will build Docker images from source code e.g. a CI server, install and configure Docker. For simplicity we'll use the control plane server just so we don't have to create another server for this exercise. Create a docker group. Users in this group will have permission to use Docker on the system: sudo groupadd docker Install required packages. Note: Some of these packages may already be present on the system, but including them here will not cause any problems: sudo apt-get update && sudo apt-get install -y apt-transport-https ca-certificates curl gnupg lsb-release Set up the Docker GPG key and package repository: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg echo \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null Install the Docker Engine: sudo apt-get update && sudo apt-get install -y docker-ce docker-ce-cli # Type N (default) or enter to keep your current containerd configuration Test the Docker setup: sudo docker version Add cloud_user to the docker group in order to give cloud_user access to use Docker: sudo usermod -aG docker cloud_user Log out of the server and log back in. Test your setup: docker version","title":"Installing Docker"},{"location":"k8s/#kubernetes-configuration-with-configmaps-and-secrets","text":"III. Config Store config in the environment Reference Encrypting Secret Data at Rest ConfigMaps Secrets Create a production Namespace: kubectl create namespace production Get base64-encoded strings for a db username and password: echo -n my_user | base64 echo -n my_password | base64 Example: Create a ConfigMap and Secret to configure the backing service connection information for the app, including the base64-encoded credentials: cat > my-app-config.yml <<End-of-message apiVersion: v1 kind: ConfigMap metadata: name: my-app-config data: mongodb.host: \"my-app-mongodb\" mongodb.port: \"27017\" --- apiVersion: v1 kind: Secret metadata: name: my-app-secure-config type: Opaque data: mongodb.username: dWxvZV91c2Vy mongodb.password: SUxvdmVUaGVMaXN0 End-of-message kubectl apply -f my-app-config.yml -n production Create a temporary Pod to test the configuration setup. Note that you need to supply your Docker Hub username as part of the image name in this file. This passes configuration data in env variables but you could also do it in files that will show up on the containers filesystem. cat > test-pod.yml <<End-of-message apiVersion: v1 kind: Pod metadata: name: test-pod spec: containers: - name: my-app-server image: <YOUR_DOCKER_HUB_USERNAME>/my-app-server:0.0.1 ports: - name: web containerPort: 3001 protocol: TCP env: - name: MONGODB_HOST valueFrom: configMapKeyRef: name: my-app-config key: mongodb.host - name: MONGODB_PORT valueFrom: configMapKeyRef: name: my-app-config key: mongodb.port - name: MONGODB_USER valueFrom: secretKeyRef: name: my-app-secure-config key: mongodb.username - name: MONGODB_PASSWORD valueFrom: secretKeyRef: name: my-app-secure-config key: mongodb.password End-of-message kubectl apply -f test-pod.yml -n production Check the logs to verify the config data is being passed to the container: kubectl logs test-pod -n production Clean up the test pod: kubectl delete pod test-pod -n production --force","title":"Kubernetes configuration with ConfigMaps and Secrets"},{"location":"k8s/#build-release-run-with-docker-and-deployments","text":"V. Build, release, run Strictly separate build and run stages Reference Labels and Selectors Deployments Example: After you docker build and docker push your image to a repository, create a deployment file for your app. The selector selects pods that have the specified label name and value. template is the pod template. This example puts 2 containers in the same pod for simplicity but in the real world you'll want separate deployments to scale them independently. cat > my-app.yml <<End-of-message apiVersion: v1 kind: ConfigMap metadata: name: my-app-config data: mongodb.host: \"my-app-mongodb\" mongodb.port: \"27017\" .env: | REACT_APP_API_PORT=\"30081\" --- apiVersion: v1 kind: Secret metadata: name: my-app-secure-config type: Opaque data: mongodb.username: dWxvZV91c2Vy mongodb.password: SUxvdmVUaGVMaXN0 --- apiVersion: v1 kind: Service metadata: name: my-app-svc spec: type: NodePort selector: app: my-app ports: - name: frontend protocol: TCP port: 30080 nodePort: 30080 targetPort: 5000 - name: server protocol: TCP port: 30081 nodePort: 30081 targetPort: 3001 --- apiVersion: apps/v1 kind: Deployment metadata: name: my-app spec: replicas: 1 selector: matchLabels: app: my-app template: metadata: labels: app: my-app spec: containers: - name: my-app-server image: <Your Docker Hub username>/my-app-server:0.0.1 ports: - name: web containerPort: 3001 protocol: TCP env: - name: MONGODB_HOST valueFrom: configMapKeyRef: name: my-app-config key: mongodb.host - name: MONGODB_PORT valueFrom: configMapKeyRef: name: my-app-config key: mongodb.port - name: MONGODB_USER valueFrom: secretKeyRef: name: my-app-secure-config key: mongodb.username - name: MONGODB_PASSWORD valueFrom: secretKeyRef: name: my-app-secure-config key: mongodb.password - name: my-app-frontend image: <Your Docker Hub username>/my-app-frontend:0.0.1 ports: - name: web containerPort: 5000 protocol: TCP volumeMounts: - name: frontend-config mountPath: /usr/src/app/.env subPath: .env readOnly: true volumes: - name: frontend-config configMap: name: my-app-config items: - key: .env path: .env End-of-message Deploy the app. kubectl apply -f my-app.yml -n production Create a new container image version to test the rollout process: docker tag <Your Docker Hub username>/my-app-frontend:0.0.1 <Your Docker Hub username>/my-app-frontend:0.0.2 docker push <Your Docker Hub username>/my-app-frontend:0.0.2 Edit the app manifest my-app.yml to use the 0.0.2 image version and then: kubectl apply -f my-app.yml -n production Get the list of Pods to see the new version rollout: kubectl get pods -n production","title":"Build, Release, Run with Docker and Deployments"},{"location":"k8s/#processes-with-stateless-containers","text":"VI. Processes Execute the app as one or more stateless processes Reference Security Context Edit the app deployment YAML my-app.yml . In the deployment Pod spec, add a new emptyDir volume under volumes: volumes: - name: added-items-log emptyDir: {} ... Mount the volume to the server container: containers: ... - name: my-app-server ... volumeMounts: - name: added-items-log mountPath: /usr/src/app/added_items.log subPath: added_items.log readOnly: false ... Make the container file system read only: containers: ... - name: my-app-server securityContext: readOnlyRootFilesystem: true ... Deploy the changes: kubectl apply -f my-app.yml -n production","title":"Processes with stateless containers"},{"location":"k8s/#persistent-volumes","text":"Reference Persistent Volumes (PV) local PV Create a StorageClass that supports volume expansion as localdisk-sc.yml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: localdisk provisioner: kubernetes.io/no-provisioner allowVolumeExpansion: true kubectl create -f localdisk-sc.yml persistentVolumeReclaimPolicy says how storage can be reused when the volume's associated claims are deleted. - Retain: Keeps all data. An admin must manually clean up and prepare the resource for reuse. - Recycle: Automatically deletes all data, allowing the volume to be reused. - Delete: Deletes underlying storage resource automatically (applies to cloud only). accessModes can be: - ReadWriteOnce: The volume can be mounted as read-write by a single node. Still can allow multiple pods to access the volume when the pods are running on the same node. - ReadOnlyMany: Can be mounted as read-only by many nodes. - ReadWriteMany: Can be mounted as read-write by many nodes. - ReadWriteOncePod: Can be mounted as read-write by a single Pod. Use ReadWriteOncePod access mode if you want to ensure that only one pod across whole cluster can read that PVC or write to it. This is only supported for CSI volumes. Create a PersistentVolume in my-pv.yml . kind: PersistentVolume apiVersion: v1 metadata: name: my-pv spec: storageClassName: localdisk persistentVolumeReclaimPolicy: Recycle capacity: storage: 1Gi accessModes: - ReadWriteOnce hostPath: path: /var/output kubectl create -f my-pv.yml Check the status of the PersistentVolume. kubectl get pv Create a PersistentVolumeClaim that will bind to the PersistentVolume as my-pvc.yml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-pvc spec: storageClassName: localdisk accessModes: - ReadWriteOnce resources: requests: storage: 100Mi kubectl create -f my-pvc.yml Check the status of the PersistentVolume and PersistentVolumeClaim to verify that they have been bound. kubectl get pv kubectl get pvc Create a Pod that uses the PersistentVolumeClaim as pv-pod.yml . apiVersion: v1 kind: Pod metadata: name: pv-pod spec: restartPolicy: Never containers: - name: busybox image: busybox command: ['sh', '-c', 'echo Success! > /output/success.txt'] volumeMounts: - name: pv-storage mountPath: /output volumes: - name: pv-storage persistentVolumeClaim: claimName: my-pvc kubectl create -f pv-pod.yml Expand the PersistentVolumeClaim and record the process. kubectl edit pvc my-pvc --record ... spec: ... resources: requests: storage: 200Mi Delete the Pod and the PersistentVolumeClaim. kubectl delete pod pv-pod kubectl delete pvc my-pvc Check the status of the PersistentVolume to verify that it has been successfully recycled and is available again. kubectl get pv","title":"Persistent Volumes"},{"location":"k8s/#port-binding-with-pods","text":"VII. Port binding Export services via port binding Reference Cluster Networking Challenge: only 1 process can listen on a port per host. So how do all apps on the host use a unique port? In k8s, each pod has its own network namespace and cluster IP address. That IP address is unique within the cluster even if there are multiple worker nodes in the cluster. Tht means ports only need to be unique within each pod. 2 pods can listen on the same port because they each have their own unique IP address within the cluster network. The pods can communicate across nodes simply using the unique IPs. Get a list of Pods in the production namespace: kubectl get pods -n production -o wide Copy the name of the IP address of the application Pod. Example: Use the IP address to make a request to the port on the Pod that serves the frontend content: curl <Pod Cluster IP address>:5000","title":"Port Binding with Pods"},{"location":"k8s/#concurrency-with-containers-and-scaling","text":"VIII. Concurrency Scale out via the process model Reference Deployments By using services to manage access to the app, the service automaticaly picks up the additional pods created during scaling and route traffic to those pods. When you have services alongside deployments you can dynamically change the number of replicas that you have and k8s will take care of everything. Edit the application deployment my-app.yml . Change the number of replicas to 3: ... replicas: 3 Apply the changes: kubectl apply -f my-app.yml -n production Get a list of Pods: kubectl get pods -n production Scale the deployment up again in my-app.yml . Change the number of replicas to 5: ... replicas: 5 Apply the changes: kubectl apply -f my-app .yml -n production Get a list of Pods: kubectl get pods -n production","title":"Concurrency with Containers and Scaling"},{"location":"k8s/#disposability-with-stateless-containers","text":"IX. Disposability Maximize robustness with fast startup and graceful shutdown Reference Security Context Pod Lifecycle Deployments can be used to maintain a specified number of running replicas automatically replacing pods that fail or are deleted. Get a list of Pods: kubectl get pods -n production Locate one of the Pods from the my-app deployment and copy the Pod's name. Delete the Pod using the Pod name: kubectl delete pod <Pod name> -n production Get the list of Pods again. You will notice that the deployment is automatically creating a new Pod to replace the one that was deleted: kubectl get pods -n production","title":"Disposability with Stateless Containers"},{"location":"k8s/#devprod-parity-with-namespaces","text":"X. Dev/prod parity Keep development, staging, and production as similar as possible Reference Namespaces k8s namespaces allow us to have multiple environments in the same cluster. A namespace is like a virtual cluster. Create a new namespace: kubectl create namespace dev Make a copy of the my-app app YAML: cp my-app.yml my-app-dev.yml NodePort services need to be unique within the cluster. We need to choose unique ports so dev doesn't conflict with production. Edit the my-app-svc service in the my-app-dev.yml file to select different nodePort s. You will also need to edit the my-app-config ConfigMap to reflect the new port. Set the nodePorts on the service: apiVersion: v1 kind: Service metadata: name: my-app-svc spec: type: NodePort selector: app: my-app ports: - name: frontend protocol: TCP port: 30082 nodePort: 30082 targetPort: 5000 - name: server protocol: TCP port: 30083 nodePort: 30083 targetPort: 3001 Update the configured port in the ConfigMap: apiVersion: v1 kind: ConfigMap metadata: name: my-app-config data: mongodb.host: \"my-app-mongodb\" mongodb.port: \"27017\" .env: | REACT_APP_API_PORT=\"30083\" Deploy the backing service setup in the new namespace: kubectl apply -f k8s-my-app-mongodb.yml -n dev kubectl apply -f my-app-mongodb.yml -n dev Deploy the app in the new namespace: kubectl apply -f my-app-dev.yml -n dev Check the status of the Pods: kubectl get pods -n dev Once all the Pods are up and running, you should be able to test the dev environment in a browser at <Control Plane Node Public IP>:30082 .","title":"Dev/Prod Parity with Namespaces"},{"location":"k8s/#logs-with-k8s-container-logging","text":"XI. Logs Treat logs as event streams Reference Logging Architecture Kubectl cheatsheet k8s captures log data written to stdout by containers. We can use the k8s API, kubectl logs or external tools to interact with container logs. Edit the source code for the server e.g. src/server/index.js . There is a log function that writes to a file. Change this function to simply write log data to the console: log = function(data) { console.log(data); } Build a new server image because we changed the source code: docker build -t <Your Docker Hub username>/my-app-server:0.0.4 --target server . Push the image: docker push <Your Docker Hub username>/my-app-server:0.0.4 Deploy the new code. Edit my-app.yml . Change the image version for the server to the new image: containers: - name: my-app-server image: <Your Docker Hub username>/my-app-server:0.0.4 kubectl apply -f my-app.yml -n production Get a list of Pods: kubectl get pods -n production Copy the name of one of the my-app deployment Pods and view its logs specifying the pod, namespace, and container. kubectl logs <Pod name> -n production -c my-app-server","title":"Logs with k8s Container Logging"},{"location":"k8s/#admin-processes-with-jobs","text":"XII. Admin processes Run admin/management tasks as one-off processes Reference Jobs A Job e.g. a database migration runs a container until its execution completes. Jobs handle re-trying execution if it fails. Jobs have a restartPolicy of Never because once they complete they don't run again. This example adds the administrative job to the server image but you could package it into its own image. Edit the source code for the admin process in e.g. src/jobs/deDeuplicateJob.js . Locate the block of code that begins with // Setup MongoDb backing database . Change the code to make the database connection configurable: // Setup MongoDb backing database const MongoClient = require('mongodb').MongoClient // MongoDB credentials const username = encodeURIComponent(process.env.MONGODB_USER || \"my-app_user\"); const password = encodeURIComponent(process.env.MONGODB_PASSWORD || \"ILoveTheList\"); // MongoDB connection info const mongoPort = process.env.MONGODB_PORT || 27017; const mongoHost = process.env.MONGODB_HOST || 'localhost'; // MongoDB connection string const mongoURI = `mongodb://${username}:${password}@${mongoHost}:${mongoPort}/my-app`; const mongoURISanitized = `mongodb://${username}:****@${mongoHost}:${mongoPort}/my-app`; console.log(\"MongoDB connection string %s\", mongoURISanitized); const client = new MongoClient(mongoURI); Edit the Dockerfile to include the admin job code in the server image. Add the following line after the other COPY directives for the server image: ... COPY --from=build /usr/src/app/src/jobs . Build and push the server image: docker build -t <Your Docker Hub username>/my-app-server:0.0.5 --target server . docker push <Your Docker Hub username>/my-app-server:0.0.5 Create a Kubernetes Job to run the admin job: Create de-duplicate-job.yml . Supply your Docker Hub username in the image tag: apiVersion: batch/v1 kind: Job metadata: name: de-duplicate spec: template: spec: containers: - name: my-app-server image: <Your Docker Hub username>/my-app-server:0.0.5 command: [\"node\", \"deDeuplicateJob.js\"] env: - name: MONGODB_HOST valueFrom: configMapKeyRef: name: my-app-config key: mongodb.host - name: MONGODB_PORT valueFrom: configMapKeyRef: name: my-app-config key: mongodb.port - name: MONGODB_USER valueFrom: secretKeyRef: name: my-app-secure-config key: mongodb.username - name: MONGODB_PASSWORD valueFrom: secretKeyRef: name: my-app-secure-config key: mongodb.password restartPolicy: Never backoffLimit: 4 Run the Job: kubectl apply -f de-duplicate-job.yml -n production Check the Job status: kubectl get jobs -n production Get the name of the Job Pod: kubectl get pods -n production Use the Pod name to view the logs for the Job Pod: kubectl logs <Pod name> -n production","title":"Admin Processes with Jobs"},{"location":"k8s/#microk8s","text":"On Raspberry Pi Note: Your boot parameters might be in /boot/cmdline.txt . Add these options at the end of the file, then sudo reboot . cgroup_enable=cpuset cgroup_enable=memory cgroup_memory=1 For Raspberry Pi OS install snap first. sudo apt update sudo apt install snapd sudo reboot # ...reconnect after reboot sudo snap install core Then install MicroK8s. pi@raspberrypi4:~ $ sudo snap install microk8s --classic pi@raspberrypi4:~ $ microk8s status --wait-ready pi@raspberrypi4:~ $ microk8s kubectl get all --all-namespaces pi@raspberrypi4:~ $ microk8s enable dns dashboard registry hostpath-storage # or any other addons pi@raspberrypi4:~ $ alias mkctl=\"microk8s kubectl\" pi@raspberrypi4:~ $ alias mkhelm=\"microk8s helm\" pi@raspberrypi4:~ $ mkctl create deployment nginx --image nginx pi@raspberrypi4:~ $ mkctl expose deployment nginx --port 80 --target-port 80 --selector app=nginx --type ClustetIP --name nginx pi@raspberrypi4:~ $ watch microk8s kubectl get all pi@raspberrypi4:~ $ microk8s reset pi@raspberrypi4:~ $ microk8s status pi@raspberrypi4:~ $ microk8s stop # microk8s start pi@raspberrypi4:~ $ microk8s kubectl version --output=yaml You can update a snap package with sudo snap refresh . Configuration file. These are the arguments you can add regarding log rotation --container-log-max-files and --container-log-max-size . They have default values. More info . cat /var/snap/microk8s/current/args/kubelet","title":"MicroK8s"},{"location":"k8s/#registry","text":"Registry doc microk8s enable registry The containerd daemon used by MicroK8s is configured to trust this insecure registry. To upload images we have to tag them with localhost:32000/your-image before pushing them.","title":"Registry"},{"location":"k8s/#microk8s-dashboard","text":"If RBAC is not enabled access the dashboard using the token retrieved with: microk8s kubectl describe secret -n kube-system microk8s-dashboard-token Use this token in the https login UI of the kubernetes-dashboard service. In an RBAC enabled setup ( microk8s enable rbac ) you need to create a user with restricted permissions as shown here . To access remotely from anywhere with port-forward : microk8s kubectl port-forward -n kube-system service/kubernetes-dashboard 10443:443 --address 0.0.0.0 You can then access the Dashboard with IP or hostname as in https://raspberrypi4.local:10443/","title":"MicroK8s dashboard"},{"location":"k8s/#troubleshooting","text":"microk8s inspect MicroK8s might not recognize that cgroup memory is enabled but you can check with cat /proc/cgroups .","title":"Troubleshooting"},{"location":"k8s/#kubernetes-dashboard","text":"Documentation kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.6.1/aio/deploy/recommended.yaml cat << EOF > dashboard-adminuser.yaml apiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kubernetes-dashboard --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kubernetes-dashboard EOF kubectl apply -f dashboard-adminuser.yaml kubectl proxy # Kubectl will make Dashboard available at # http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/ kubectl -n kubernetes-dashboard create token admin-user # Now copy the token and paste it into the Enter token field on the login screen.","title":"Kubernetes Dashboard"},{"location":"mkdocs/","text":"MkDocs For full documentation visit mkdocs.org . pip install mkdocs python-markdown-math Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files. MathJax Test When a \\ne 0 , there are two solutions to ax^2 + bx + c = 0 and they are x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}.","title":"MkDocs"},{"location":"mkdocs/#mkdocs","text":"For full documentation visit mkdocs.org . pip install mkdocs python-markdown-math","title":"MkDocs"},{"location":"mkdocs/#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"mkdocs/#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"mkdocs/#mathjax-test","text":"When a \\ne 0 , there are two solutions to ax^2 + bx + c = 0 and they are x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}.","title":"MathJax Test"},{"location":"mysql/","text":"MySQL development Install MySQL Community Edition Use your OS package manager or download .dmg installer for macOS. Take note of the root password. Configure your PATH Add the mysql location to your PATH. Typically as part of your ~/.bash_profile export PATH=/usr/local/mysql/bin:$PATH Start the MySQL service On macOS sudo launchctl load -F /Library/LaunchDaemons/com.oracle.oss.mysql.mysqld.plist Verify it's running On macOS sudo launchctl list | grep mysql Connect to your MySQL instance Use MySQL Workbench or other client tool. To stop MySQL On macOS sudo launchctl unload -F /Library/LaunchDaemons/com.oracle.oss.mysql.mysqld.plist Exporting data If you need to export data you may need to disable or set a security setting. On macOS: View the variable using your SQL client. If it's NULL you will be restricted regarding file operations. show variables like 'secure_file_priv'; Open the configuration file. cd /Library/LaunchDaemons sudo nano com.oracle.oss.mysql.mysqld.plist and set the --secure-file-priv to an empty string (to disable the restriction) or a dir of your choice. <key>ProgramArguments</key> <array> <string>--secure-file-priv=</string> </array> Then restart MySQL. Now you can export data: SELECT * INTO OUTFILE 'your_file.csv' FIELDS TERMINATED BY ',' ENCLOSED BY '\"' FROM `your_db`.`your_table` You can find your exported data: sudo find /usr/local/mysql/data -name your_file.csv","title":"MySQL"},{"location":"mysql/#mysql-development","text":"Install MySQL Community Edition Use your OS package manager or download .dmg installer for macOS. Take note of the root password. Configure your PATH Add the mysql location to your PATH. Typically as part of your ~/.bash_profile export PATH=/usr/local/mysql/bin:$PATH Start the MySQL service On macOS sudo launchctl load -F /Library/LaunchDaemons/com.oracle.oss.mysql.mysqld.plist Verify it's running On macOS sudo launchctl list | grep mysql Connect to your MySQL instance Use MySQL Workbench or other client tool. To stop MySQL On macOS sudo launchctl unload -F /Library/LaunchDaemons/com.oracle.oss.mysql.mysqld.plist Exporting data If you need to export data you may need to disable or set a security setting. On macOS: View the variable using your SQL client. If it's NULL you will be restricted regarding file operations. show variables like 'secure_file_priv'; Open the configuration file. cd /Library/LaunchDaemons sudo nano com.oracle.oss.mysql.mysqld.plist and set the --secure-file-priv to an empty string (to disable the restriction) or a dir of your choice. <key>ProgramArguments</key> <array> <string>--secure-file-priv=</string> </array> Then restart MySQL. Now you can export data: SELECT * INTO OUTFILE 'your_file.csv' FIELDS TERMINATED BY ',' ENCLOSED BY '\"' FROM `your_db`.`your_table` You can find your exported data: sudo find /usr/local/mysql/data -name your_file.csv","title":"MySQL development"},{"location":"node/","text":"Node.js Install macOS Download the Node.js installer from https://nodejs.org/en/download/ or brew install node Linux sudo apt install nodejs sudo apt install npm [Optional] Install nvm from https://github.com/creationix/nvm to manage multiple versions of node and npm. Windows Subsystem for Linux On WSL the recommended approach for installing a current version of Node.js is nvm. touch ~/.bashrc curl -o- https://raw.githubusercontent.com/creationix/nvm/v0.33.11/install.sh | bash # Reload your configuration source ~/.bashrc nvm install node # Upgrade npm. You may need to run this twice on WSL. npm install npm@latest -g Update npm If you installed npm as part of node you may need to update npm. sudo npm install -g npm Initialize a node project by creating a package.json npm init Installing dependencies examples sudo npm install --save ask-sdk moment sudo npm install --save-dev mocha chai eslint virtual-alexa Troubleshooting mocha If you get an error running mocha tests e.g. node_modules/.bin/mocha not having execute permissions or mocha Error: Cannot find module './options' delete your node_modules folder and npm install . Set up ESLint with a configuration file eslint --init # you may need to run it as: # sudo ./node_modules/.bin/eslint --init","title":"Node"},{"location":"node/#nodejs","text":"Install macOS Download the Node.js installer from https://nodejs.org/en/download/ or brew install node Linux sudo apt install nodejs sudo apt install npm [Optional] Install nvm from https://github.com/creationix/nvm to manage multiple versions of node and npm. Windows Subsystem for Linux On WSL the recommended approach for installing a current version of Node.js is nvm. touch ~/.bashrc curl -o- https://raw.githubusercontent.com/creationix/nvm/v0.33.11/install.sh | bash # Reload your configuration source ~/.bashrc nvm install node # Upgrade npm. You may need to run this twice on WSL. npm install npm@latest -g Update npm If you installed npm as part of node you may need to update npm. sudo npm install -g npm Initialize a node project by creating a package.json npm init Installing dependencies examples sudo npm install --save ask-sdk moment sudo npm install --save-dev mocha chai eslint virtual-alexa Troubleshooting mocha If you get an error running mocha tests e.g. node_modules/.bin/mocha not having execute permissions or mocha Error: Cannot find module './options' delete your node_modules folder and npm install . Set up ESLint with a configuration file eslint --init # you may need to run it as: # sudo ./node_modules/.bin/eslint --init","title":"Node.js"},{"location":"package-mgmt/","text":"Package manager macOS Get Homebrew , a package manager for macOS. /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\" brew doctor You can use a more detailed guide if needed. Linux Depending on your distribution, you may have apt (Ubuntu), yum (Red Hat), or zypper (openSUSE).","title":"Package Manager"},{"location":"package-mgmt/#package-manager","text":"","title":"Package manager"},{"location":"package-mgmt/#macos","text":"Get Homebrew , a package manager for macOS. /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\" brew doctor You can use a more detailed guide if needed.","title":"macOS"},{"location":"package-mgmt/#linux","text":"Depending on your distribution, you may have apt (Ubuntu), yum (Red Hat), or zypper (openSUSE).","title":"Linux"},{"location":"python/","text":"Python Packaging Python Projects Install Python, the pip Python package installer, and Setuptools With Homebrew Even if you already have Python on your OS it might be an outdated version. This installs python and pip. macOS brew install python python3 --version The python formulae install pip (as pip3) and Setuptools. Setuptools can be updated via pip3 and pip3 can be used to upgrade itself. python3 -m pip install --upgrade setuptools python3 -m pip install --upgrade pip Add the unversioned symlinks to your $PATH by adding this to your .zshrc file. export PATH=\"$(brew --prefix)/opt/python/libexec/bin:$PATH\" See the Homebrew Python documentation for details. site-packages is here. Example for python 3.10 on Homebrew ls -al $(brew --prefix)/lib/python3.10/site-packages Without Homebrew Linux (Ubuntu) sudo apt install python3 macOS Download .pkg installer from pythong.org. Then get pip. curl -o get-pip.py https://bootstrap.pypa.io/get-pip.py sudo python get-pip.py Linux sudo apt install python-pip pip install --upgrade pip setuptools On Ubuntu, upgrading to pip 10 may break pip. If this happens you need to: sudo nano /usr/bin/pip # change \"from pip import main\" to \"from pip._internal import main\" Get the pep8 python style checker # On Linux you may need to use sudo. pip install pep8 Anaconda For data science and machine learning. Anaconda is a distribution of the Python and R programming languages for scientific computing. See Getting Started . brew install --cask anaconda Installing in silent mode Add anaconda to your PATH on your .zshrc and source ~/.zshrc to apply the changes. export PATH=\"$(brew --prefix)/anaconda3/bin:$PATH\" To use zsh: conda init zsh If conda init zsh messed up the PATH in your ~/.zshrc by adding the condabin directory instead of bin you can fix it with a symlink: ln -sf $(brew --prefix)/anaconda3/bin/jupyter-lab $(brew --prefix)/anaconda3/condabin/jupyter-lab ln -sf $(brew --prefix)/anaconda3/bin/jupyter $(brew --prefix)/anaconda3/condabin/jupyter If you don't want to activate the base environment every time you open your terminal: conda config --set auto_activate_base false When creating a conda environment you can use optionally use a configuration file. You can also of course, save a config file from an existing environment to back it up. conda env create -f myenv.yml conda activate myenv conda env list conda env remove --name ldm Notebooks Fresh installs of Anaconda no longer include notebook extensions in their Jupyter installer. This means that the nb_conda libraries need to be added into your environment separately to access conda environments from your Jupyter notebook. Just run these to add them and you should be able to select your environment as a kernel within a Jupyter notebook. conda activate <myenv> conda install ipykernel conda install nb_conda_kernels # or defaults::nb_conda_kernels Now you can launch the new JupyterLab or the classic Jupyter Notebook. jupyter-lab # or the classic \"jupyter notebook\"","title":"Python"},{"location":"python/#python","text":"Packaging Python Projects","title":"Python"},{"location":"python/#install-python-the-pip-python-package-installer-and-setuptools","text":"","title":"Install Python, the pip Python package installer, and Setuptools"},{"location":"python/#with-homebrew","text":"Even if you already have Python on your OS it might be an outdated version. This installs python and pip. macOS brew install python python3 --version The python formulae install pip (as pip3) and Setuptools. Setuptools can be updated via pip3 and pip3 can be used to upgrade itself. python3 -m pip install --upgrade setuptools python3 -m pip install --upgrade pip Add the unversioned symlinks to your $PATH by adding this to your .zshrc file. export PATH=\"$(brew --prefix)/opt/python/libexec/bin:$PATH\" See the Homebrew Python documentation for details. site-packages is here. Example for python 3.10 on Homebrew ls -al $(brew --prefix)/lib/python3.10/site-packages","title":"With Homebrew"},{"location":"python/#without-homebrew","text":"Linux (Ubuntu) sudo apt install python3 macOS Download .pkg installer from pythong.org. Then get pip. curl -o get-pip.py https://bootstrap.pypa.io/get-pip.py sudo python get-pip.py Linux sudo apt install python-pip pip install --upgrade pip setuptools On Ubuntu, upgrading to pip 10 may break pip. If this happens you need to: sudo nano /usr/bin/pip # change \"from pip import main\" to \"from pip._internal import main\"","title":"Without Homebrew"},{"location":"python/#get-the-pep8-python-style-checker","text":"# On Linux you may need to use sudo. pip install pep8","title":"Get the pep8 python style checker"},{"location":"python/#anaconda","text":"For data science and machine learning. Anaconda is a distribution of the Python and R programming languages for scientific computing. See Getting Started . brew install --cask anaconda Installing in silent mode Add anaconda to your PATH on your .zshrc and source ~/.zshrc to apply the changes. export PATH=\"$(brew --prefix)/anaconda3/bin:$PATH\" To use zsh: conda init zsh If conda init zsh messed up the PATH in your ~/.zshrc by adding the condabin directory instead of bin you can fix it with a symlink: ln -sf $(brew --prefix)/anaconda3/bin/jupyter-lab $(brew --prefix)/anaconda3/condabin/jupyter-lab ln -sf $(brew --prefix)/anaconda3/bin/jupyter $(brew --prefix)/anaconda3/condabin/jupyter If you don't want to activate the base environment every time you open your terminal: conda config --set auto_activate_base false When creating a conda environment you can use optionally use a configuration file. You can also of course, save a config file from an existing environment to back it up. conda env create -f myenv.yml conda activate myenv conda env list conda env remove --name ldm","title":"Anaconda"},{"location":"python/#notebooks","text":"Fresh installs of Anaconda no longer include notebook extensions in their Jupyter installer. This means that the nb_conda libraries need to be added into your environment separately to access conda environments from your Jupyter notebook. Just run these to add them and you should be able to select your environment as a kernel within a Jupyter notebook. conda activate <myenv> conda install ipykernel conda install nb_conda_kernels # or defaults::nb_conda_kernels Now you can launch the new JupyterLab or the classic Jupyter Notebook. jupyter-lab # or the classic \"jupyter notebook\"","title":"Notebooks"},{"location":"rpi/","text":"Raspberry Pi Setup Write and preconfigure Raspberry Pi OS on the SD card using Raspberry Pi Imager ( brew install --cask raspberry-pi-imager ). Make sure you use the correct version of the OS (32-bit or 64-bit). Change the default password for the pi user. Enable SSH (password or ssh keys). Configure WiFi if needed. Take note of the hostname. Insert the SD card in your Raspberry Pi and turn it on. If you didn't do so during setup, you can still generate and add an ssh key at any time. Example: # on your laptop ssh-keygen ssh-copy-id -i ~/.ssh/id_rsa pi@raspberrypi4.local To remove password authentication: # on the Pi sudo nano /etc/ssh/sshd_config and replace #PasswordAuthentication yes with PasswordAuthentication no . Test the validity of the config file and restart the service (or reboot). sudo sshd -t sudo service sshd restart sudo service sshd status Configuration Find the IP of your Raspberry Pi using its hostname or find all devices on your network with arp -a . arp raspberrypi4.local SSH into it with the pi user and the IP address or hostname. Examples: ssh pi@192.168.xxx.xxx ssh pi@raspberrypi4.local # You'll be asked for the password if applicable Configure it. pi@raspberrypi4:~ $ sudo raspi-config # Go to Interface Options, VNC (for graphical remote access) # Tab to the Finish option and reboot. Update it. upgrade is used to install available upgrades of all packages currently installed on the system. New packages will be installed if required to satisfy dependencies, but existing packages will never be removed. If an upgrade for a package requires the removal of an installed package the upgrade for this package isn't performed. full-upgrade performs the function of upgrade but will remove currently installed packages if this is needed to upgrade the system as a whole. pi@raspberrypi4:~ $ sudo apt update # updates the package list pi@raspberrypi4:~ $ sudo apt full-upgrade To give it a static IP Find the IP adddress of your router. It's the address that appears after default via . pi@raspberrypi4:~ $ ip r default via [IP] Get the IP of your DNS server (it may or may not be your router) pi@raspberrypi4:~ $ grep nameserver /etc/resolv.conf Open this file: nano /etc/dhcpcd.conf and add/edit these lines at the end filling in the correct info. interface [wlan0 for Wi-Fi or eth0 for Ethernet] static_routers=[ROUTER IP] static domain_name_servers=[DNS IP] [static or inform] ip_address=[STATIC IP ADDRESS YOU WANT]/24 inform means that the Raspberry Pi will attempt to get the IP address you requested, but if it's not available, it will choose another. If you use static , it will have no IP v4 address at all if the requested one is in use. Save the file and sudo reboot . From now on, upon each boot, the Pi will attempt to obtain the static ip address you requested. You could also set your router to manually assign the static IP to the Raspberry Pi under its DHCP settings e.g. LAN, DHCP server. To set it up as a DNS server Install and configure a DNS Server e.g. DNSmasq or Pi-Hole on the Raspberry Pi. Change your router\u2019s DNS settings to point to the Raspberry Pi. Log in to your router's admin interface and look for DNS e.g. in LAN, DHCP Server. Set the primary DNS server to the IP of your Raspberry Pi and make sure it's the only DNS server. The Raspberry Pi will handle upstream DNS services. Remote GUI access Now you'll need a VNC viewer on your laptop to connect to the Raspberry Pi using the graphical interface. brew install --cask vnc-viewer Apparently, on Raspberry Pi pip does not download from the python package index (PyPi), it downloads from PiWheels. PiWheels wheels do not come with pygame's dependencies that are bundled in normal releases. Install Pygame dependencies and Pygame. pi@raspberrypi4:~ $ sudo apt install libvorbisenc2 libwayland-server0 libxi6 libfluidsynth2 libgbm1 libxkbcommon0 libopus0 libwayland-cursor0 libsndfile1 libwayland-client0 libportmidi0 libvorbis0a libopusfile0 libmpg123-0 libflac8 libxcursor1 libxinerama1 libasyncns0 libxrandr2 libdrm2 libpulse0 libxfixes3 libvorbisfile3 libmodplug1 libxrender1 libsdl2-2.0-0 libxxf86vm1 libwayland-egl1 libsdl2-ttf-2.0-0 libsdl2-image-2.0-0 libjack0 libsdl2-mixer-2.0-0 libinstpatch-1.0-2 libxss1 libogg0 pi@raspberrypi4:~ $ sudo pip3 install pygame Check that the installation worked by running one of its demos pi@raspberrypi4:~ $ python3 -m pygame.examples.aliens Copy files Copy files between Pi and local machine. On local machine run: scp -r pi@raspberrypi2.local:/home/pi/Documents/ ~/Documents/pidocs Find info about your Pi 32 or 64-bit kernel? getconf LONG_BIT # or check machine's hardware name: armv7l is 32-bit and aarch64 is 64-bit pi@raspberrypi4:~ $ uname -m See OS version cat /etc/os-release Architecture If the following returns a Tag_ABI_VFP_args tag of VFP registers , it's an armhf ( arm ) system. A blank output means armel ( arm/v6 ). pi@raspberrypi2:~ $ readelf -A /proc/self/exe | grep Tag_ABI_VFP_args Or check the architecture with: hostnamectl You can find info about the hardware like ports, pins, RAM, SoC, connectivity, etc. with: pi@raspberrypi4:~ $ pinout Learn about electronics I've added some sample code from the MagPi Essentials book . Sample code GPIO Header","title":"Raspberry Pi"},{"location":"rpi/#raspberry-pi","text":"","title":"Raspberry Pi"},{"location":"rpi/#setup","text":"Write and preconfigure Raspberry Pi OS on the SD card using Raspberry Pi Imager ( brew install --cask raspberry-pi-imager ). Make sure you use the correct version of the OS (32-bit or 64-bit). Change the default password for the pi user. Enable SSH (password or ssh keys). Configure WiFi if needed. Take note of the hostname. Insert the SD card in your Raspberry Pi and turn it on. If you didn't do so during setup, you can still generate and add an ssh key at any time. Example: # on your laptop ssh-keygen ssh-copy-id -i ~/.ssh/id_rsa pi@raspberrypi4.local To remove password authentication: # on the Pi sudo nano /etc/ssh/sshd_config and replace #PasswordAuthentication yes with PasswordAuthentication no . Test the validity of the config file and restart the service (or reboot). sudo sshd -t sudo service sshd restart sudo service sshd status","title":"Setup"},{"location":"rpi/#configuration","text":"Find the IP of your Raspberry Pi using its hostname or find all devices on your network with arp -a . arp raspberrypi4.local SSH into it with the pi user and the IP address or hostname. Examples: ssh pi@192.168.xxx.xxx ssh pi@raspberrypi4.local # You'll be asked for the password if applicable Configure it. pi@raspberrypi4:~ $ sudo raspi-config # Go to Interface Options, VNC (for graphical remote access) # Tab to the Finish option and reboot. Update it. upgrade is used to install available upgrades of all packages currently installed on the system. New packages will be installed if required to satisfy dependencies, but existing packages will never be removed. If an upgrade for a package requires the removal of an installed package the upgrade for this package isn't performed. full-upgrade performs the function of upgrade but will remove currently installed packages if this is needed to upgrade the system as a whole. pi@raspberrypi4:~ $ sudo apt update # updates the package list pi@raspberrypi4:~ $ sudo apt full-upgrade","title":"Configuration"},{"location":"rpi/#to-give-it-a-static-ip","text":"Find the IP adddress of your router. It's the address that appears after default via . pi@raspberrypi4:~ $ ip r default via [IP] Get the IP of your DNS server (it may or may not be your router) pi@raspberrypi4:~ $ grep nameserver /etc/resolv.conf Open this file: nano /etc/dhcpcd.conf and add/edit these lines at the end filling in the correct info. interface [wlan0 for Wi-Fi or eth0 for Ethernet] static_routers=[ROUTER IP] static domain_name_servers=[DNS IP] [static or inform] ip_address=[STATIC IP ADDRESS YOU WANT]/24 inform means that the Raspberry Pi will attempt to get the IP address you requested, but if it's not available, it will choose another. If you use static , it will have no IP v4 address at all if the requested one is in use. Save the file and sudo reboot . From now on, upon each boot, the Pi will attempt to obtain the static ip address you requested. You could also set your router to manually assign the static IP to the Raspberry Pi under its DHCP settings e.g. LAN, DHCP server.","title":"To give it a static IP"},{"location":"rpi/#to-set-it-up-as-a-dns-server","text":"Install and configure a DNS Server e.g. DNSmasq or Pi-Hole on the Raspberry Pi. Change your router\u2019s DNS settings to point to the Raspberry Pi. Log in to your router's admin interface and look for DNS e.g. in LAN, DHCP Server. Set the primary DNS server to the IP of your Raspberry Pi and make sure it's the only DNS server. The Raspberry Pi will handle upstream DNS services.","title":"To set it up as a DNS server"},{"location":"rpi/#remote-gui-access","text":"Now you'll need a VNC viewer on your laptop to connect to the Raspberry Pi using the graphical interface. brew install --cask vnc-viewer Apparently, on Raspberry Pi pip does not download from the python package index (PyPi), it downloads from PiWheels. PiWheels wheels do not come with pygame's dependencies that are bundled in normal releases. Install Pygame dependencies and Pygame. pi@raspberrypi4:~ $ sudo apt install libvorbisenc2 libwayland-server0 libxi6 libfluidsynth2 libgbm1 libxkbcommon0 libopus0 libwayland-cursor0 libsndfile1 libwayland-client0 libportmidi0 libvorbis0a libopusfile0 libmpg123-0 libflac8 libxcursor1 libxinerama1 libasyncns0 libxrandr2 libdrm2 libpulse0 libxfixes3 libvorbisfile3 libmodplug1 libxrender1 libsdl2-2.0-0 libxxf86vm1 libwayland-egl1 libsdl2-ttf-2.0-0 libsdl2-image-2.0-0 libjack0 libsdl2-mixer-2.0-0 libinstpatch-1.0-2 libxss1 libogg0 pi@raspberrypi4:~ $ sudo pip3 install pygame Check that the installation worked by running one of its demos pi@raspberrypi4:~ $ python3 -m pygame.examples.aliens","title":"Remote GUI access"},{"location":"rpi/#copy-files","text":"Copy files between Pi and local machine. On local machine run: scp -r pi@raspberrypi2.local:/home/pi/Documents/ ~/Documents/pidocs","title":"Copy files"},{"location":"rpi/#find-info-about-your-pi","text":"32 or 64-bit kernel? getconf LONG_BIT # or check machine's hardware name: armv7l is 32-bit and aarch64 is 64-bit pi@raspberrypi4:~ $ uname -m See OS version cat /etc/os-release Architecture If the following returns a Tag_ABI_VFP_args tag of VFP registers , it's an armhf ( arm ) system. A blank output means armel ( arm/v6 ). pi@raspberrypi2:~ $ readelf -A /proc/self/exe | grep Tag_ABI_VFP_args Or check the architecture with: hostnamectl You can find info about the hardware like ports, pins, RAM, SoC, connectivity, etc. with: pi@raspberrypi4:~ $ pinout","title":"Find info about your Pi"},{"location":"rpi/#learn-about-electronics","text":"I've added some sample code from the MagPi Essentials book . Sample code","title":"Learn about electronics"},{"location":"rpi/#gpio-header","text":"","title":"GPIO Header"},{"location":"shell/","text":"Shell Bash If you want to use bash brew install bash # get the latest version of bash chsh -s $(which bash) nano ~/.bash_profile # and paste from sample dot file Zsh brew install zsh # get the latest version # You may need to add the Homebrew version of Zsh to the list on /etc/shells sudo nano /etc/shells # add the output of \"which zsh\" to the top of the list, save the file. where zsh # you might see both the shell that came with your Mac and the latest from Homebrew which zsh # make sure this returns the one from Homebrew. chsh -s $(which zsh) # Install Oh My Zsh, a framework for managing your Zsh configuration with plugins and themes sh -c \"$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\" Oh My Zsh Add any built-in plugins you need to your ~/.zshrc plugins=(git macos python) You can also add custom plugins by cloning the repo into the plugins directory or with Homebrew. This plugin auto suggests previous commands. git clone https://github.com/zsh-users/zsh-autosuggestions $ZSH_CUSTOM/plugins/zsh-autosuggestions # add zsh-autosuggestions to plugins list in ~/.zshrc and: source ~/.zshrc This plugin zsh-syntax-highlighting highlights valid commands green and invalid ones red so you don't have to test the command to see if it will work. git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting Or just use Homebrew: brew install zsh-syntax-highlighting # To activate the syntax highlighting, add the following at the end of your ~/.zshrc: source $(brew --prefix)/share/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh Some Oh My Zsh themes like Spaceship have requirements like the Powerline Fonts. Get them with: git clone https://github.com/powerline/fonts.git --depth=1 ./fonts/install.sh rm -rf fonts You can also install fonts with Homebrew by adding the fonts repository. Fira Code is a good one for programming and computer science. brew tap homebrew/cask-fonts brew install --cask font-fira-code Install your Oh My Zsh theme e.g. Spaceship brew install spaceship # If the theme is not copied to your themes folder, sim link from the Homebrew dir to your custom themes folder e.g. ln -sf $(brew --prefix)/Cellar/spaceship/4.4.1/spaceship.zsh $ZSH_CUSTOM/themes/spaceship.zsh-theme touch ~/.spaceshiprc.zsh Any time you edit your zsh configuration file you can reload it to apply changes. source ~/.zshrc Terminal replacement You can use iTerm2 brew install --cask iterm2 After you have installed the font(s) required by your Oh My Zsh theme set your iTerm preferences like default shell and font. Verify that you're using the shell you want. In the output of the env command look for something like Shell=/opt/homebrew/bin/zsh . Install iTerm 2 color schemes git clone https://github.com/mbadolato/iTerm2-Color-Schemes.git cd iTerm2-Color-Schemes # Import all color schemes tools/import-scheme.sh schemes/* Restart iTerm 2 (need to quit iTerm 2 to reload the configuration file). iTerm2 > Preferences > Profile > Colors > Color Presets","title":"Shell"},{"location":"shell/#shell","text":"","title":"Shell"},{"location":"shell/#bash","text":"If you want to use bash brew install bash # get the latest version of bash chsh -s $(which bash) nano ~/.bash_profile # and paste from sample dot file","title":"Bash"},{"location":"shell/#zsh","text":"brew install zsh # get the latest version # You may need to add the Homebrew version of Zsh to the list on /etc/shells sudo nano /etc/shells # add the output of \"which zsh\" to the top of the list, save the file. where zsh # you might see both the shell that came with your Mac and the latest from Homebrew which zsh # make sure this returns the one from Homebrew. chsh -s $(which zsh) # Install Oh My Zsh, a framework for managing your Zsh configuration with plugins and themes sh -c \"$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\" Oh My Zsh Add any built-in plugins you need to your ~/.zshrc plugins=(git macos python) You can also add custom plugins by cloning the repo into the plugins directory or with Homebrew. This plugin auto suggests previous commands. git clone https://github.com/zsh-users/zsh-autosuggestions $ZSH_CUSTOM/plugins/zsh-autosuggestions # add zsh-autosuggestions to plugins list in ~/.zshrc and: source ~/.zshrc This plugin zsh-syntax-highlighting highlights valid commands green and invalid ones red so you don't have to test the command to see if it will work. git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting Or just use Homebrew: brew install zsh-syntax-highlighting # To activate the syntax highlighting, add the following at the end of your ~/.zshrc: source $(brew --prefix)/share/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh Some Oh My Zsh themes like Spaceship have requirements like the Powerline Fonts. Get them with: git clone https://github.com/powerline/fonts.git --depth=1 ./fonts/install.sh rm -rf fonts You can also install fonts with Homebrew by adding the fonts repository. Fira Code is a good one for programming and computer science. brew tap homebrew/cask-fonts brew install --cask font-fira-code Install your Oh My Zsh theme e.g. Spaceship brew install spaceship # If the theme is not copied to your themes folder, sim link from the Homebrew dir to your custom themes folder e.g. ln -sf $(brew --prefix)/Cellar/spaceship/4.4.1/spaceship.zsh $ZSH_CUSTOM/themes/spaceship.zsh-theme touch ~/.spaceshiprc.zsh Any time you edit your zsh configuration file you can reload it to apply changes. source ~/.zshrc","title":"Zsh"},{"location":"shell/#terminal-replacement","text":"You can use iTerm2 brew install --cask iterm2 After you have installed the font(s) required by your Oh My Zsh theme set your iTerm preferences like default shell and font. Verify that you're using the shell you want. In the output of the env command look for something like Shell=/opt/homebrew/bin/zsh . Install iTerm 2 color schemes git clone https://github.com/mbadolato/iTerm2-Color-Schemes.git cd iTerm2-Color-Schemes # Import all color schemes tools/import-scheme.sh schemes/* Restart iTerm 2 (need to quit iTerm 2 to reload the configuration file). iTerm2 > Preferences > Profile > Colors > Color Presets","title":"Terminal replacement"},{"location":"utilities/","text":"Install utilities These will vary for each person. Some examples on a Mac: brew install wget brew install gcc brew install jq brew install kompose # translate Docker Compose files to Kubernetes resources brew install helm brew install --cask docker brew install --cask drawio # Diagrams (UML, AWS, etc.) brew install --cask 1password brew install --cask nordvpn brew install --cask google-chrome # or chromium brew install --cask firefox brew install --cask visual-studio-code # or vscodium brew install --cask kindle brew install --cask authy brew install --cask teamviewer brew install --cask raspberry-pi-imager brew install --cask balenaetcher # to make bootable media brew install --cask zoom brew install --cask spotify brew install --cask whatsapp brew install --cask messenger # Facebook messenger brew install --cask rectangle # to snap windows brew install --cask avast-security # if it fails, download from website brew install --cask handbrake # if you need to rip DVDs brew install --cask virtualbox # currently Intel Macs only. ARM installer in beta. brew install --cask multipass # run Ubuntu VMs in a local mini-cloud. brew install --cask steam # if you want games brew tap homebrew/cask-drivers # add repository of drivers brew install logitech-options # driver for Logitech mouse On macOS you can also install gcc by installing the xcode developer tools if you're not using Homebrew. xcode-select -v # check if tools are installed xcode-select --install","title":"Utilities"},{"location":"utilities/#install-utilities","text":"These will vary for each person. Some examples on a Mac: brew install wget brew install gcc brew install jq brew install kompose # translate Docker Compose files to Kubernetes resources brew install helm brew install --cask docker brew install --cask drawio # Diagrams (UML, AWS, etc.) brew install --cask 1password brew install --cask nordvpn brew install --cask google-chrome # or chromium brew install --cask firefox brew install --cask visual-studio-code # or vscodium brew install --cask kindle brew install --cask authy brew install --cask teamviewer brew install --cask raspberry-pi-imager brew install --cask balenaetcher # to make bootable media brew install --cask zoom brew install --cask spotify brew install --cask whatsapp brew install --cask messenger # Facebook messenger brew install --cask rectangle # to snap windows brew install --cask avast-security # if it fails, download from website brew install --cask handbrake # if you need to rip DVDs brew install --cask virtualbox # currently Intel Macs only. ARM installer in beta. brew install --cask multipass # run Ubuntu VMs in a local mini-cloud. brew install --cask steam # if you want games brew tap homebrew/cask-drivers # add repository of drivers brew install logitech-options # driver for Logitech mouse On macOS you can also install gcc by installing the xcode developer tools if you're not using Homebrew. xcode-select -v # check if tools are installed xcode-select --install","title":"Install utilities"}]}